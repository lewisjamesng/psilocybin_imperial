{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 25.800, val_loss: 29.074\n",
      "[2] loss: 24.847, val_loss: 27.641\n",
      "[3] loss: 23.634, val_loss: 25.453\n",
      "[4] loss: 21.749, val_loss: 22.125\n",
      "[5] loss: 18.866, val_loss: 17.189\n",
      "[6] loss: 14.732, val_loss: 10.086\n",
      "[7] loss: 9.260, val_loss: 6.398\n",
      "[8] loss: 5.823, val_loss: 13.247\n",
      "[9] loss: 10.956, val_loss: 12.019\n",
      "[10] loss: 9.761, val_loss: 8.059\n",
      "[11] loss: 6.025, val_loss: 6.353\n",
      "[12] loss: 5.931, val_loss: 7.435\n",
      "[13] loss: 6.927, val_loss: 8.068\n",
      "[14] loss: 7.428, val_loss: 7.723\n",
      "[15] loss: 7.102, val_loss: 6.643\n",
      "[16] loss: 6.321, val_loss: 6.239\n",
      "[17] loss: 5.638, val_loss: 7.056\n",
      "[18] loss: 5.559, val_loss: 8.048\n",
      "[19] loss: 5.981, val_loss: 7.833\n",
      "[20] loss: 5.903, val_loss: 7.051\n",
      "[21] loss: 5.501, val_loss: 6.421\n",
      "[22] loss: 5.440, val_loss: 6.406\n",
      "[23] loss: 5.518, val_loss: 6.514\n",
      "[24] loss: 5.535, val_loss: 6.457\n",
      "[25] loss: 5.461, val_loss: 6.450\n",
      "[26] loss: 5.296, val_loss: 6.833\n",
      "[27] loss: 5.185, val_loss: 7.220\n",
      "[28] loss: 5.225, val_loss: 7.195\n",
      "[29] loss: 5.179, val_loss: 6.740\n",
      "[30] loss: 5.060, val_loss: 6.739\n",
      "[31] loss: 5.054, val_loss: 6.782\n",
      "[32] loss: 4.982, val_loss: 6.839\n",
      "[33] loss: 4.929, val_loss: 6.865\n",
      "[34] loss: 4.896, val_loss: 6.751\n",
      "[35] loss: 4.854, val_loss: 6.789\n",
      "[36] loss: 4.830, val_loss: 6.753\n",
      "[37] loss: 4.813, val_loss: 6.799\n",
      "[38] loss: 4.826, val_loss: 6.580\n",
      "[39] loss: 4.764, val_loss: 6.533\n",
      "[40] loss: 4.753, val_loss: 6.533\n",
      "[41] loss: 4.710, val_loss: 6.467\n",
      "[42] loss: 4.671, val_loss: 6.466\n",
      "[43] loss: 4.655, val_loss: 6.383\n",
      "[44] loss: 4.618, val_loss: 6.439\n",
      "[45] loss: 4.587, val_loss: 6.189\n",
      "[46] loss: 4.674, val_loss: 6.220\n",
      "[47] loss: 4.597, val_loss: 6.375\n",
      "[48] loss: 4.588, val_loss: 6.270\n",
      "[49] loss: 4.555, val_loss: 6.148\n",
      "[50] loss: 4.553, val_loss: 6.043\n",
      "[51] loss: 4.508, val_loss: 6.134\n",
      "[52] loss: 4.449, val_loss: 6.216\n",
      "[53] loss: 4.441, val_loss: 6.012\n",
      "[54] loss: 4.431, val_loss: 5.944\n",
      "[55] loss: 4.393, val_loss: 5.963\n",
      "[56] loss: 4.347, val_loss: 6.197\n",
      "[57] loss: 4.332, val_loss: 5.765\n",
      "[58] loss: 4.307, val_loss: 5.927\n",
      "[59] loss: 4.267, val_loss: 5.914\n",
      "[60] loss: 4.237, val_loss: 6.182\n",
      "[61] loss: 4.220, val_loss: 5.891\n",
      "[62] loss: 4.177, val_loss: 6.277\n",
      "[63] loss: 4.204, val_loss: 5.918\n",
      "[64] loss: 4.185, val_loss: 6.422\n",
      "[65] loss: 4.148, val_loss: 5.669\n",
      "[66] loss: 4.284, val_loss: 6.218\n",
      "[67] loss: 4.061, val_loss: 6.360\n",
      "[68] loss: 4.047, val_loss: 5.712\n",
      "[69] loss: 4.189, val_loss: 6.480\n",
      "[70] loss: 4.085, val_loss: 5.803\n",
      "[71] loss: 4.142, val_loss: 6.168\n",
      "[72] loss: 4.011, val_loss: 7.355\n",
      "[73] loss: 4.532, val_loss: 5.712\n",
      "[74] loss: 4.088, val_loss: 6.275\n",
      "[75] loss: 4.010, val_loss: 5.971\n",
      "[76] loss: 4.004, val_loss: 6.428\n",
      "[77] loss: 3.975, val_loss: 5.909\n",
      "[78] loss: 3.897, val_loss: 6.398\n",
      "[79] loss: 3.911, val_loss: 6.364\n",
      "[80] loss: 3.835, val_loss: 6.433\n",
      "[81] loss: 3.798, val_loss: 6.194\n",
      "[82] loss: 3.801, val_loss: 6.336\n",
      "[83] loss: 3.790, val_loss: 5.498\n",
      "[84] loss: 4.123, val_loss: 7.577\n",
      "[85] loss: 4.400, val_loss: 6.401\n",
      "[86] loss: 3.779, val_loss: 5.655\n",
      "[87] loss: 5.278, val_loss: 5.499\n",
      "[88] loss: 5.100, val_loss: 6.523\n",
      "[89] loss: 3.837, val_loss: 6.976\n",
      "[90] loss: 4.085, val_loss: 5.332\n",
      "[91] loss: 4.150, val_loss: 5.910\n",
      "[92] loss: 3.611, val_loss: 7.860\n",
      "[93] loss: 4.576, val_loss: 6.174\n",
      "[94] loss: 3.565, val_loss: 5.517\n",
      "[95] loss: 4.421, val_loss: 6.000\n",
      "[96] loss: 3.638, val_loss: 8.075\n",
      "[97] loss: 4.711, val_loss: 6.653\n",
      "[98] loss: 3.756, val_loss: 5.739\n",
      "[99] loss: 4.923, val_loss: 5.646\n",
      "[100] loss: 4.329, val_loss: 7.120\n",
      "[101] loss: 4.004, val_loss: 7.299\n",
      "[102] loss: 4.099, val_loss: 5.982\n",
      "[103] loss: 3.585, val_loss: 6.001\n",
      "[104] loss: 3.655, val_loss: 6.874\n",
      "[105] loss: 3.788, val_loss: 6.391\n",
      "[106] loss: 3.543, val_loss: 5.812\n",
      "[107] loss: 4.277, val_loss: 5.913\n",
      "[108] loss: 3.724, val_loss: 7.426\n",
      "[109] loss: 4.141, val_loss: 7.020\n",
      "[110] loss: 3.808, val_loss: 6.019\n",
      "[111] loss: 3.925, val_loss: 6.050\n",
      "[112] loss: 3.689, val_loss: 7.038\n",
      "[113] loss: 3.725, val_loss: 6.945\n",
      "[114] loss: 3.629, val_loss: 6.061\n",
      "[115] loss: 3.820, val_loss: 6.340\n",
      "[116] loss: 3.391, val_loss: 7.445\n",
      "[117] loss: 4.051, val_loss: 6.406\n",
      "[118] loss: 3.311, val_loss: 6.212\n",
      "[119] loss: 4.397, val_loss: 6.236\n",
      "[120] loss: 3.691, val_loss: 7.575\n",
      "[121] loss: 4.359, val_loss: 6.591\n",
      "[122] loss: 3.423, val_loss: 6.085\n",
      "[123] loss: 4.237, val_loss: 6.101\n",
      "[124] loss: 3.673, val_loss: 7.775\n",
      "[125] loss: 4.255, val_loss: 7.195\n",
      "[126] loss: 3.513, val_loss: 6.104\n",
      "[127] loss: 3.632, val_loss: 6.096\n",
      "[128] loss: 3.474, val_loss: 7.202\n",
      "[129] loss: 3.489, val_loss: 7.129\n",
      "[130] loss: 3.387, val_loss: 6.139\n",
      "[131] loss: 3.528, val_loss: 6.607\n",
      "[132] loss: 3.163, val_loss: 7.247\n",
      "[133] loss: 3.393, val_loss: 6.545\n",
      "[134] loss: 3.145, val_loss: 6.306\n",
      "[135] loss: 3.272, val_loss: 7.387\n",
      "[136] loss: 3.345, val_loss: 6.582\n",
      "[137] loss: 3.077, val_loss: 6.775\n",
      "[138] loss: 3.008, val_loss: 7.028\n",
      "[139] loss: 3.051, val_loss: 6.593\n",
      "[140] loss: 3.037, val_loss: 7.260\n",
      "[141] loss: 3.202, val_loss: 6.491\n",
      "[142] loss: 3.082, val_loss: 6.936\n",
      "[143] loss: 2.933, val_loss: 6.960\n",
      "[144] loss: 2.992, val_loss: 6.450\n",
      "[145] loss: 3.223, val_loss: 7.328\n",
      "[146] loss: 3.236, val_loss: 6.334\n",
      "[147] loss: 3.230, val_loss: 6.967\n",
      "[148] loss: 2.831, val_loss: 6.800\n",
      "[149] loss: 2.835, val_loss: 7.078\n",
      "[150] loss: 2.848, val_loss: 6.225\n",
      "[151] loss: 3.509, val_loss: 6.937\n",
      "[152] loss: 2.744, val_loss: 7.348\n",
      "[153] loss: 2.998, val_loss: 6.025\n",
      "[154] loss: 3.838, val_loss: 6.445\n",
      "[155] loss: 2.963, val_loss: 7.726\n",
      "[156] loss: 4.152, val_loss: 6.915\n",
      "[157] loss: 2.828, val_loss: 5.941\n",
      "[158] loss: 3.915, val_loss: 6.333\n",
      "[159] loss: 3.172, val_loss: 8.275\n",
      "[160] loss: 4.649, val_loss: 7.596\n",
      "[161] loss: 3.423, val_loss: 5.935\n",
      "[162] loss: 4.522, val_loss: 5.935\n",
      "[163] loss: 4.488, val_loss: 6.993\n",
      "[164] loss: 2.926, val_loss: 7.465\n",
      "[165] loss: 3.559, val_loss: 5.976\n",
      "[166] loss: 3.090, val_loss: 5.859\n",
      "[167] loss: 3.389, val_loss: 6.956\n",
      "[168] loss: 2.808, val_loss: 7.049\n",
      "[169] loss: 2.707, val_loss: 6.067\n",
      "[170] loss: 3.496, val_loss: 6.462\n",
      "[171] loss: 2.875, val_loss: 8.115\n",
      "[172] loss: 4.087, val_loss: 7.554\n",
      "[173] loss: 3.049, val_loss: 6.151\n",
      "[174] loss: 4.538, val_loss: 6.022\n",
      "[175] loss: 4.112, val_loss: 7.174\n",
      "[176] loss: 2.825, val_loss: 7.509\n",
      "[177] loss: 3.236, val_loss: 6.169\n",
      "[178] loss: 3.155, val_loss: 6.129\n",
      "[179] loss: 3.239, val_loss: 7.046\n",
      "[180] loss: 2.823, val_loss: 7.349\n",
      "[181] loss: 3.111, val_loss: 6.232\n",
      "[182] loss: 3.003, val_loss: 6.574\n",
      "[183] loss: 2.800, val_loss: 7.719\n",
      "[184] loss: 3.139, val_loss: 7.271\n",
      "[185] loss: 2.514, val_loss: 6.125\n",
      "[186] loss: 3.559, val_loss: 6.557\n",
      "[187] loss: 2.709, val_loss: 7.805\n",
      "[188] loss: 3.309, val_loss: 7.026\n",
      "[189] loss: 2.480, val_loss: 6.707\n",
      "[190] loss: 2.737, val_loss: 7.659\n",
      "[191] loss: 2.720, val_loss: 7.347\n",
      "[192] loss: 2.429, val_loss: 7.059\n",
      "[193] loss: 2.380, val_loss: 7.408\n",
      "[194] loss: 2.550, val_loss: 6.670\n",
      "[195] loss: 2.609, val_loss: 6.845\n",
      "[196] loss: 2.427, val_loss: 7.484\n",
      "[197] loss: 2.495, val_loss: 6.953\n",
      "[198] loss: 2.497, val_loss: 7.824\n",
      "[199] loss: 2.655, val_loss: 7.118\n",
      "[200] loss: 2.395, val_loss: 7.663\n",
      "[1] loss: 25.334, val_loss: 31.171\n",
      "[2] loss: 24.204, val_loss: 29.685\n",
      "[3] loss: 22.983, val_loss: 27.558\n",
      "[4] loss: 21.230, val_loss: 24.304\n",
      "[5] loss: 18.485, val_loss: 19.393\n",
      "[6] loss: 14.601, val_loss: 12.110\n",
      "[7] loss: 9.484, val_loss: 7.394\n",
      "[8] loss: 5.734, val_loss: 10.105\n",
      "[9] loss: 9.825, val_loss: 9.374\n",
      "[10] loss: 9.044, val_loss: 7.653\n",
      "[11] loss: 6.049, val_loss: 7.355\n",
      "[12] loss: 5.679, val_loss: 7.836\n",
      "[13] loss: 6.806, val_loss: 8.232\n",
      "[14] loss: 7.108, val_loss: 7.616\n",
      "[15] loss: 6.561, val_loss: 7.323\n",
      "[16] loss: 5.608, val_loss: 7.236\n",
      "[17] loss: 5.414, val_loss: 7.610\n",
      "[18] loss: 5.820, val_loss: 7.619\n",
      "[19] loss: 5.801, val_loss: 7.216\n",
      "[20] loss: 5.280, val_loss: 7.349\n",
      "[21] loss: 5.275, val_loss: 7.416\n",
      "[22] loss: 5.466, val_loss: 7.451\n",
      "[23] loss: 5.498, val_loss: 7.470\n",
      "[24] loss: 5.253, val_loss: 7.468\n",
      "[25] loss: 5.073, val_loss: 7.447\n",
      "[26] loss: 5.036, val_loss: 7.600\n",
      "[27] loss: 5.187, val_loss: 7.507\n",
      "[28] loss: 5.003, val_loss: 7.580\n",
      "[29] loss: 4.917, val_loss: 7.596\n",
      "[30] loss: 4.888, val_loss: 7.649\n",
      "[31] loss: 4.919, val_loss: 7.601\n",
      "[32] loss: 4.798, val_loss: 7.584\n",
      "[33] loss: 4.699, val_loss: 7.552\n",
      "[34] loss: 4.682, val_loss: 7.627\n",
      "[35] loss: 4.660, val_loss: 7.762\n",
      "[36] loss: 4.509, val_loss: 7.832\n",
      "[37] loss: 4.482, val_loss: 7.844\n",
      "[38] loss: 4.428, val_loss: 7.902\n",
      "[39] loss: 4.373, val_loss: 7.876\n",
      "[40] loss: 4.330, val_loss: 7.853\n",
      "[41] loss: 4.262, val_loss: 7.824\n",
      "[42] loss: 4.261, val_loss: 7.882\n",
      "[43] loss: 4.222, val_loss: 7.865\n",
      "[44] loss: 4.200, val_loss: 7.941\n",
      "[45] loss: 4.145, val_loss: 8.054\n",
      "[46] loss: 4.250, val_loss: 8.144\n",
      "[47] loss: 4.102, val_loss: 8.241\n",
      "[48] loss: 4.111, val_loss: 8.339\n",
      "[49] loss: 4.136, val_loss: 8.245\n",
      "[50] loss: 4.176, val_loss: 8.225\n",
      "[51] loss: 4.181, val_loss: 8.048\n",
      "[52] loss: 3.970, val_loss: 8.013\n",
      "[53] loss: 4.047, val_loss: 7.875\n",
      "[54] loss: 3.909, val_loss: 7.810\n",
      "[55] loss: 3.831, val_loss: 7.863\n",
      "[56] loss: 3.884, val_loss: 7.922\n",
      "[57] loss: 3.757, val_loss: 7.941\n",
      "[58] loss: 3.730, val_loss: 7.876\n",
      "[59] loss: 3.682, val_loss: 7.894\n",
      "[60] loss: 3.751, val_loss: 7.658\n",
      "[61] loss: 3.759, val_loss: 7.725\n",
      "[62] loss: 3.743, val_loss: 7.413\n",
      "[63] loss: 3.690, val_loss: 7.666\n",
      "[64] loss: 3.864, val_loss: 7.207\n",
      "[65] loss: 3.683, val_loss: 7.314\n",
      "[66] loss: 3.574, val_loss: 7.514\n",
      "[67] loss: 3.578, val_loss: 7.458\n",
      "[68] loss: 3.642, val_loss: 7.865\n",
      "[69] loss: 3.646, val_loss: 7.518\n",
      "[70] loss: 3.443, val_loss: 7.406\n",
      "[71] loss: 3.369, val_loss: 7.314\n",
      "[72] loss: 3.408, val_loss: 7.113\n",
      "[73] loss: 3.990, val_loss: 6.940\n",
      "[74] loss: 3.425, val_loss: 7.308\n",
      "[75] loss: 3.452, val_loss: 7.283\n",
      "[76] loss: 3.593, val_loss: 7.838\n",
      "[77] loss: 3.393, val_loss: 7.763\n",
      "[78] loss: 3.297, val_loss: 7.565\n",
      "[79] loss: 3.343, val_loss: 7.840\n",
      "[80] loss: 3.407, val_loss: 7.414\n",
      "[81] loss: 3.322, val_loss: 7.484\n",
      "[82] loss: 3.320, val_loss: 8.297\n",
      "[83] loss: 3.475, val_loss: 7.940\n",
      "[84] loss: 3.183, val_loss: 7.846\n",
      "[85] loss: 3.450, val_loss: 8.290\n",
      "[86] loss: 3.297, val_loss: 8.038\n",
      "[87] loss: 3.148, val_loss: 7.933\n",
      "[88] loss: 3.138, val_loss: 7.815\n",
      "[89] loss: 3.087, val_loss: 7.942\n",
      "[90] loss: 3.005, val_loss: 7.549\n",
      "[91] loss: 3.271, val_loss: 8.569\n",
      "[92] loss: 3.631, val_loss: 8.150\n",
      "[93] loss: 3.166, val_loss: 7.611\n",
      "[94] loss: 4.170, val_loss: 8.162\n",
      "[95] loss: 2.992, val_loss: 8.317\n",
      "[96] loss: 3.114, val_loss: 7.783\n",
      "[97] loss: 3.799, val_loss: 8.386\n",
      "[98] loss: 2.956, val_loss: 8.917\n",
      "[99] loss: 3.263, val_loss: 8.461\n",
      "[100] loss: 3.111, val_loss: 8.524\n",
      "[101] loss: 2.921, val_loss: 9.026\n",
      "[102] loss: 3.338, val_loss: 8.053\n",
      "[103] loss: 3.195, val_loss: 8.494\n",
      "[104] loss: 2.888, val_loss: 8.537\n",
      "[105] loss: 2.902, val_loss: 8.305\n",
      "[106] loss: 2.988, val_loss: 9.205\n",
      "[107] loss: 3.367, val_loss: 8.780\n",
      "[108] loss: 2.846, val_loss: 8.288\n",
      "[109] loss: 3.173, val_loss: 8.909\n",
      "[110] loss: 3.065, val_loss: 8.750\n",
      "[111] loss: 2.816, val_loss: 8.031\n",
      "[112] loss: 3.950, val_loss: 8.448\n",
      "[113] loss: 2.886, val_loss: 10.153\n",
      "[114] loss: 4.302, val_loss: 8.960\n",
      "[115] loss: 2.853, val_loss: 8.540\n",
      "[116] loss: 4.605, val_loss: 8.596\n",
      "[117] loss: 3.588, val_loss: 10.527\n",
      "[118] loss: 4.413, val_loss: 10.332\n",
      "[119] loss: 4.155, val_loss: 9.087\n",
      "[120] loss: 3.246, val_loss: 8.670\n",
      "[121] loss: 4.060, val_loss: 9.083\n",
      "[122] loss: 2.993, val_loss: 9.796\n",
      "[123] loss: 3.445, val_loss: 8.468\n",
      "[124] loss: 2.793, val_loss: 7.667\n",
      "[125] loss: 3.457, val_loss: 8.000\n",
      "[126] loss: 2.808, val_loss: 9.426\n",
      "[127] loss: 3.607, val_loss: 8.340\n",
      "[128] loss: 2.649, val_loss: 7.975\n",
      "[129] loss: 3.335, val_loss: 8.935\n",
      "[130] loss: 2.636, val_loss: 9.145\n",
      "[131] loss: 2.678, val_loss: 8.535\n",
      "[132] loss: 3.079, val_loss: 9.407\n",
      "[133] loss: 2.674, val_loss: 9.532\n",
      "[134] loss: 2.705, val_loss: 8.467\n",
      "[135] loss: 3.427, val_loss: 8.844\n",
      "[136] loss: 2.680, val_loss: 10.255\n",
      "[137] loss: 3.608, val_loss: 9.280\n",
      "[138] loss: 2.595, val_loss: 8.180\n",
      "[139] loss: 3.909, val_loss: 8.313\n",
      "[140] loss: 3.328, val_loss: 9.918\n",
      "[141] loss: 3.312, val_loss: 9.675\n",
      "[142] loss: 2.991, val_loss: 8.486\n",
      "[143] loss: 3.408, val_loss: 8.664\n",
      "[144] loss: 3.036, val_loss: 9.819\n",
      "[145] loss: 3.049, val_loss: 9.449\n",
      "[146] loss: 2.593, val_loss: 8.551\n",
      "[147] loss: 3.030, val_loss: 8.928\n",
      "[148] loss: 2.479, val_loss: 9.837\n",
      "[149] loss: 2.929, val_loss: 8.871\n",
      "[150] loss: 2.540, val_loss: 8.827\n",
      "[151] loss: 2.429, val_loss: 9.366\n",
      "[152] loss: 2.538, val_loss: 8.750\n",
      "[153] loss: 2.409, val_loss: 9.169\n",
      "[154] loss: 2.463, val_loss: 8.793\n",
      "[155] loss: 2.336, val_loss: 9.137\n",
      "[156] loss: 2.327, val_loss: 8.753\n",
      "[157] loss: 2.501, val_loss: 9.405\n",
      "[158] loss: 2.759, val_loss: 8.773\n",
      "[159] loss: 2.376, val_loss: 9.072\n",
      "[160] loss: 2.345, val_loss: 8.485\n",
      "[161] loss: 2.602, val_loss: 9.121\n",
      "[162] loss: 2.730, val_loss: 8.659\n",
      "[163] loss: 2.266, val_loss: 8.850\n",
      "[164] loss: 2.147, val_loss: 8.906\n",
      "[165] loss: 2.091, val_loss: 9.249\n",
      "[166] loss: 2.449, val_loss: 8.525\n",
      "[167] loss: 2.823, val_loss: 9.139\n",
      "[168] loss: 2.176, val_loss: 8.995\n",
      "[169] loss: 2.167, val_loss: 8.269\n",
      "[170] loss: 2.926, val_loss: 8.967\n",
      "[171] loss: 2.133, val_loss: 9.583\n",
      "[172] loss: 2.834, val_loss: 8.544\n",
      "[173] loss: 3.334, val_loss: 9.112\n",
      "[174] loss: 2.560, val_loss: 10.350\n",
      "[175] loss: 3.178, val_loss: 9.718\n",
      "[176] loss: 2.308, val_loss: 9.005\n",
      "[177] loss: 2.797, val_loss: 9.302\n",
      "[178] loss: 2.135, val_loss: 10.102\n",
      "[179] loss: 3.044, val_loss: 9.096\n",
      "[180] loss: 2.052, val_loss: 8.152\n",
      "[181] loss: 3.456, val_loss: 8.208\n",
      "[182] loss: 2.288, val_loss: 10.479\n",
      "[183] loss: 3.833, val_loss: 9.024\n",
      "[184] loss: 2.500, val_loss: 8.113\n",
      "[185] loss: 3.692, val_loss: 8.134\n",
      "[186] loss: 3.206, val_loss: 9.753\n",
      "[187] loss: 2.974, val_loss: 9.761\n",
      "[188] loss: 2.826, val_loss: 8.805\n",
      "[189] loss: 3.163, val_loss: 8.899\n",
      "[190] loss: 2.909, val_loss: 9.866\n",
      "[191] loss: 2.471, val_loss: 9.729\n",
      "[192] loss: 2.250, val_loss: 8.946\n",
      "[193] loss: 2.370, val_loss: 9.249\n",
      "[194] loss: 2.045, val_loss: 9.755\n",
      "[195] loss: 2.537, val_loss: 8.251\n",
      "[196] loss: 2.361, val_loss: 8.471\n",
      "[197] loss: 2.120, val_loss: 9.133\n",
      "[198] loss: 2.304, val_loss: 8.516\n",
      "[199] loss: 2.033, val_loss: 9.193\n",
      "[200] loss: 2.036, val_loss: 9.072\n",
      "[1] loss: 26.606, val_loss: 26.568\n",
      "[2] loss: 25.456, val_loss: 25.342\n",
      "[3] loss: 24.113, val_loss: 23.415\n",
      "[4] loss: 22.045, val_loss: 20.487\n",
      "[5] loss: 18.846, val_loss: 16.096\n",
      "[6] loss: 14.298, val_loss: 9.944\n",
      "[7] loss: 8.566, val_loss: 4.374\n",
      "[8] loss: 6.235, val_loss: 6.699\n",
      "[9] loss: 10.916, val_loss: 5.213\n",
      "[10] loss: 9.466, val_loss: 4.264\n",
      "[11] loss: 6.458, val_loss: 6.519\n",
      "[12] loss: 5.861, val_loss: 8.351\n",
      "[13] loss: 6.964, val_loss: 8.752\n",
      "[14] loss: 7.257, val_loss: 7.915\n",
      "[15] loss: 6.497, val_loss: 6.426\n",
      "[16] loss: 5.717, val_loss: 5.289\n",
      "[17] loss: 5.713, val_loss: 4.598\n",
      "[18] loss: 6.016, val_loss: 4.519\n",
      "[19] loss: 6.032, val_loss: 5.054\n",
      "[20] loss: 5.722, val_loss: 5.942\n",
      "[21] loss: 5.508, val_loss: 6.628\n",
      "[22] loss: 5.597, val_loss: 6.907\n",
      "[23] loss: 5.625, val_loss: 6.850\n",
      "[24] loss: 5.534, val_loss: 6.609\n",
      "[25] loss: 5.384, val_loss: 6.176\n",
      "[26] loss: 5.339, val_loss: 5.737\n",
      "[27] loss: 5.277, val_loss: 5.548\n",
      "[28] loss: 5.287, val_loss: 5.895\n",
      "[29] loss: 5.216, val_loss: 6.121\n",
      "[30] loss: 5.221, val_loss: 6.252\n",
      "[31] loss: 5.168, val_loss: 6.364\n",
      "[32] loss: 5.092, val_loss: 6.347\n",
      "[33] loss: 5.087, val_loss: 6.097\n",
      "[34] loss: 4.995, val_loss: 5.972\n",
      "[35] loss: 4.931, val_loss: 5.732\n",
      "[36] loss: 4.916, val_loss: 5.659\n",
      "[37] loss: 4.850, val_loss: 5.914\n",
      "[38] loss: 4.733, val_loss: 6.221\n",
      "[39] loss: 4.724, val_loss: 6.283\n",
      "[40] loss: 4.708, val_loss: 6.065\n",
      "[41] loss: 4.593, val_loss: 5.686\n",
      "[42] loss: 4.572, val_loss: 5.646\n",
      "[43] loss: 4.490, val_loss: 5.868\n",
      "[44] loss: 4.438, val_loss: 5.911\n",
      "[45] loss: 4.423, val_loss: 5.595\n",
      "[46] loss: 4.418, val_loss: 5.844\n",
      "[47] loss: 4.317, val_loss: 5.406\n",
      "[48] loss: 4.370, val_loss: 5.822\n",
      "[49] loss: 4.352, val_loss: 5.376\n",
      "[50] loss: 4.238, val_loss: 5.298\n",
      "[51] loss: 4.248, val_loss: 5.796\n",
      "[52] loss: 4.208, val_loss: 4.941\n",
      "[53] loss: 4.288, val_loss: 5.359\n",
      "[54] loss: 4.115, val_loss: 5.797\n",
      "[55] loss: 4.232, val_loss: 5.137\n",
      "[56] loss: 4.155, val_loss: 5.021\n",
      "[57] loss: 4.145, val_loss: 5.716\n",
      "[58] loss: 4.175, val_loss: 5.136\n",
      "[59] loss: 3.978, val_loss: 4.771\n",
      "[60] loss: 4.058, val_loss: 5.249\n",
      "[61] loss: 3.920, val_loss: 5.143\n",
      "[62] loss: 3.892, val_loss: 4.852\n",
      "[63] loss: 3.882, val_loss: 5.481\n",
      "[64] loss: 4.011, val_loss: 4.742\n",
      "[65] loss: 3.875, val_loss: 4.841\n",
      "[66] loss: 3.791, val_loss: 5.175\n",
      "[67] loss: 3.776, val_loss: 4.689\n",
      "[68] loss: 3.767, val_loss: 5.117\n",
      "[69] loss: 3.756, val_loss: 5.163\n",
      "[70] loss: 3.817, val_loss: 4.533\n",
      "[71] loss: 4.092, val_loss: 5.074\n",
      "[72] loss: 3.717, val_loss: 4.876\n",
      "[73] loss: 3.657, val_loss: 4.598\n",
      "[74] loss: 3.770, val_loss: 5.163\n",
      "[75] loss: 3.693, val_loss: 4.699\n",
      "[76] loss: 3.591, val_loss: 4.903\n",
      "[77] loss: 3.523, val_loss: 4.684\n",
      "[78] loss: 3.636, val_loss: 5.014\n",
      "[79] loss: 3.689, val_loss: 4.638\n",
      "[80] loss: 3.673, val_loss: 4.808\n",
      "[81] loss: 3.436, val_loss: 4.849\n",
      "[82] loss: 3.485, val_loss: 4.716\n",
      "[83] loss: 3.822, val_loss: 4.979\n",
      "[84] loss: 3.679, val_loss: 4.679\n",
      "[85] loss: 3.447, val_loss: 5.005\n",
      "[86] loss: 3.480, val_loss: 4.829\n",
      "[87] loss: 3.980, val_loss: 5.042\n",
      "[88] loss: 3.527, val_loss: 4.846\n",
      "[89] loss: 3.446, val_loss: 4.803\n",
      "[90] loss: 3.312, val_loss: 5.101\n",
      "[91] loss: 3.648, val_loss: 4.845\n",
      "[92] loss: 3.900, val_loss: 5.101\n",
      "[93] loss: 3.540, val_loss: 4.888\n",
      "[94] loss: 3.371, val_loss: 4.841\n",
      "[95] loss: 3.290, val_loss: 4.888\n",
      "[96] loss: 3.299, val_loss: 4.878\n",
      "[97] loss: 3.560, val_loss: 4.990\n",
      "[98] loss: 3.307, val_loss: 4.935\n",
      "[99] loss: 3.454, val_loss: 5.145\n",
      "[100] loss: 3.476, val_loss: 4.937\n",
      "[101] loss: 3.250, val_loss: 4.908\n",
      "[102] loss: 3.159, val_loss: 5.019\n",
      "[103] loss: 3.308, val_loss: 4.923\n",
      "[104] loss: 3.295, val_loss: 5.126\n",
      "[105] loss: 3.741, val_loss: 4.973\n",
      "[106] loss: 3.198, val_loss: 4.990\n",
      "[107] loss: 3.410, val_loss: 5.274\n",
      "[108] loss: 3.816, val_loss: 5.125\n",
      "[109] loss: 3.169, val_loss: 5.161\n",
      "[110] loss: 3.087, val_loss: 5.114\n",
      "[111] loss: 3.072, val_loss: 5.106\n",
      "[112] loss: 3.581, val_loss: 5.198\n",
      "[113] loss: 3.724, val_loss: 5.010\n",
      "[114] loss: 3.183, val_loss: 5.082\n",
      "[115] loss: 4.080, val_loss: 5.099\n",
      "[116] loss: 3.280, val_loss: 5.015\n",
      "[117] loss: 2.986, val_loss: 5.085\n",
      "[118] loss: 2.970, val_loss: 5.108\n",
      "[119] loss: 3.055, val_loss: 5.229\n",
      "[120] loss: 3.601, val_loss: 5.097\n",
      "[121] loss: 3.045, val_loss: 5.126\n",
      "[122] loss: 2.952, val_loss: 5.186\n",
      "[123] loss: 3.114, val_loss: 5.156\n",
      "[124] loss: 3.561, val_loss: 5.315\n",
      "[125] loss: 3.409, val_loss: 5.122\n",
      "[126] loss: 3.022, val_loss: 5.190\n",
      "[127] loss: 2.964, val_loss: 5.143\n",
      "[128] loss: 3.268, val_loss: 5.249\n",
      "[129] loss: 3.250, val_loss: 5.119\n",
      "[130] loss: 3.228, val_loss: 5.270\n",
      "[131] loss: 2.977, val_loss: 5.241\n",
      "[132] loss: 2.869, val_loss: 5.237\n",
      "[133] loss: 3.018, val_loss: 5.321\n",
      "[134] loss: 3.342, val_loss: 5.428\n",
      "[135] loss: 3.421, val_loss: 5.232\n",
      "[136] loss: 2.895, val_loss: 5.247\n",
      "[137] loss: 3.176, val_loss: 5.251\n",
      "[138] loss: 3.401, val_loss: 5.227\n",
      "[139] loss: 3.067, val_loss: 5.294\n",
      "[140] loss: 3.077, val_loss: 5.362\n",
      "[141] loss: 3.179, val_loss: 5.443\n",
      "[142] loss: 3.229, val_loss: 5.412\n",
      "[143] loss: 3.136, val_loss: 5.329\n",
      "[144] loss: 2.926, val_loss: 5.369\n",
      "[145] loss: 4.132, val_loss: 5.367\n",
      "[146] loss: 2.884, val_loss: 5.344\n",
      "[147] loss: 3.680, val_loss: 5.420\n",
      "[148] loss: 2.898, val_loss: 5.354\n",
      "[149] loss: 2.759, val_loss: 5.461\n",
      "[150] loss: 2.680, val_loss: 5.388\n",
      "[151] loss: 2.734, val_loss: 5.419\n",
      "[152] loss: 2.824, val_loss: 5.407\n",
      "[153] loss: 3.574, val_loss: 5.474\n",
      "[154] loss: 2.955, val_loss: 5.510\n",
      "[155] loss: 2.684, val_loss: 5.514\n",
      "[156] loss: 2.691, val_loss: 5.423\n",
      "[157] loss: 3.571, val_loss: 5.485\n",
      "[158] loss: 2.788, val_loss: 5.417\n",
      "[159] loss: 2.695, val_loss: 5.402\n",
      "[160] loss: 2.949, val_loss: 5.440\n",
      "[161] loss: 3.495, val_loss: 5.444\n",
      "[162] loss: 2.700, val_loss: 5.427\n",
      "[163] loss: 2.677, val_loss: 5.511\n",
      "[164] loss: 4.260, val_loss: 5.575\n",
      "[165] loss: 3.170, val_loss: 5.878\n",
      "[166] loss: 4.414, val_loss: 5.796\n",
      "[167] loss: 4.144, val_loss: 5.533\n",
      "[168] loss: 3.256, val_loss: 5.561\n",
      "[169] loss: 3.031, val_loss: 5.634\n",
      "[170] loss: 3.714, val_loss: 5.474\n",
      "[171] loss: 3.030, val_loss: 5.609\n",
      "[172] loss: 4.028, val_loss: 5.655\n",
      "[173] loss: 3.232, val_loss: 5.609\n",
      "[174] loss: 3.903, val_loss: 5.680\n",
      "[175] loss: 3.921, val_loss: 5.503\n",
      "[176] loss: 2.938, val_loss: 5.668\n",
      "[177] loss: 3.264, val_loss: 5.552\n",
      "[178] loss: 3.024, val_loss: 5.630\n",
      "[179] loss: 2.805, val_loss: 5.634\n",
      "[180] loss: 2.956, val_loss: 5.586\n",
      "[181] loss: 2.612, val_loss: 5.544\n",
      "[182] loss: 2.874, val_loss: 5.574\n",
      "[183] loss: 3.096, val_loss: 5.581\n",
      "[184] loss: 2.683, val_loss: 5.574\n",
      "[185] loss: 3.529, val_loss: 5.560\n",
      "[186] loss: 2.625, val_loss: 5.719\n",
      "[187] loss: 3.740, val_loss: 5.716\n",
      "[188] loss: 2.634, val_loss: 5.946\n",
      "[189] loss: 4.328, val_loss: 5.770\n",
      "[190] loss: 3.735, val_loss: 5.710\n",
      "[191] loss: 3.086, val_loss: 5.618\n",
      "[192] loss: 2.999, val_loss: 5.549\n",
      "[193] loss: 3.518, val_loss: 5.519\n",
      "[194] loss: 3.207, val_loss: 5.664\n",
      "[195] loss: 2.936, val_loss: 5.524\n",
      "[196] loss: 2.702, val_loss: 5.553\n",
      "[197] loss: 3.394, val_loss: 5.649\n",
      "[198] loss: 2.936, val_loss: 5.737\n",
      "[199] loss: 3.573, val_loss: 5.756\n",
      "[200] loss: 2.887, val_loss: 5.772\n",
      "[1] loss: 27.166, val_loss: 22.837\n",
      "[2] loss: 25.998, val_loss: 21.422\n",
      "[3] loss: 24.595, val_loss: 19.290\n",
      "[4] loss: 22.504, val_loss: 16.031\n",
      "[5] loss: 19.286, val_loss: 11.120\n",
      "[6] loss: 14.714, val_loss: 5.928\n",
      "[7] loss: 8.759, val_loss: 6.424\n",
      "[8] loss: 5.996, val_loss: 12.878\n",
      "[9] loss: 9.441, val_loss: 10.674\n",
      "[10] loss: 7.804, val_loss: 5.768\n",
      "[11] loss: 6.003, val_loss: 4.046\n",
      "[12] loss: 6.345, val_loss: 4.662\n",
      "[13] loss: 6.942, val_loss: 4.923\n",
      "[14] loss: 7.137, val_loss: 4.563\n",
      "[15] loss: 6.653, val_loss: 4.358\n",
      "[16] loss: 5.970, val_loss: 5.264\n",
      "[17] loss: 5.703, val_loss: 6.904\n",
      "[18] loss: 5.883, val_loss: 7.697\n",
      "[19] loss: 6.139, val_loss: 6.279\n",
      "[20] loss: 5.570, val_loss: 5.197\n",
      "[21] loss: 5.461, val_loss: 4.770\n",
      "[22] loss: 5.527, val_loss: 4.836\n",
      "[23] loss: 5.534, val_loss: 5.019\n",
      "[24] loss: 5.346, val_loss: 5.298\n",
      "[25] loss: 5.226, val_loss: 5.807\n",
      "[26] loss: 5.225, val_loss: 5.761\n",
      "[27] loss: 5.123, val_loss: 5.384\n",
      "[28] loss: 5.044, val_loss: 5.559\n",
      "[29] loss: 5.123, val_loss: 5.540\n",
      "[30] loss: 4.896, val_loss: 5.622\n",
      "[31] loss: 4.849, val_loss: 5.641\n",
      "[32] loss: 4.825, val_loss: 5.827\n",
      "[33] loss: 4.783, val_loss: 5.838\n",
      "[34] loss: 4.639, val_loss: 5.831\n",
      "[35] loss: 4.643, val_loss: 5.962\n",
      "[36] loss: 4.560, val_loss: 6.078\n",
      "[37] loss: 4.564, val_loss: 6.064\n",
      "[38] loss: 4.494, val_loss: 5.959\n",
      "[39] loss: 4.555, val_loss: 6.241\n",
      "[40] loss: 4.491, val_loss: 6.239\n",
      "[41] loss: 4.366, val_loss: 6.221\n",
      "[42] loss: 4.380, val_loss: 6.329\n",
      "[43] loss: 4.322, val_loss: 6.258\n",
      "[44] loss: 4.265, val_loss: 6.188\n",
      "[45] loss: 4.346, val_loss: 6.334\n",
      "[46] loss: 4.279, val_loss: 6.323\n",
      "[47] loss: 4.146, val_loss: 6.266\n",
      "[48] loss: 4.162, val_loss: 6.290\n",
      "[49] loss: 4.113, val_loss: 6.457\n",
      "[50] loss: 4.156, val_loss: 6.361\n",
      "[51] loss: 4.115, val_loss: 6.351\n",
      "[52] loss: 4.021, val_loss: 6.511\n",
      "[53] loss: 4.076, val_loss: 6.374\n",
      "[54] loss: 4.107, val_loss: 6.386\n",
      "[55] loss: 3.955, val_loss: 6.726\n",
      "[56] loss: 4.272, val_loss: 6.650\n",
      "[57] loss: 4.025, val_loss: 6.452\n",
      "[58] loss: 4.401, val_loss: 6.464\n",
      "[59] loss: 3.857, val_loss: 6.701\n",
      "[60] loss: 3.960, val_loss: 6.553\n",
      "[61] loss: 3.863, val_loss: 6.432\n",
      "[62] loss: 3.965, val_loss: 6.726\n",
      "[63] loss: 4.039, val_loss: 6.429\n",
      "[64] loss: 3.807, val_loss: 6.370\n",
      "[65] loss: 3.921, val_loss: 6.511\n",
      "[66] loss: 3.882, val_loss: 6.433\n",
      "[67] loss: 3.718, val_loss: 6.405\n",
      "[68] loss: 4.122, val_loss: 6.631\n",
      "[69] loss: 3.910, val_loss: 6.450\n",
      "[70] loss: 3.730, val_loss: 6.555\n",
      "[71] loss: 3.621, val_loss: 6.534\n",
      "[72] loss: 3.622, val_loss: 6.717\n",
      "[73] loss: 3.581, val_loss: 6.586\n",
      "[74] loss: 3.827, val_loss: 6.789\n",
      "[75] loss: 3.634, val_loss: 6.388\n",
      "[76] loss: 3.905, val_loss: 6.526\n",
      "[77] loss: 3.587, val_loss: 6.620\n",
      "[78] loss: 3.490, val_loss: 6.535\n",
      "[79] loss: 3.684, val_loss: 6.870\n",
      "[80] loss: 3.894, val_loss: 6.447\n",
      "[81] loss: 3.995, val_loss: 6.590\n",
      "[82] loss: 3.639, val_loss: 6.961\n",
      "[83] loss: 4.617, val_loss: 6.687\n",
      "[84] loss: 3.715, val_loss: 7.605\n",
      "[85] loss: 5.463, val_loss: 7.104\n",
      "[86] loss: 4.936, val_loss: 6.341\n",
      "[87] loss: 3.794, val_loss: 6.554\n",
      "[88] loss: 4.359, val_loss: 6.055\n",
      "[89] loss: 3.676, val_loss: 6.236\n",
      "[90] loss: 4.228, val_loss: 6.334\n",
      "[91] loss: 3.620, val_loss: 6.610\n",
      "[92] loss: 3.792, val_loss: 6.302\n",
      "[93] loss: 3.778, val_loss: 6.404\n",
      "[94] loss: 3.478, val_loss: 6.682\n",
      "[95] loss: 3.548, val_loss: 6.515\n",
      "[96] loss: 3.608, val_loss: 6.672\n",
      "[97] loss: 3.431, val_loss: 6.651\n",
      "[98] loss: 3.340, val_loss: 6.693\n",
      "[99] loss: 3.346, val_loss: 6.753\n",
      "[100] loss: 3.362, val_loss: 6.724\n",
      "[101] loss: 3.984, val_loss: 6.709\n",
      "[102] loss: 3.379, val_loss: 7.257\n",
      "[103] loss: 4.869, val_loss: 7.056\n",
      "[104] loss: 4.235, val_loss: 6.814\n",
      "[105] loss: 4.262, val_loss: 6.919\n",
      "[106] loss: 4.439, val_loss: 6.780\n",
      "[107] loss: 3.648, val_loss: 7.011\n",
      "[108] loss: 4.084, val_loss: 6.538\n",
      "[109] loss: 3.729, val_loss: 6.587\n",
      "[110] loss: 3.514, val_loss: 6.941\n",
      "[111] loss: 3.956, val_loss: 6.686\n",
      "[112] loss: 3.360, val_loss: 6.607\n",
      "[113] loss: 3.933, val_loss: 6.863\n",
      "[114] loss: 3.343, val_loss: 7.271\n",
      "[115] loss: 4.345, val_loss: 6.988\n",
      "[116] loss: 3.460, val_loss: 7.097\n",
      "[117] loss: 4.832, val_loss: 6.969\n",
      "[118] loss: 4.346, val_loss: 7.075\n",
      "[119] loss: 3.660, val_loss: 7.010\n",
      "[120] loss: 3.594, val_loss: 6.797\n",
      "[121] loss: 4.168, val_loss: 6.610\n",
      "[122] loss: 3.812, val_loss: 6.916\n",
      "[123] loss: 3.887, val_loss: 6.822\n",
      "[124] loss: 3.751, val_loss: 6.335\n",
      "[125] loss: 3.712, val_loss: 6.350\n",
      "[126] loss: 3.844, val_loss: 6.570\n",
      "[127] loss: 3.548, val_loss: 6.891\n",
      "[128] loss: 3.953, val_loss: 6.603\n",
      "[129] loss: 3.327, val_loss: 6.602\n",
      "[130] loss: 3.630, val_loss: 6.877\n",
      "[131] loss: 3.276, val_loss: 6.856\n",
      "[132] loss: 3.189, val_loss: 7.136\n",
      "[133] loss: 3.381, val_loss: 6.955\n",
      "[134] loss: 3.771, val_loss: 6.894\n",
      "[135] loss: 3.312, val_loss: 7.300\n",
      "[136] loss: 4.310, val_loss: 7.083\n",
      "[137] loss: 3.679, val_loss: 6.846\n",
      "[138] loss: 4.382, val_loss: 6.613\n",
      "[139] loss: 4.017, val_loss: 6.609\n",
      "[140] loss: 3.619, val_loss: 6.747\n",
      "[141] loss: 4.002, val_loss: 6.333\n",
      "[142] loss: 3.383, val_loss: 6.501\n",
      "[143] loss: 4.115, val_loss: 6.413\n",
      "[144] loss: 3.304, val_loss: 7.061\n",
      "[145] loss: 3.955, val_loss: 6.999\n",
      "[146] loss: 3.530, val_loss: 6.701\n",
      "[147] loss: 3.990, val_loss: 6.693\n",
      "[148] loss: 3.628, val_loss: 7.162\n",
      "[149] loss: 3.647, val_loss: 7.055\n",
      "[150] loss: 3.516, val_loss: 6.681\n",
      "[151] loss: 3.719, val_loss: 6.642\n",
      "[152] loss: 3.399, val_loss: 7.151\n",
      "[153] loss: 3.791, val_loss: 6.972\n",
      "[154] loss: 3.346, val_loss: 6.668\n",
      "[155] loss: 4.028, val_loss: 6.497\n",
      "[156] loss: 3.552, val_loss: 6.958\n",
      "[157] loss: 3.609, val_loss: 6.856\n",
      "[158] loss: 3.371, val_loss: 6.594\n",
      "[159] loss: 3.596, val_loss: 6.617\n",
      "[160] loss: 3.264, val_loss: 6.975\n",
      "[161] loss: 3.418, val_loss: 6.845\n",
      "[162] loss: 3.172, val_loss: 6.796\n",
      "[163] loss: 3.129, val_loss: 7.158\n",
      "[164] loss: 3.084, val_loss: 7.236\n",
      "[165] loss: 3.113, val_loss: 7.017\n",
      "[166] loss: 3.641, val_loss: 7.013\n",
      "[167] loss: 3.159, val_loss: 7.424\n",
      "[168] loss: 4.263, val_loss: 7.148\n",
      "[169] loss: 3.727, val_loss: 6.969\n",
      "[170] loss: 3.897, val_loss: 6.881\n",
      "[171] loss: 3.815, val_loss: 6.887\n",
      "[172] loss: 3.422, val_loss: 7.070\n",
      "[173] loss: 3.694, val_loss: 6.835\n",
      "[174] loss: 3.137, val_loss: 6.948\n",
      "[175] loss: 3.599, val_loss: 7.187\n",
      "[176] loss: 2.999, val_loss: 7.413\n",
      "[177] loss: 3.485, val_loss: 7.147\n",
      "[178] loss: 3.220, val_loss: 7.238\n",
      "[179] loss: 3.071, val_loss: 7.414\n",
      "[180] loss: 3.360, val_loss: 7.097\n",
      "[181] loss: 3.153, val_loss: 7.166\n",
      "[182] loss: 2.957, val_loss: 7.312\n",
      "[183] loss: 3.159, val_loss: 6.994\n",
      "[184] loss: 3.143, val_loss: 7.267\n",
      "[185] loss: 2.984, val_loss: 7.128\n",
      "[186] loss: 2.944, val_loss: 7.386\n",
      "[187] loss: 3.219, val_loss: 7.143\n",
      "[188] loss: 3.405, val_loss: 7.093\n",
      "[189] loss: 2.885, val_loss: 7.396\n",
      "[190] loss: 3.237, val_loss: 7.205\n",
      "[191] loss: 3.110, val_loss: 7.274\n",
      "[192] loss: 2.850, val_loss: 7.306\n",
      "[193] loss: 2.926, val_loss: 7.164\n",
      "[194] loss: 3.196, val_loss: 7.352\n",
      "[195] loss: 3.109, val_loss: 7.235\n",
      "[196] loss: 2.851, val_loss: 7.318\n",
      "[197] loss: 2.776, val_loss: 7.553\n",
      "[198] loss: 2.939, val_loss: 7.547\n",
      "[199] loss: 3.807, val_loss: 7.402\n",
      "[200] loss: 3.181, val_loss: 7.746\n",
      "[1] loss: 27.033, val_loss: 24.860\n",
      "[2] loss: 26.146, val_loss: 23.925\n",
      "[3] loss: 25.188, val_loss: 22.524\n",
      "[4] loss: 23.773, val_loss: 20.321\n",
      "[5] loss: 21.566, val_loss: 16.990\n",
      "[6] loss: 18.182, val_loss: 13.446\n",
      "[7] loss: 13.213, val_loss: 9.111\n",
      "[8] loss: 6.884, val_loss: 7.729\n",
      "[9] loss: 6.700, val_loss: 10.996\n",
      "[10] loss: 9.721, val_loss: 8.565\n",
      "[11] loss: 7.556, val_loss: 7.172\n",
      "[12] loss: 5.245, val_loss: 7.903\n",
      "[13] loss: 5.793, val_loss: 8.983\n",
      "[14] loss: 6.756, val_loss: 9.102\n",
      "[15] loss: 6.781, val_loss: 8.421\n",
      "[16] loss: 5.929, val_loss: 7.586\n",
      "[17] loss: 5.377, val_loss: 7.118\n",
      "[18] loss: 5.342, val_loss: 7.340\n",
      "[19] loss: 5.638, val_loss: 7.315\n",
      "[20] loss: 5.575, val_loss: 7.102\n",
      "[21] loss: 5.191, val_loss: 7.403\n",
      "[22] loss: 5.021, val_loss: 7.624\n",
      "[23] loss: 5.110, val_loss: 7.661\n",
      "[24] loss: 5.122, val_loss: 7.574\n",
      "[25] loss: 4.896, val_loss: 7.399\n",
      "[26] loss: 4.766, val_loss: 7.325\n",
      "[27] loss: 4.903, val_loss: 7.298\n",
      "[28] loss: 4.850, val_loss: 7.415\n",
      "[29] loss: 4.687, val_loss: 7.571\n",
      "[30] loss: 4.690, val_loss: 7.647\n",
      "[31] loss: 4.713, val_loss: 7.566\n",
      "[32] loss: 4.620, val_loss: 7.484\n",
      "[33] loss: 4.579, val_loss: 7.494\n",
      "[34] loss: 4.551, val_loss: 7.647\n",
      "[35] loss: 4.516, val_loss: 7.707\n",
      "[36] loss: 4.483, val_loss: 7.530\n",
      "[37] loss: 4.452, val_loss: 7.575\n",
      "[38] loss: 4.396, val_loss: 7.712\n",
      "[39] loss: 4.348, val_loss: 7.752\n",
      "[40] loss: 4.338, val_loss: 7.750\n",
      "[41] loss: 4.290, val_loss: 7.738\n",
      "[42] loss: 4.287, val_loss: 7.762\n",
      "[43] loss: 4.230, val_loss: 7.877\n",
      "[44] loss: 4.206, val_loss: 7.889\n",
      "[45] loss: 4.161, val_loss: 7.857\n",
      "[46] loss: 4.160, val_loss: 8.033\n",
      "[47] loss: 4.143, val_loss: 7.819\n",
      "[48] loss: 4.194, val_loss: 7.862\n",
      "[49] loss: 4.145, val_loss: 8.043\n",
      "[50] loss: 3.974, val_loss: 8.141\n",
      "[51] loss: 4.185, val_loss: 7.824\n",
      "[52] loss: 4.110, val_loss: 7.775\n",
      "[53] loss: 4.218, val_loss: 7.937\n",
      "[54] loss: 3.955, val_loss: 8.170\n",
      "[55] loss: 4.357, val_loss: 7.820\n",
      "[56] loss: 3.951, val_loss: 7.701\n",
      "[57] loss: 4.148, val_loss: 7.715\n",
      "[58] loss: 3.919, val_loss: 8.044\n",
      "[59] loss: 3.975, val_loss: 7.855\n",
      "[60] loss: 3.782, val_loss: 7.807\n",
      "[61] loss: 3.776, val_loss: 7.808\n",
      "[62] loss: 3.746, val_loss: 7.962\n",
      "[63] loss: 3.620, val_loss: 8.034\n",
      "[64] loss: 3.608, val_loss: 7.831\n",
      "[65] loss: 3.725, val_loss: 7.997\n",
      "[66] loss: 3.548, val_loss: 8.096\n",
      "[67] loss: 3.876, val_loss: 7.791\n",
      "[68] loss: 3.622, val_loss: 7.737\n",
      "[69] loss: 3.578, val_loss: 7.860\n",
      "[70] loss: 3.585, val_loss: 7.699\n",
      "[71] loss: 3.413, val_loss: 7.720\n",
      "[72] loss: 3.468, val_loss: 7.971\n",
      "[73] loss: 3.594, val_loss: 7.779\n",
      "[74] loss: 3.550, val_loss: 7.755\n",
      "[75] loss: 3.317, val_loss: 7.881\n",
      "[76] loss: 3.483, val_loss: 7.769\n",
      "[77] loss: 3.790, val_loss: 7.564\n",
      "[78] loss: 3.420, val_loss: 7.885\n",
      "[79] loss: 4.039, val_loss: 7.623\n",
      "[80] loss: 3.391, val_loss: 7.612\n",
      "[81] loss: 3.830, val_loss: 7.472\n",
      "[82] loss: 3.349, val_loss: 7.690\n",
      "[83] loss: 3.544, val_loss: 7.468\n",
      "[84] loss: 3.190, val_loss: 7.527\n",
      "[85] loss: 3.260, val_loss: 7.710\n",
      "[86] loss: 3.382, val_loss: 7.577\n",
      "[87] loss: 3.172, val_loss: 7.600\n",
      "[88] loss: 3.236, val_loss: 7.720\n",
      "[89] loss: 3.268, val_loss: 7.528\n",
      "[90] loss: 3.290, val_loss: 7.517\n",
      "[91] loss: 3.095, val_loss: 7.526\n",
      "[92] loss: 3.119, val_loss: 7.450\n",
      "[93] loss: 3.175, val_loss: 7.561\n",
      "[94] loss: 3.103, val_loss: 7.387\n",
      "[95] loss: 3.060, val_loss: 7.591\n",
      "[96] loss: 3.134, val_loss: 7.526\n",
      "[97] loss: 3.528, val_loss: 7.656\n",
      "[98] loss: 2.954, val_loss: 7.555\n",
      "[99] loss: 2.908, val_loss: 7.708\n",
      "[100] loss: 3.135, val_loss: 7.219\n",
      "[101] loss: 3.427, val_loss: 7.373\n",
      "[102] loss: 2.920, val_loss: 7.744\n",
      "[103] loss: 3.531, val_loss: 7.338\n",
      "[104] loss: 2.826, val_loss: 7.313\n",
      "[105] loss: 3.182, val_loss: 7.645\n",
      "[106] loss: 3.175, val_loss: 7.546\n",
      "[107] loss: 2.958, val_loss: 7.400\n",
      "[108] loss: 3.291, val_loss: 7.490\n",
      "[109] loss: 2.871, val_loss: 7.362\n",
      "[110] loss: 2.671, val_loss: 7.236\n",
      "[111] loss: 2.781, val_loss: 7.507\n",
      "[112] loss: 2.910, val_loss: 7.085\n",
      "[113] loss: 3.112, val_loss: 7.472\n",
      "[114] loss: 2.863, val_loss: 7.234\n",
      "[115] loss: 2.731, val_loss: 7.460\n",
      "[116] loss: 2.889, val_loss: 7.153\n",
      "[117] loss: 2.822, val_loss: 7.483\n",
      "[118] loss: 2.864, val_loss: 7.045\n",
      "[119] loss: 2.909, val_loss: 7.285\n",
      "[120] loss: 2.588, val_loss: 7.261\n",
      "[121] loss: 2.662, val_loss: 7.096\n",
      "[122] loss: 2.974, val_loss: 7.351\n",
      "[123] loss: 2.630, val_loss: 7.119\n",
      "[124] loss: 2.560, val_loss: 7.406\n",
      "[125] loss: 2.866, val_loss: 7.112\n",
      "[126] loss: 2.582, val_loss: 7.343\n",
      "[127] loss: 2.590, val_loss: 7.033\n",
      "[128] loss: 2.760, val_loss: 7.462\n",
      "[129] loss: 2.635, val_loss: 7.243\n",
      "[130] loss: 2.444, val_loss: 7.027\n",
      "[131] loss: 2.690, val_loss: 7.458\n",
      "[132] loss: 3.209, val_loss: 6.918\n",
      "[133] loss: 2.583, val_loss: 6.924\n",
      "[134] loss: 2.440, val_loss: 7.161\n",
      "[135] loss: 2.657, val_loss: 6.940\n",
      "[136] loss: 2.460, val_loss: 6.732\n",
      "[137] loss: 2.705, val_loss: 7.158\n",
      "[138] loss: 2.553, val_loss: 6.889\n",
      "[139] loss: 2.414, val_loss: 6.907\n",
      "[140] loss: 2.421, val_loss: 7.445\n",
      "[141] loss: 2.790, val_loss: 6.814\n",
      "[142] loss: 2.497, val_loss: 6.930\n",
      "[143] loss: 2.203, val_loss: 7.053\n",
      "[144] loss: 2.172, val_loss: 6.742\n",
      "[145] loss: 2.298, val_loss: 7.501\n",
      "[146] loss: 2.853, val_loss: 6.592\n",
      "[147] loss: 2.465, val_loss: 6.819\n",
      "[148] loss: 2.090, val_loss: 7.048\n",
      "[149] loss: 2.246, val_loss: 6.547\n",
      "[150] loss: 2.708, val_loss: 7.298\n",
      "[151] loss: 2.177, val_loss: 7.028\n",
      "[152] loss: 2.035, val_loss: 6.736\n",
      "[153] loss: 2.086, val_loss: 7.953\n",
      "[154] loss: 3.123, val_loss: 6.661\n",
      "[155] loss: 2.085, val_loss: 6.820\n",
      "[156] loss: 2.070, val_loss: 8.092\n",
      "[157] loss: 2.951, val_loss: 7.024\n",
      "[158] loss: 2.171, val_loss: 6.874\n",
      "[159] loss: 2.260, val_loss: 8.102\n",
      "[160] loss: 2.688, val_loss: 7.184\n",
      "[161] loss: 1.840, val_loss: 6.272\n",
      "[162] loss: 2.867, val_loss: 7.393\n",
      "[163] loss: 2.236, val_loss: 7.145\n",
      "[164] loss: 2.083, val_loss: 6.227\n",
      "[165] loss: 3.090, val_loss: 7.052\n",
      "[166] loss: 1.907, val_loss: 7.534\n",
      "[167] loss: 2.200, val_loss: 6.415\n",
      "[168] loss: 2.666, val_loss: 7.172\n",
      "[169] loss: 1.758, val_loss: 7.535\n",
      "[170] loss: 2.084, val_loss: 6.584\n",
      "[171] loss: 2.199, val_loss: 7.419\n",
      "[172] loss: 1.952, val_loss: 6.564\n",
      "[173] loss: 2.109, val_loss: 7.431\n",
      "[174] loss: 1.936, val_loss: 6.746\n",
      "[175] loss: 2.027, val_loss: 7.853\n",
      "[176] loss: 2.316, val_loss: 6.906\n",
      "[177] loss: 1.797, val_loss: 7.221\n",
      "[178] loss: 1.620, val_loss: 7.289\n",
      "[179] loss: 1.560, val_loss: 6.426\n",
      "[180] loss: 2.342, val_loss: 7.418\n",
      "[181] loss: 2.004, val_loss: 6.490\n",
      "[182] loss: 2.100, val_loss: 7.288\n",
      "[183] loss: 1.589, val_loss: 7.071\n",
      "[184] loss: 1.572, val_loss: 7.892\n",
      "[185] loss: 2.452, val_loss: 6.745\n",
      "[186] loss: 2.113, val_loss: 7.563\n",
      "[187] loss: 1.649, val_loss: 7.042\n",
      "[188] loss: 1.682, val_loss: 7.922\n",
      "[189] loss: 2.248, val_loss: 6.903\n",
      "[190] loss: 2.228, val_loss: 7.266\n",
      "[191] loss: 1.480, val_loss: 7.482\n",
      "[192] loss: 1.613, val_loss: 6.693\n",
      "[193] loss: 2.053, val_loss: 7.627\n",
      "[194] loss: 2.023, val_loss: 6.868\n",
      "[195] loss: 1.616, val_loss: 7.515\n",
      "[196] loss: 1.603, val_loss: 6.905\n",
      "[197] loss: 2.017, val_loss: 7.935\n",
      "[198] loss: 2.180, val_loss: 7.197\n",
      "[199] loss: 1.695, val_loss: 7.098\n",
      "[200] loss: 1.473, val_loss: 7.635\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import ConcatDataset\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import VAE, SameTPred\n",
    "from utils import BrainGraphDataset, project_root\n",
    "import json\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "root = project_root()\n",
    "\n",
    "dataroot = 'fc_matrices/psilo_ica_100_before'\n",
    "annotations = 'annotations-before.csv'\n",
    "dataset = BrainGraphDataset(img_dir=os.path.join(root, dataroot),\n",
    "                            annotations_file=os.path.join(root, annotations),\n",
    "                            transform=None, extra_data=None, setting='upper_triangular')\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create empty lists to store train and validation loaders\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for train_index, val_index in kf.split(dataset):\n",
    "    # Split dataset into train and validation sets for the current fold\n",
    "    train_set = torch.utils.data.Subset(dataset, train_index)\n",
    "    val_set = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "    # Define the dataloaders for the current fold\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Append the loaders to the respective lists\n",
    "    train_loaders.append(train_loader)\n",
    "    val_loaders.append(val_loader)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Dictionary to store training and validation curves\n",
    "curves = {}\n",
    "\n",
    "best_set = [None] * 5\n",
    "\n",
    "input_dim = 4950\n",
    "for dropout in [0]:\n",
    "    for t, train_loader in enumerate(train_loaders):\n",
    "        val_loader = val_loaders[t]\n",
    "    \n",
    "        vae = VAE(4950, [128] * 2, 64).to(device)  # move model to device\n",
    "        vae.load_state_dict(torch.load(os.path.join(root, 'vae_weights/vae_dropout_psilo_ica_before_0.pt'), map_location=device))\n",
    "        vae = vae.to(device)\n",
    "\n",
    "        model = SameTPred(64, 256, dropout=dropout)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # Convert the MLP to the device\n",
    "        model.to(device)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_state = None\n",
    "\n",
    "        # Lists to store training and validation losses\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        best_pass = None\n",
    "        # train the MLP on the new dataset\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                # get the inputs\n",
    "                graphs, labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                _, _, _, z = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                preds = model(z)\n",
    "\n",
    "                # calculate the loss and backpropagate\n",
    "                loss = model.loss(preds, labels.view(preds.shape))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Validation check\n",
    "            val_loss = 0.0\n",
    "            val_label = []\n",
    "            val_output = []\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    graphs, labels = data\n",
    "\n",
    "                    graphs = graphs.to(device)\n",
    "                    labels = labels.to(device).float()\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    _, _, _, z = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    preds = model(z)\n",
    "\n",
    "                    val_label.extend(labels.flatten().tolist())\n",
    "                    val_output.extend(preds.flatten().tolist())\n",
    "\n",
    "                    val_loss += criterion(preds, labels.view(preds.shape)).item()\n",
    "            val_loss /= len(val_set)\n",
    "\n",
    "            # Save the best model so far\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = (copy.deepcopy(model.state_dict()), copy.deepcopy(vae.state_dict()))\n",
    "                best_pass = (val_label, val_output)\n",
    "\n",
    "            # Print statistics and perform testing every 5 epochs\n",
    "            print('[%d] loss: %.3f, val_loss: %.3f' % (epoch + 1, running_loss / (len(train_set)), val_loss))\n",
    "\n",
    "            train_losses.append(running_loss / (len(train_set)))\n",
    "            val_losses.append(val_loss)\n",
    "        best_set[t] = best_pass\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.41517727343572886\n",
      "PearsonRResult(statistic=0.04067191139017011, pvalue=0.798155259911576)\n",
      "5.106329191298712\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnnElEQVR4nO3dd1hTZxsG8DsgewQREURB3FsUR3Gh4m4ddZe6rROtoxbFWWtbXHV0YdXWVbdtHXVWVKyKfg5wVKUO6gZFZW9yvj/SRMIMEDgZ9++6cpWcc5I8nMbk5j3vkAiCIICIiIhIBxmJXQARERFRcTHIEBERkc5ikCEiIiKdxSBDREREOotBhoiIiHQWgwwRERHpLAYZIiIi0lkMMkRERKSzGGSIiIhIZzHIEJHWkkgk+Oyzz5T3N23aBIlEgn///Vcjz//vv/9CIpFg06ZNGnm+4ujZsyfGjh2rvK/4HS9fvqzR1zl69Cg8PDxgbm4OiUSC2NhYjT5/Yd555x34+/uX6WuSYWCQIcrmxo0bGDBgANzc3GBubg4XFxd06dIF3377rdilFcnIkSMhkUgKvY0cOTLf5/jss89UjrW0tET9+vUxb948xMfHl90vowHbt2/H6tWrxS4jl3PnzuH48eOYNWtWqb7Oq1evMGjQIFhYWOD777/H1q1bYWVlVaqvmdOsWbPw/fffIyoqqkxfl/RfObELINIW58+fR8eOHeHq6oqxY8fCyckJjx8/xoULF7BmzRpMmTJF7BLVNn78eHTu3Fl5PzIyEgsWLMC4cePQrl075fYaNWoU+lxBQUGwtrZGYmIijh8/ji+//BInT57EuXPnIJFISqX+/AwbNgxDhgyBmZlZkR63fft23Lx5E9OmTVPZ7ubmhpSUFJiYmGiwSvUtX74cPj4+qFmzZqm+zqVLl5CQkIDFixervC/KUp8+fWBra4sffvgBn3/+uSg1kH5ikCH6z5dffgmpVIpLly7Bzs5OZd+LFy/EKaqYvLy84OXlpbx/+fJlLFiwAF5eXhg6dGiRnmvAgAFwcHAAAEyYMAH9+/fHb7/9hgsXLqi8RnbJycmwtLQs/i+QD2NjYxgbG2vs+SQSCczNzTX2fEXx4sULHDp0CGvXri2T1wKQ631dEklJSUVq1TEyMsKAAQOwZcsWLFq0qMxDMOkvXloi+s/9+/fRoEGDPD/sHR0dVe5v3LgRnTp1gqOjI8zMzFC/fn0EBQXlely1atXw3nvv4fTp02jevDksLCzQqFEjnD59GgDw22+/oVGjRjA3N4enpyfCwsJyPcedO3cwYMAA2Nvbw9zcHM2bN8eBAwc08jsXR6dOnQDIW3kAoEOHDmjYsCGuXLmC9u3bw9LSEnPmzAEApKWlYeHChahZsybMzMxQtWpV+Pv7Iy0tTeU509LSMH36dFSsWBE2Njbo3bs3njx5kuu18+sjc+TIEXh7e8PGxga2trZo0aIFtm/frqzv0KFDePjwofIyWbVq1QDk30fm5MmTaNeuHaysrGBnZ4c+ffrg9u3bKscoLr3du3cPI0eOhJ2dHaRSKUaNGoXk5ORCz+OhQ4eQmZmZbwtJcnIyxo8fjwoVKsDW1hbDhw/Hmzdvch135MgRZa02NjZ499138ffffyv3d+jQASNGjAAAtGjRItclxT179sDT0xMWFhZwcHDA0KFD8fTpU5XXGDlyJKytrXH//n307NkTNjY2+PDDDwEAMpkMq1evRoMGDWBubo5KlSph/PjxedbapUsXPHz4EOHh4YWeHyJ1sUWG6D9ubm4IDQ3FzZs30bBhwwKPDQoKQoMGDdC7d2+UK1cOBw8exKRJkyCTyeDn56dy7L179+Dr64vx48dj6NChWLFiBXr16oW1a9dizpw5mDRpEgAgMDAQgwYNQkREBIyM5H9j/P3332jTpg1cXFwwe/ZsWFlZYffu3ejbty9+/fVXvP/++6VzMgpw//59AECFChWU2169eoUePXpgyJAhGDp0KCpVqgSZTIbevXvj7NmzGDduHOrVq4cbN25g1apV+Oeff7Bv3z7l4z/66CP88ssv8PX1RevWrXHy5Em8++67atWzadMmjB49Gg0aNEBAQADs7OwQFhaGo0ePwtfXF3PnzkVcXByePHmCVatWAQCsra3zfb4TJ06gR48eqF69Oj777DOkpKTg22+/RZs2bXD16lVlCFIYNGgQ3N3dERgYiKtXr2LDhg1wdHTE0qVLC6z7/PnzqFChAtzc3PLcP3nyZNjZ2eGzzz5DREQEgoKC8PDhQ5w+fVrZmrF161aMGDEC3bp1w9KlS5GcnIygoCC0bdsWYWFhqFatGubOnYs6depg3bp1+Pzzz+Hu7q68pLhp0yaMGjUKLVq0QGBgIKKjo7FmzRqcO3cOYWFhKqE+MzMT3bp1Q9u2bbFixQpli9v48eOVz/Pxxx8jMjIS3333HcLCwnDu3DmVy3aenp4A5H2DmjZtWuD5IVKbQESCIAjC8ePHBWNjY8HY2Fjw8vIS/P39hWPHjgnp6em5jk1OTs61rVu3bkL16tVVtrm5uQkAhPPnzyu3HTt2TAAgWFhYCA8fPlRu//HHHwUAwqlTp5TbfHx8hEaNGgmpqanKbTKZTGjdurVQq1YttX+3S5cuCQCEjRs3qv2YhQsXCgCEiIgI4eXLl0JkZKTw448/CmZmZkKlSpWEpKQkQRAEwdvbWwAgrF27VuXxW7duFYyMjIS//vpLZfvatWsFAMK5c+cEQRCE8PBwAYAwadIkleN8fX0FAMLChQuV2zZu3CgAECIjIwVBEITY2FjBxsZGaNWqlZCSkqLyeJlMpvz53XffFdzc3HL9jpGRkbnOi4eHh+Do6Ci8evVKue3atWuCkZGRMHz48FznZ/To0SrP+f777wsVKlTI9Vo5tW3bVvD09My1XfE7enp6qrz3li1bJgAQ9u/fLwiCICQkJAh2dnbC2LFjVR4fFRUlSKVSle2K57x06ZJyW3p6uuDo6Cg0bNhQ5dz98ccfAgBhwYIFym0jRowQAAizZ89Wea2//vpLACBs27ZNZfvRo0fz3C4IgmBqaipMnDixwHNDVBS8tET0ny5duiA0NBS9e/fGtWvXsGzZMnTr1g0uLi65LuVYWFgof46Li0NMTAy8vb3x4MEDxMXFqRxbv359lb4krVq1AiC/ROPq6ppr+4MHDwAAr1+/xsmTJzFo0CAkJCQgJiYGMTExePXqFbp164a7d+/mugRQGurUqYOKFSvC3d0d48ePR82aNXHo0CGVPjBmZmYYNWqUyuP27NmDevXqoW7dusraY2JilJemTp06BQA4fPgwAODjjz9WeXzOjrl5+fPPP5GQkIDZs2fn6utSnD4Yz58/R3h4OEaOHAl7e3vl9saNG6NLly7KWrObMGGCyv127drh1atXhY7sevXqFcqXL5/v/nHjxqm0ZkycOBHlypVT1vDnn38iNjYWH3zwgcr5NTY2RqtWrZTnNz+XL1/GixcvMGnSJJVz9+6776Ju3bo4dOhQrsdMnDhR5f6ePXsglUrRpUsXlRo8PT1hbW2dZw3ly5dHTExMgbURFQUvLRFl06JFC/z2229IT0/HtWvX8Pvvv2PVqlUYMGAAwsPDUb9+fQDypvGFCxciNDQ0V3+IuLg4SKVS5f3sYQWAcl/VqlXz3K7oW3Dv3j0IgoD58+dj/vz5edb74sULuLi4lOA3Ltyvv/4KW1tbmJiYoEqVKnmOdHJxcYGpqanKtrt37+L27duoWLFins+r6ID68OFDGBkZ5XreOnXqFFqb4jJXYZcC1fXw4cN8X7tevXo4duxYrk6uOf//KsLJmzdvYGtrW+DrCYKQ775atWqp3Le2toazs7Oyf9Ddu3cBvO2zlFNhr13Q71q3bl2cPXtWZVu5cuVQpUoVlW13795FXFxcrj5kCnl1khcEgR19SaMYZIjyYGpqihYtWqBFixaoXbs2Ro0ahT179mDhwoW4f/8+fHx8ULduXaxcuRJVq1aFqakpDh8+jFWrVkEmk6k8V36jbPLbrvhyUzzPzJkz0a1btzyPLe1huwDQvn175ail/GRvoVKQyWRo1KgRVq5cmedjcgY5XVXY/8f8VKhQIc8OsepSvD+2bt0KJyenXPvLldPsx7uZmZmy71b2GhwdHbFt27Y8H5NXiI2NjS30/URUFAwyRIVo3rw5APllBwA4ePAg0tLScODAAZW/xgtryi+q6tWrAwBMTExEm/ujJGrUqIFr167Bx8enwL/A3dzcIJPJcP/+fZXWgYiICLVeAwBu3rxZYKhTtwVA0fE2r9e+c+cOHBwcNDaRXN26dfHrr7/mu//u3bvo2LGj8n5iYiKeP3+Onj17Anj7uzs6Ohbr/ZH9d83ZqhMREZFvJ+TsatSogRMnTqBNmzZ5htmcnj59ivT0dNSrV6/I9RLlh31kiP5z6tSpPP+KVvRJUHzJKv4Cz35sXFwcNm7cqNF6HB0d0aFDB/z444/KEJXdy5cvNfp6mjZo0CA8ffoU69evz7UvJSUFSUlJAIAePXoAAL755huVY9SZibdr166wsbFBYGAgUlNTVfZl//9jZWWVq+9SXpydneHh4YHNmzerTOF/8+ZNHD9+XBkiNMHLywtv3rxR9onKad26dcjIyFDeDwoKQmZmpvJ8devWDba2tvjqq69UjlMo7P3RvHlzODo6Yu3atSrD4Y8cOYLbt2+rNWps0KBByMrKwuLFi3Pty8zMzLUMwpUrVwAArVu3LvS5idTFFhmi/0yZMgXJycl4//33UbduXaSnp+P8+fPYtWsXqlWrpuzM2rVrV5iamqJXr14YP348EhMTsX79ejg6OuYZOEri+++/R9u2bdGoUSOMHTsW1atXR3R0NEJDQ/HkyRNcu3ZNo6+nScOGDcPu3bsxYcIEnDp1Cm3atEFWVhbu3LmD3bt349ixY2jevDk8PDzwwQcf4IcffkBcXBxat26N4OBg3Lt3r9DXsLW1xapVq/DRRx+hRYsW8PX1Rfny5XHt2jUkJydj8+bNAOTDfnft2oUZM2agRYsWsLa2Rq9evfJ8zuXLl6NHjx7w8vLCmDFjlMOvpVKpyrpPJfXuu++iXLlyOHHiBMaNG5drf3p6Onx8fJRD8n/44Qe0bdsWvXv3Vv7uQUFBGDZsGJo1a4YhQ4agYsWKePToEQ4dOoQ2bdrgu+++y/f1TUxMsHTpUowaNQre3t744IMPlMOvq1WrhunTpxf6O3h7e2P8+PEIDAxEeHg4unbtChMTE9y9exd79uzBmjVrMGDAAOXxf/75J1xdXTn0mjRLvAFTRNrlyJEjwujRo4W6desK1tbWgqmpqVCzZk1hypQpQnR0tMqxBw4cEBo3biyYm5sL1apVE5YuXSr8/PPPKkODBUE+/Prdd9/N9VoABD8/P5VtiqHAy5cvV9l+//59Yfjw4YKTk5NgYmIiuLi4CO+9956wd+9etX+3kgy/fvnyZYHHeXt7Cw0aNMhzX3p6urB06VKhQYMGgpmZmVC+fHnB09NTWLRokRAXF6c8LiUlRfj444+FChUqCFZWVkKvXr2Ex48fFzr8WuHAgQNC69atBQsLC8HW1lZo2bKlsGPHDuX+xMREwdfXV7CzsxMAKIdi5zX8WhAE4cSJE0KbNm2Uz9erVy/h1q1bap2f/GrMS+/evQUfH588Hx8SEiKMGzdOKF++vGBtbS18+OGHKkPCFU6dOiV069ZNkEqlgrm5uVCjRg1h5MiRwuXLl3M9Z/bh1wq7du0SmjZtKpiZmQn29vbChx9+KDx58kTlmBEjRghWVlb5/h7r1q0TPD09BQsLC8HGxkZo1KiR4O/vLzx79kx5TFZWluDs7CzMmzev0PNCVBQSQSikRxoREZWKv/76Cx06dMCdO3dyjVLSN/v27YOvry/u378PZ2dnscshPcIgQ0Qkoh49eqBKlSp59iXSJ15eXmjXrh2WLVsmdimkZxhkiIiISGdx1BIRERHpLAYZIiIi0lkMMkRERKSzGGSIiIhIZ+n9hHgymQzPnj2DjY0NFyojIiLSEYIgICEhAZUrV861zld2eh9knj17pjeL0xERERmax48f51p5PTu9DzI2NjYA5CeisGXtiYiISDvEx8ejatWqyu/x/Oh9kFFcTrK1tWWQISIi0jGFdQthZ18iIiLSWQwyREREpLMYZIiIiEhnMcgQERGRzmKQISIiIp3FIENEREQ6i0GGiIiIdBaDDBEREeksBhkiIiLSWQwyREREpLMYZIiIiEhnMcgQERGRzmKQISIiIp3FIENEREQ6i0GmKJKSgJiYvPfFxMj3ExER6bkzZ85AJpOJXQYABhn1JSUBCxcCAQG5w0xMjHz7woUMM0REpLfS0tIwdepUeHt7Y+nSpWKXA4BBRn0pKUBcHBAVpRpmFCEmKkq+PyVF3DqJiIhKwf3799GmTRt88803AIC4uDiRK5JjkFGXgwMQGAg4Ob0NM7dvvw0xTk7y/Q4OYldKRESkUXv27EGzZs1w5coV2Nvb4+DBg1iyZInYZQFgkCmanGHG358hhoiI9FZqaiomTZqEQYMGIT4+Hm3atEF4eDjee+89sUtTYpApKgcHYMYM1W0zZjDEEBGR3rl79y5++uknAMDs2bNx6tQpVK1aVeSqVJUTuwCdExMDrFypum3lSrbIEBGR3mnUqBHWrl0LZ2dndO/eXexy8sQWmaLI3rHXyQlYtky1z0x+Q7OJiIh0QEpKCiZOnIjLly8rt40aNUprQwzAIKO+nCEmMBCoVy93B2CGGSIi0kF37txBy5YtsXbtWnzwwQfIyMgQuyS1MMioy8ICkEpzd+zN3gFYKpUfR0REpEO2bNkCT09P3Lx5E5UqVUJQUBBMTEzELkstEkEQBLGLKE3x8fGQSqWIi4uDra1tyZ4sKUk+T0xefWFiYuQhxsqqZK9BRERURpKSkjBlyhRs3LgRANCpUyds27YNTk5OIlem/vc3O/sWhZVV/kGFHX2JiEiHvHjxAh07dsStW7dgZGSEhQsXYu7cuTA2Nha7tCJhkCEiIjJADg4OcHNzw5s3b7B9+3Z06NBB7JKKhUGGiIjIQCQmJkIikcDKygpGRkbYsmULZDIZHB0dxS6t2NjZl4iIyABcv34dzZs3x+TJk5XbHBwcdDrEAGyRISIi0muCIGD9+vX4+OOPkZaWhqSkJLx8+RIVK1YUuzSNYIsMERGRnoqPj4evry/Gjx+PtLQ0vPvuuwgLC9ObEAMwyBAREemlsLAweHp6YufOnShXrhyWL1+OAwcOwEHPRtny0hIREZGeSU9PR58+ffD48WO4urpi165deOedd8Quq1SwRYaIiEjPmJqaYv369ejbty/CwsL0NsQAnNmXiIhIL1y6dAkvX75Ez549xS5FI9T9/maLDBERkQ4TBAGrV69GmzZt4Ovri8jISLFLKlPsI0NERKSjXr9+jVGjRuHAgQMAgM6dO6N8+fIiV1W22CJDRESkg0JDQ9G0aVMcOHAApqam+O6777Bnzx7Y2dmJXVqZYpAhIiLSMStWrED79u3x6NEj1KhRA6GhofDz84NEIhG7tDLHIENERKRjHj16hMzMTAwePBhXr15Fs2bNxC5JNOwjQ0REpANkMhmMjOTtD8uXL0fbtm0xcOBAg2yFyY4tMkRERFpMJpMhMDAQXbp0QWZmJgDAzMwMgwYNMvgQA7BFhoiISGu9ePECw4YNw/HjxwEAv//+OwYOHChyVdqFLTJERERa6PTp0/Dw8MDx48dhYWGBn376CQMGDBC7LK3DIENERKRFsrKy8Pnnn8PHxwfPnz9HvXr1cOnSJYwePZqXkvLAIENERKRFpkyZgoULF0Imk2HUqFG4dOkSGjRoIHZZWkvUIBMUFITGjRvD1tYWtra28PLywpEjR5T7O3ToAIlEonKbMGGCiBUTERGVLj8/P1SsWBFbtmzBzz//DCsrK7FL0mqidvatUqUKlixZglq1akEQBGzevBl9+vRBWFiYMn2OHTsWn3/+ufIxlpaWYpVLRESkcZmZmQgNDUW7du0AAA0aNMC///7L7zs1idoi06tXL/Ts2RO1atVC7dq18eWXX8La2hoXLlxQHmNpaQknJyfljStYExGRvnj69Cl8fHzQsWNHnDt3TrmdIUZ9WtNHJisrCzt37kRSUhK8vLyU27dt2wYHBwc0bNgQAQEBSE5OLvB50tLSEB8fr3IjIiLSNkePHoWHhwfOnDkDCwsLvHjxQuySdJLo88jcuHEDXl5eSE1NhbW1NX7//XfUr18fAODr6ws3NzdUrlwZ169fx6xZsxAREYHffvst3+cLDAzEokWLyqp8IiKiIsnIyMCCBQuwZMkSAECTJk2we/du1K5dW+TKdJNEEARBzALS09Px6NEjxMXFYe/evdiwYQNCQkKUYSa7kydPwsfHB/fu3UONGjXyfL60tDSkpaUp78fHx6Nq1aqIi4vjZSkiIhLV48ePMWTIEJw/fx4AMGnSJHz99dcwNzcXuTLtEx8fD6lUWuj3t+gtMqampqhZsyYAwNPTE5cuXcKaNWvw448/5jq2VatWAFBgkDEzM4OZmVnpFUxERFRMhw4dwvnz52Fra4sNGzZwll4NED3I5CSTyVRaVLILDw8HADg7O5dhRURERJoxfvx4PHnyBKNGjcr3D3IqGlGDTEBAAHr06AFXV1ckJCRg+/btOH36NI4dO4b79+9j+/bt6NmzJypUqIDr169j+vTpaN++PRo3bixm2URERGp5+PAhAgICEBQUBKlUColEgi+++ELssvSKqEHmxYsXGD58OJ4/fw6pVIrGjRvj2LFj6NKlCx4/fowTJ05g9erVSEpKQtWqVdG/f3/MmzdPzJKJiIjUsm/fPowaNQqxsbGwtLTEhg0bxC5JL4ne2be0qdtZiIiISBPS09Ph7++PNWvWAJD379y1axfc3NxErky3qPv9rTXzyBAREem6Bw8eoE2bNsoQ88knn+DMmTMMMaVI6zr7EhER6aKQkBD07t0b8fHxsLe3x+bNm/Hee+9p/oWSkoCUFMDBIfe+mBjAwgIwoPWZGGSIiIg0oH79+rC2tkbDhg2xc+dOVK1aVfMvkpQELFwIxMUBgYGqYSYmBggIAKRSYNEigwkzvLRERERUTNHR0cqfK1asiNOnT+P06dOlE2IAeUtMXBwQFSUPLTEx8u2KEBMVJd+fklI6r6+FGGSIiIiKYceOHahZsyZ++eUX5bZatWrBxMSk9F7UwUHeEuPk9DbM3L79NsQ4OeVuqdFzDDJERERFkJKSgnHjxsHX1xeJiYnYuXMnynQAcM4w4+9vsCEGYJAhIiJS2507d9CyZUusX78eEokE8+bNw759+yCRSMq2EAcHYMYM1W0zZhhciAEYZIiIiNSyZcsWeHp64ubNm6hUqRKOHz+OxYsXo1w5EcbNxMQAK1eqblu58m2fGQPCIENERFSI69evY8SIEUhOTkanTp0QHh6Ozp07i1NM9o69Tk7AsmWqfWYMLMxwZl8iIiI1zJ07F2ZmZpg7dy6MjY3FKSJniFH0iclvuw5T9/ub88gQERHlIAgCNm/eDG9vb7i7uwMAvvzyS5GrgnyyO6lU/nP2sKLoAKyYR8bCQrwayxhbZIiIiLJJTEzEhAkTsG3bNrRs2RJ//fUXTE1NxS7rLQOZ2ZctMkREREV0/fp1DBw4EP/88w+MjY3Rt29fcTrzFsTKKv+gouOXk4pDy/7vEBERlT1BELB+/Xp8/PHHSEtLg4uLC3bu3Im2bduKXRoVgkGGiIgMWmJiIsaOHYudO3cCAHr27InNmzfDwQBbN3QRh18TEZFhSUpSGaJcrlw5REREoFy5clj+2Wc4uHMnQ4wOYZAhIiLD8d/q0cLs2ZC9eAEAMDc3x+7du3HmwAHMfPIERosWyY8jncAgQ0REhiMlBbEvXmDQ4cP4rHt3ZctMTTs7eP32m0GuHq3rGGSIiMhgXIqMRLMzZ7D3+XMsu3YNzz7+2OBXj9Z1DDJERKT3BEHA6tWr0aZNG0Q+fAh3Nzf81asXKickGPzq0bqOQYaIiPTa69ev8f7772P69OnIyMhA//79cTU8HC0CA1UPNNDVo3UdgwwREemtzMxMtG3bFvv374epqSm+++477NmzB3aZmVw9Wk8wyBARkd4qV64cZs6ciRo1aiA0NBR+fn6QvHrF1aP1CNdaIiIi3VLIWkOvkpPxNDYWjRs3BiDvH5OSkgJLS0uDWj1a16n7/c0WGSIi0h1JScCcOcD06blbTmJicPbDD+HRoAHee/ddvH79GgAgkUjkIQZ4u3p0zrCiWD3aycngVo/WdVyigIiIdEdMDHDqFBAbKw8zq1YBDg6QvXiBpd26YX54OLIA1HZ0RExMDOzt7VUfb2UFLFqUd4uOIszoyerRhoItMkSGJsf07CpiYjijKWk3KyugSRP5zyEhwPTpeHH2LHp6eGDOfyFmqLs7rgQHo3bt2vk/R36XjRwcGGJ0DIMMkSH5b3r2PDs0KvoILFzIMEPay8FB3grj7Q0ACDl2DB4+Pjj2/DksJBL89M472HLxIqyrVRO3TiozDDJEhiQlRT79es7RGdk7OnJ6dtJ2ijDTsiW+i4/H8/R01Dc2xqXOnTH64EFIKlYUu0IqQwwyRIYke4dGRZjh9OykqyQSrLOxwaeWlvifvT0aSKViV0Qi4PBrIkOUvQVGgSGG1FHI0OfS7igbHByM/Tt3Yk1KCiRnzsg3Vq8OPHgg/9nbW9kBmHQbh18TUf4cHOTTsWfH6dmpMJrqY1WMDueZmZlYsGABunTpgm83bMDOI0fkO7y9gaAgZZ8ZRQdgTmpnOBhkiAxRTAynZ6ei00Qfq2KEoWfPnsHHxweLFy+GIAgYW748+lpYvG19qVdPpQMwrl1jh3UDwiBDZGhyzmDK6dlJXZroY1XEMHT06FE0adIEZ86cgbW1Nbb//DPWDRsGi44dVS8hZR/N1LEjWxcNCPvIEBkSTs9OmlDSPlY5328zZshbBHO8/5YvXw5/f38AgIeHB3bt2iWfG0bkfjpUNthHhohy4/TspAkl7WOVs2XH3z/PEN2iRQsYGRlh0qRJCA0NfTvBHSe0o2zYIkNkaPjXLJWUpka93b4tDzEKy5Yh2t4elSpVUm66c+cO6tatq4GiSdewRYaI8sa/ZqkkNNXHKkeH83SZDDMHD0atmjXxzz//KLczxFBhGGSIiEg9efWlqlcvdwfgwsJMjuf5d+pUtL90CV/fuIGExET8sWtX2fw+pBcYZIiISD2a6GOVI8Tsa9cOTQcOxMWXL2FnaorfPD0x49Ejjp4jtZUTuwAiItIRVlbAokV597FShJnC+lj9F4bSsrLgL5HgmxEjAAAtW7bErqAgVAsKYodzKhJ29iUqS+xoSwQkJWHN6tWYNm8eAOCTTz7BV199BVNTU/47ICV1v7/ZIkNUVhQzmsbF5R7doWhul0rlf/HyQ5z0mZUVJvn7I/jiRYwdOxa9evV6u4/zF1ERsY8MUVnRxPTuRDoqNTUVy5cvR3p6OgDAxMQEBw4cUA0xRMXAIENUVjQxvTuRDrp79y68vLzg7++POXPmiF0O6RkGGaKypOaMpkT6YseOHWjWrBnCw8Ph4OCAzp07i10S6RkGGaKyVtLp3Yl0QEpKCsaNGwdfX18kJiaiffv2CA8PR/fu3cUujfQMgwxRWcsxoykA+X3Om0F64p9//kGrVq2wfv16SCQSzJs3D8HBwXBxcRG7NNJDDDJERZGUlH/giImR7y+IpqZ3J9JiEokEkZGRqFSpEo4fP47FixejXDkOkqXSIWqQCQoKQuPGjWFrawtbW1t4eXnhyJEjyv2pqanw8/NDhQoVYG1tjf79+yM6OlrEismgKYZP5xU4FAFl4cL8w4ympncn0kJZWVnKn2vVqoV9+/YhPDycfWKo1IkaZKpUqYIlS5bgypUruHz5Mjp16oQ+ffrg77//BgBMnz4dBw8exJ49exASEoJnz56hX79+YpZMhqykw6c1Mb07kRb6+++/0bRpU5w8eVK5zcfHB05OTiJWRYZC62b2tbe3x/LlyzFgwABUrFgR27dvx4ABAwDIl3OvV68eQkND8c4776j1fJzZlzQqZ6vKjBny/i3qjjzizL6kRwRBwKZNm+Dn54eUlBQ0a9YMly9fhkQiEbs00gPqfn9rTR+ZrKws7Ny5E0lJSfDy8sKVK1eQkZGh0ixZt25duLq6IjQ0NN/nSUtLQ3x8vMqNSGNKOnzayir/YxwcGGJIZyQmJmL48OEYPXo0UlJS0LVrVxw5coQhhsqc6EHmxo0bsLa2hpmZGSZMmIDff/8d9evXR1RUFExNTWFnZ6dyfKVKlRAVFZXv8wUGBkIqlSpvVatWLeXfgAwOh0+Tgbt+/TqaN2+OX375BcbGxvjqq69w5MgRODo6il0aGSDRg0ydOnUQHh6OixcvYuLEiRgxYgRu3bpV7OcLCAhAXFyc8vb48WMNVksEDp8m3VbCkXeKodURERFwcXHB6dOnERAQACMj0b9OyECJ/s4zNTVFzZo14enpicDAQDRp0gRr1qyBk5MT0tPTERsbq3J8dHR0gR3IzMzMlKOgFDcijeHwadJlJR15B/mIpH79+qFnz54IDw9H27ZtS7loooKJHmRykslkSEtLg6enJ0xMTBAcHKzcFxERgUePHsHLy0vECslgcfg06bpijrwLCwvD69evAcjniPnpp59w8OBBOPByKmkBUYNMQEAAzpw5g3///Rc3btxAQEAATp8+jQ8//BBSqRRjxozBjBkzcOrUKVy5cgWjRo2Cl5eX2iOWiDSKw6dJ1xVx4VJBEPDdd9/hnXfewahRo6AY5Gpubs5LSaQ1RJ1q8cWLFxg+fDieP38OqVSKxo0b49ixY+jSpQsAYNWqVTAyMkL//v2RlpaGbt264YcffhCzZDJkVlbAokV5D59WfEFw+DRpO8V7VRFe/P3l23OEmNjYWHz00Uf49ddflQ9NSUmBpaWlGFUT5Uvr5pHRNM4jQ0SUh9u334YYQN7fq149AMClS5cwePBgREZGwsTEBMuWLcPUqVM5tJrKlM7NI0NERGUkn5F3wsuXWL16Ndq0aYPIyEi4u7vj3LlzmDZtGkMMaS0GGSIiQ1LAyLuEmTOx6uuvkZGRgf79++Pq1ato0aKF2BUTFYhBhojIUMTEADNnAo8e5TnyzjYqCrsaN8Z3S5Zgz549uSYkJdJGXFediMhQyGTAzZtAYiLwzTeQ2dtj5YoVsLe3x+iZM4E+ffCOtTXeGTUK4KUk0hEMMkREhsLICGjUCHj2DDFffIGR0dE49OefMDc3R6c+fVDNzQ2oXFl+HJGO4LuViMhQODgAy5fjrKkpmu7di0N//gkzU1Os9vSEW2Ii4OoKLF/OdcNIp7BFhojIQMhkMixdvx7zjxxBVlYWaltZYbenJ5rY2qq/gjuRlmGQISLSJUlJeU/KCMg78+YzKaNMJkOvXr1w+PBhAMDQXr0QlJEB63L/fQ0Y0gruxTyHpJ14aYmISFeUYNFHIyMjtGzZEhYWFvhp9WpscXR8G2IAw1nBXQMLZ5J2YZAhItIVRVz0MSsrCy9evFA+fN68ebh26hRG37wJSXS0Ya7gXsyFM0l7McgQEekKBwdg3jzAzi7vRR/t7OT7HRwQFRWFbt26oWvXrkhNTQUAGL95g1obNhj2Cu5FXDiTtB+DDBGRrkhKAr79Vv6zIsz4+78NMQDw7bcI/uMPeHh4IDg4GHfv3kVYWJh8n4UFYGkpPzavFdzt7OT79X0F95xhRnEOGWJ0EoMMEZGuUFwWiY0FUlOB9HT59vR0IDUVWW/eYMHJk+jSuzeio6PRsGFDXL58GV5eXqKWrXUU/V9mzFDdrrjP/jE6haOWiIh0haIlYfp0ICREvq16deDBAzzLyoJvZiZC/usTM3bsWKxZswYW2VtXUlKA5GR5EAoIeNv6oOgfEhsLmJvLj9PXUTuKzr7R0bn3ffWV/L+VKgGLFunvOdAzbJEhIiqKpKT8+5DExJT9X/P/LSUw4fVrhLx4AWsrK2zbtg3r1q1TDTEA+4cA8pAWHS0PgiEh8stpy5bJ/6vYFh3Nzr46hEGGikbbPsSJypLYQ3ezt5y0bCnvqGttDdSrh287dECnChVwpWdP+Hbtmv9zsH8I6RkGGVKf2B/iRGITe+iuhQUglQJ2dngik2Hd8+fy7aamcHNwQHCPHqjt6lp4Z10Hh7z7hxhCiLGwkF868vaW32Jj5WEuNvbttkqV9L/Dsx5hkCH1if0hTiQ2sS/NWFkBU6bg0JMn8PjjD4y/cQOH3n1X/rqxsfJjpkwpvG9HTIx8ArzsDGVCPCsref+XVauAOXNU982ZI9/O/jE6hUGG1Cf2hziRNhDx0kzG8+f4tFcvvHf6NF5lZMCzSRPU7dLlbT2xscAXXxQcSLL/4WGIE+IBb0NKXmEu+37SCQwyVDS8vk4kyqWZhw8fon3fvlhx4wYA4OOxY3Hu4kXUqFFD9d+lVJr/ZZGcIcYQJ8QDGOb0jEQQBEGdA5s2bQrJf73jC3L16tUSF6VJ8fHxkEqliIuLg62trdjl6I/bt+UhRmHZMvkHIpEhyP5FqFCKYf7gwYMYPnw4YmNjYWdnh41r1qDv8OF511XQgoeKfm5xcblrVfxOUql+X1rJK8xlH4LOP8y0hrrf32rPI9O3b19N1EX6IL/r6/yHT4Yg5xfejBny97/ir/lS+HeQmJiI2NhYtGrVCjt37kS1atXyPrCw11X0D8lr5WdFq46+r/ys6DAN5D27sSLMsbOvzlC7RUZXsUVGwwr6EOdfMaTvyvCv+aysLBgbGyvv79mzB3369IGpqWlJfwtKSso7zAGFt2pRmVH3+7tYfWRiYmJw+fJlXLlyBa9evSp2kaRjeH2dDJ3ir/mcYUXdPiqAWnMx7d27Fw0bNlRZuXrgwIEMMZpiZZV/0HRwYIjRMUUKMn///Tfat2+PSpUqoVWrVmjZsiUcHR3RqVMnRERElFaNpC008SFOpMsUl2byanFR/DsoqH9JIXMxpX76KfzatMHAgQNx584dLF++vHR+DyI9ovalpaioKDRs2BAVK1bEhAkTULduXQiCgFu3bmH9+vV49eoVbt68CUdHx9KuuUh4aUnD2CRLVHwFXJq6O2kSBh07hvD4eADArFmzsHjxYpiYmIhcNJE41P3+VjvIzJo1CydOnMC5c+dgbm6usi8lJQVt27ZF165dERgYWLLKNYxBhoi0Sh79zHb4+WHcmTNIzMqCQ4UK2PrLL+jevbvYlRKJSuN9ZP7880/MmjUrV4gBAAsLC3z66ac4duxY8aolIjIUOeZi+nnIEPieOoXErCy09/JC+LVrDDFERaB2kHnw4AGaNWuW7/7mzZvjwYMHGimKiEhvKdYi+29CvQHOzqhjZYV5EyYg+Lff4GJnJ15tRDpI7XlkEhISCmzasbGxQWJiokaKIiLSS/919g0OD0cnJydIANiamCCsfXtYJCYCn34qX7BQnyekI9KwIo1aSkhIQHx8fL43PZ+ShohIreHT+T40Jgaj9+1D5+BgfHPoEGBnByxbBosKFYCQEPktOpoLrxIVgdotMoIgoHbt2gXuV2cJAyIinVWCKf7//vtvDOrfH7fu34cRgGT+4UekEWoHmVOnTpVmHURE2i8lRR5ici5HkHPtpZQUZZARBAEbN27E5MmTkZKSAicrK+zw8kIHxWrVijXLvL3l/61UiXMxERUBlyggIiqKIizTkZiYiIkTJ+KXX34BAHTt2hVb166Fo40N8PJl7oVXK1bkXExE/ynVJQoUBEHAyZMncejQIbx586YkT0VEZaUEfTwIuYZPw98/3zWWbt26hZ07d8LIyAhffvkljhw5Akd3d/nOvBZeBRhiiIpI7SATGxuLESNGoFGjRhg7dizi4+PRrl07dO7cGb169UK9evVw/fr10qyViEqqkCnyERAg388wUzAHB+XwaaUZM3LNeN2yZUt8//33OH36NObMmQMjI6PcLTrLlnGtMqISUDvIzJw5E6GhoRgyZAhu3LiB7t27IysrC6Ghobh48SLq1auHuXPnlmatRFRSOft4KL40s3+5xsVx1ExhYmLybFGJj4zEqFGjcPPmTeXmcePGoV27dm8fx4VXiTRK7T4yLi4u2L59O7y9vfH06VNUrVoVJ0+eRIcOHQAA//vf/9C7d29EKTq7aQn2kSG9U9L1rorQx4PykM/5C4uIwKBr13AvPh6NGjVCeHi4vAUmuxKMeiLSGmW05p7G11oqV64cHj9+DGdnZwCApaUlbty4gRo1agCQLyrp4uKCrKysEhevSQwypFc09UWYc5QNwBCjjjxaVIQKFfDDsmWYMWcO0mUyuFpbY+fu3fDq0SPv5+DCq6TLyjCMa7yzr0wmg7GxsfK+sbGxyrwxnEOGqAxo6tKQmn08KAcLC/mH9H8hJrZcOQwaNAiTZ89GukyG3q6uCBs+HF7t2+f/HFZW+Z9nBweGGNJuWnh5Wu0WGSMjI3zxxRewtrYGIF8N+9NPP4XDf/8gExISsGDBArbIEJU2TVwaYotM8f3XovIoORkdOnRAZGQkTExMsHTpUkwbOhQSS0uGEdJvZXR5WuOXlqpVq6ZWq0tkZKT6VZYBBhnSSyUJIg8fAvPmySdjy/khZGcHfPEF4OZWmtXrhaysLHTu3Bn//vsvdu3ahZYtW4pdElHZKYM/hjQeZHQVgwzprdu3c0+oVq9ewY95+BDo1UseYry9gVWr3s5MO326fK0fOzvg4EGGmTy8efMGFhYWMDc3BwBER0fDzMwMdoa0YjX7+JBCcT6DiqBMJsQjIpHkM/yXw3ZLz4ULF+Dh4YGZM2cqt1WqVMnwQgznISJAqz6DGGSIdE1JJlRzc5O3tnh7y1tlAgLkf1UFBLxtpWFrjAqZTIYVK1agXbt2ePToEY4dO4b4+HixyxKHFnb0JBFo2aSOvLREpEvymlAt56KF6lynZmdftcTExGDkyJE4dOgQAGDw4MFYt26dYX+WcB4iw6apzyA18NISkT7KMfxX+UGRff0fqbTw1ZM5/LpQZ8+eRdOmTXHo0CGYmZnhxx9/xI4dOww7xABFWmuK9JCmPoM0iC0yRLpGE50t2SJToKSkJFSrVg0xMTGoXbs29uzZg8aNG4tdlnYp5Y6epMV0cWbfolwP1rawwCBDlAMvDahl3759+PXXXxEUFKScP4sg/xJ78gRYsSJ3EJ45E6hShaOWSCM0emnJzs4O5cuXV+tWFIGBgWjRogVsbGzg6OiIvn37IiIiQuWYDh06QCKRqNwmTJhQpNchov9oYtHCpKT898fE6OyIlZCQEPz555/K+3379sXWrVsZYrJLSpK3wvTpAzx6pNrR89Ej+XZ/f519D5BuKqfOQadOnVL+/O+//2L27NkYOXIkvLy8AAChoaHYvHkzAgMDi/TiISEh8PPzQ4sWLZCZmYk5c+aga9euuHXrFqyyJfqxY8fi888/V963tLQs0usQ0X8U17eBvK9vK9ZJye/6th4uepiVlYUvv/wSixYtgr29PcLDw+Hi4iJ2WdrpyRMgOBh480Z+/5tvgDp15C0xffrItwcHy4+rU0fcWslgqBVkvL29lT9//vnnWLlyJT744APltt69e6NRo0ZYt24dRowYofaLHz16VOX+pk2b4OjoiCtXrqB9trVKLC0t4eTkpPbzElE+rKzkISOv69uKMFPQ9e2UFHlgef5cHlpyjlh49AhwdpYfpwNBJioqCkOHDkVwcDAA4L333jOseWGKqkoVoFMn4ORJ+RD9FSveXppUDNnv1El+HJUeTkqoosijlkJDQ9G8efNc25s3b47//e9/JSomLi4OAGBvb6+yfdu2bXBwcEDDhg0REBCA5OTkfJ8jLS0N8fHxKjciyqYkixYqPiAfPpSHluzz0Dx6JN9uZVXwiAUtuTQVHBwMDw8PBAcHw9LSEps3b8bGjRtVWoMpBysrYPlyYP9+wNVVddSSq6t8+/LlBvUlWuY4KWEuRQ4yVatWxfr163Nt37BhA6pWrVrsQmQyGaZNm4Y2bdqgYcOGyu2+vr745ZdfcOrUKQQEBGDr1q0YOnRovs8TGBgIqVSqvJWkJiLKISUFSE+X//WtCDP+/m9DjJubfH9+E6JpwYewIAhYsGABunTpgujoaDRs2BBXrlzB8OHDS+019YqVlfyyUV7D9+vUYYgpbZyUMJciD78+fPgw+vfvj5o1a6JVq1YAgP/973+4e/cufv31V/Ts2bNYhUycOBFHjhzB2bNnUaWAZsmTJ0/Cx8cH9+7dQ40aNXLtT0tLQ1pamvJ+fHw8qlatylFLRJqi+MC8dw+4fx+oXh148ACoUQOoWbPgUU9lOJlWQcaMGYOff/4ZY8eOxZo1a2BRhnNe6AUO3xeXgYw8LNVFIx8/foygoCDcuXMHAFCvXj1MmDCh2K0fkydPxv79+3HmzBm4u7sXeGxSUhKsra1x9OhRdOvWrdDn5vBrolIQEfG2c6dC+fLySwuFdfIU6UM4KysLxsbGAIDk5GQcP34cffv21fjr6D0D+RLVegYQJnVi9WtBEDBlyhT8/vvvOH36NGrVqlXoY86dO4e2bdvi2rVrak1QxSBDpGGKD9C7d9+2xChaZmrVUu+DtAw/hDMzMzF//nyEh4fj0KFDMDLihObFpiUtavQfPZ+UsFSXKPjrr78wdOhQtG7dGk+fPgUAbN26FWfPni3S8/j5+eGXX37B9u3bYWNjg6ioKERFRSHlv2t79+/fx+LFi3HlyhX8+++/OHDgAIYPH4727dtzlk0iMWQfnRQVJf/QtLaW/zcq6m0H4MIWjSujJRIeP36MDh06YMmSJTh69CiOHz+u0ec3OFo4Pb3B0qLVp8VW5CDz66+/olu3brCwsMDVq1eV/VHi4uLw1VdfFem5goKCEBcXhw4dOsDZ2Vl527VrFwDA1NQUJ06cQNeuXVG3bl188skn6N+/Pw4ePFjUsolIEywsABOTtx17XV3lfwW6ur7tAGxiUvgXWRl8CB86dAgeHh44d+4cbG1tsWvXLnTv3l1jz2+QFMP382pxUYQZHZpDSGdp2erTYivypaWmTZti+vTpGD58OGxsbHDt2jVUr14dYWFh6NGjB6KyNxVrAV5aItKgmBjg00+BZ8/k4SWveWQqV5YPwVW3w6+G+1hkZGQgICAAX3/9NQDA09MTu3btynNwAJHOMaDLe6V2aSkiIkJlsjoFqVSK2NjYoj4dEekSCwugQgXVEAO8/Wvc1VW+P78WGU0skVCIkSNHKkPMxx9/jHPnzjHEkP7g5b1c1JrZNzsnJyfcu3cP1apVU9l+9uxZVK9eXVN1EZE2KunMwCVdIkENM2fORHBwMIKCgvD+++8X+3mItFJJ/w3qoSIHmbFjx2Lq1Kn4+eefIZFI8OzZM4SGhmLmzJmYP39+adRIRNrEyir/D8nCmrJL4UM4PT0dFy5cULYUN23aFJGRkZwbhvRXSf4N6qEiB5nZs2dDJpPBx8cHycnJaN++PczMzDBz5kxMmTKlNGokIn2iwQ/hBw8eYPDgwbh27RrOnz+vXD6FIYbIcBQ5yEgkEsydOxeffvop7t27h8TERNSvX59L3RNRmdq7dy/GjBmD+Ph4lC9fHq9fvxa7JCISQZE7+44ePRoJCQkwNTVF/fr10bJlS1hbWyMpKQmjR48ujRqJiJRSU1Ph5+eHgQMHIj4+Hq1bt0Z4eDi6du0qdmlEJIIiB5nNmzcrJ6zLLiUlBVu2bNFIUUREebl79y5av/MOfvjhBwDyS92nT5+Gq6ur/IAyXD2biLSD2peW4uPjIQgCBEFAQkICzM3NlfuysrJw+PBhODo6lkqRREQAsH/PHoRduwYHc3Ns3bQJ3QcPfrtTMbRbKuWkbKTfkpLy7jAPyP8dcNRS3uzs7CCRSCCRSFC7du1c+yUSCRYtWqTR4oiIspvx0UeI3bMHEytUgMuJE4CPT+7JwAD5h7wBfZCTAUlKAhYuBOLick96Z6BhXu0gc+rUKQiCgE6dOuHXX3+Fvb29cp+pqSnc3NxQuXLlUimSiAzTnTt3sHDhQvz888+wsrKCkaMjvvjzz7ehJSCAqy+TYUlJkYcYxfs/r5l9FccZSJAp8hIFDx8+hKurKyQSSWnVpFFcooBIN23duhUTJ05EUlISpk6ditWrV7/dWYarZxNpnVJe5kNblNoSBSdPnsTevXtzbd+zZw82b95c1KcjIlKhGAE5fPhwJCUloVOnTpg9e7bqQWW0ejaRVsq+HEFUFODvr3chpiiKHGQCAwPhkMdJcnR0LPLq10RE2f39999o2bIlNm7cCCMjIyxatAjHjx+Hk5OT6oFlsHo2kVZjmFcqcpB59OgR3N3dc213c3PDo0ePNFIUERmew4cPo0WLFrh16xacnZ0RHByMBQsWwNjYWPXAnM3qy5ZpbMFJIp3BMK9U5CDj6OiI69ev59p+7do1VKhQQSNFEZHhadKkCaysrNC1a1eEh4ejQ4cOuQ8qg9WzibQew7yKIgeZDz74AB9//DFOnTqFrKwsZGVl4eTJk5g6dSqGDBlSGjUSkZ56/vy58mcXFxeEhobiyJEj+c9JpVg9O2dfgOx9Bkq4ejZRqUtKyj9sFDapI8N8LkUetZSeno5hw4Zhz549KFdOPnpbJpNh+PDhWLt2LUxNTUul0OLiqCUi7SMIAtavX4+pU6di27Zt6Nevn/oP5mRgpMtKOg+MAc0jo+73d5GDjMI///yDa9euwcLCAo0aNYKbm1uxiy1NDDJE2iU+Ph7jx4/Hzp07AQC+vr7Ytm2byFURlZG8WlRyzgNT2OgjAwnzpR5kdAWDDJH2CAsLw6BBg3Dv3j0YGxsjMDAQn3zyCYyMinyVm0h3Gcg8MCWl0SAzY8YMLF68GFZWVpiRc7hXDitz9qIWGYMMkfgEQcAPP/yAGTNmID09Ha6urti5cye8vLzELo1IHJzUsVDqfn+rtURBWFgYMjIylD/nR1dm+yWisnXhwgVMnjwZANC7d29s3LhRZZkTIoOjmAfG3//tNgOdB6akeGmJiMrEzJkzUaVKFUydOpV/9BCxRaZQpbZEARFRYRSXkp4+farctmLFCkybNo0hRteVZOgwyXEeGI1Sq0WmKEMjf/vttxIVpGlskSEqW69fv8bo0aOxf/9+tG/fHidPnsw9Oy/pJgMa+ltqNDFqyUBotEVGKpUqb7a2tggODsbly5eV+69cuYLg4GBIpdKSV05EOuvChQto2rQp9u/fD1NTUwwaNIgjkvRJSoo8xORsOcj+JRwXJz+O8sZJHTWuyH1kZs2ahdevX2Pt2rXKv7KysrIwadIk2NraYvny5aVSaHGxRYao9MlkMqxcuRIBAQHIzMxEjRo1sHv3bjRr1kzs0kjTOHS45AxkHpiSKrV5ZCpWrIizZ8+iTp06KtsjIiLQunVrvHr1qngVlxIGGaLS9ebNGwwbNgyHDh0CAAwaNAjr16/nvzd9xo6qVAZKrbNvZmYm7ty5k2v7nTt3IJPJivp0RKTjTE1NERkZCTMzMwQFBWHnzp0MMfpOMXQ4Ow4dJpGoNY9MdqNGjcKYMWNw//59tGzZEgBw8eJFLFmyBKNGjdJ4gUSkfRR/tBgZGcHKygp79+5FWloaPDw8xC2MykZMjPxyUnYrV7JFhkRR5EtLMpkMK1aswJo1a5Qr1zo7O2Pq1Kn45JNPtG50Ai8tEWnWixcvMHz4cHTs2BGzZs0Suxwqa+wjQ2WkTNZaio+PBwCtDggMMkSaExISgg8++ADPnz+HjY0NIiMjUaFCBbHLorLCocNUhkp1QrzMzEycOHECO3bsUE5u9ezZMyQmJhavWiLSallZWVi8eDE6deqE58+fo169ejh//jxDjKHh0GHSQkVukXn48CG6d++OR48eIS0tDf/88w+qV6+OqVOnIi0tDWvXri2tWouFLTJEJRMVFYWhQ4ciODgYADBy5Eh89913sOLwUMPEocNURkqtRWbq1Klo3rw53rx5A4tsqfv9999XftARkX5ITU1Fq1atEBwcDEtLS2zevBkbN25kiDFkVlb5XzZycGCIoTJX5FFLf/31F86fPw9TU1OV7dWqVVNZV4WIdJ+5uTlmzpyJdevWYc+ePahbty7/IicirVLkFhmZTIasrKxc2588eQIbGxuNFEVE4nn27Bn+/vtv5f3Jkyfj0qVLb0PMwoV5L2yn6PC5cCEXDiSiMlPkINO1a1esXr1aeV8ikSAxMRELFy5Ez549NVkbEZWxY8eOwcPDA3369FGOSpRIJDA3N5cfwLV2iEjLFDnIrFixAufOnUP9+vWRmpoKX19f5WWlpUuXlkaNRFTKMjMzMWfOHHTv3h0vX76EtbU13rx5k/vA7KNTFGHm9m0OvSUi0RRrHpnMzEzs2rUL165dQ2JiIpo1a4YPP/xQpfOvtuCoJaKCPX78GB988AHOnTsHAJg0aRK+/vrrt60weeFaO0RUykplQryMjAzUrVsXf/zxB+rVq6eRQksbgwxR/g4dOoThw4fj9evXsLW1xYYNGzBw4ED1Hnz7NuDv//b+smWAjnwuEJH2K5Xh1yYmJkhNTS1xcUQkPkEQ8N133+H169fw9PTE1atX1Q8x+a21k7MDMBFRKStyHxk/Pz8sXboUmZmZpVEPEZURiUSCzZs3Y+7cuTh37hxq1Kih3gNzTke/bJlqnxmGGSIqQ0XuI6OY+M7a2hqNGjXKNTHWb7/9ptECS4qXloje2rdvH/766y98/fXXxXsCrrVDRGVE3e/vIk+IZ2dnh/79+5eoOCIqW+np6fD398eaNWsAAB06dECvXr2K/kSKtXaAvNfaCQjgWjtEVKZKtPq1LmCLDBm6Bw8eYPDgwbh8+TIA4JNPPsFXX32Va3ZutXFmXyIqAxrv7CuTybB06VK0adMGLVq0wOzZs5HCSa+ItNrevXvRtGlTXL58Gfb29jhw4ABWrFhR/BADcK0dItIqageZL7/8EnPmzIG1tTVcXFywZs0a+Pn5lWZtRFQC8+bNw8CBAxEfH4/WrVsjLCyseJeTSL8kJeXfITsmhstLkM5RO8hs2bIFP/zwA44dO4Z9+/bh4MGD2LZtG2QyWWnWR0TF1LZtWxgZGWHWrFk4ffo0XF1dxS6JxMa1skgPqR1kHj16pLKWUufOnSGRSPDs2bNSKYyIii77v8fu3bvjzp07WLJkCUxMTESsirQG18oiPaR2kMnMzMw1ZbmJiQkyMjKK/eKBgYFo0aIFbGxs4OjoiL59+yIiIkLlmNTUVPj5+aFChQqwtrZG//79ER0dXezXJNJHKSkpGD9+PBo0aIB///1Xub1WrVriFUXah2tlkR5Se9SSkZERevToATMzM+W2gwcPolOnTipzyRRlHpnu3btjyJAhaNGihXLRups3b+LWrVvK55w4cSIOHTqETZs2QSqVYvLkyTAyMlKuC1MYjloifXfnzh0MGjQIN27cgEQiwbp16/DRRx+JXRZpM66VRTpA42stjRo1Sq0X3rhxo3oV5uHly5dwdHRESEgI2rdvj7i4OFSsWBHbt2/HgAEDAMg/tOvVq4fQ0FC88847hT4ngwzps61bt2LixIlISkqCo6MjfvnlF3Tp0kXsskgXcK0s0nIanxCvJAFFXXFxcQAAe3t7AMCVK1eQkZGBzp07K4+pW7cuXF1d8w0yaWlpSEtLU96Pj48v5aqJyl5ycjImT56s/HfZsWNHbNu2Dc7OziJXRjohv7Wy2CJDOqjIay2VFplMhmnTpqFNmzZo2LAhACAqKgqmpqaws7NTObZSpUqIyt4kmk1gYCCkUqnyVrVq1dIunajMrVy5Ehs3boSRkREWLVqEP//8kyGG1MO1skjPaE2Q8fPzw82bN7Fz584SPU9AQADi4uKUt8ePH2uoQiLtMXPmTLz77rsIDg7GggULYGxsLHZJpAvyWhOrXr3cHYAZZkiHaEWQmTx5Mv744w+cOnUKVapUUW53cnJCeno6YmNjVY6Pjo6Gk5NTns9lZmYGW1tblRuRrktMTMSSJUuUq86bm5vjjz/+QIcOHcQtjHSLYq2snB17s49m4lpZpGOKvGikJgmCgClTpuD333/H6dOn4e7urrLf09MTJiYmCA4OVi5UGRERgUePHsHLy0uMkonK3PXr1zF48GDcuXMHycnJ+Pzzz8UuiXSVlRWwaFHea2UpwgzXyiIdI2qQ8fPzw/bt27F//37Y2Ngo+71IpVJYWFhAKpVizJgxmDFjBuzt7WFra4spU6bAy8tLrRFLRLpMEASsX78eU6dORWpqKlxcXDgiiUrOyir/oMKOvqSDRF39WiKR5Ll948aNGDlyJAD5hHiffPIJduzYgbS0NHTr1g0//PBDvpeWcuLwa9JF8fHxGD9+vLLPWI8ePbBlyxY48IuGiAyExueR0VUMMqRrrl+/jv79++PevXswNjZGYGAgPvnkExgZaUWXNiLxJSXlfXkMkHdU5uUxvaDxeWSIqGwYGxvj6dOncHV1xc6dO9kfjCg7xcKXcXG5571RjMqSSuV9gRhmDAL/xCPSAorRSADQoEED7N+/H2FhYQwxRDlx4UvKgUGGSGSXL19GgwYNcP78eeW2Ll26KGe41rikpPznCYmJke8n0lZc+JJyYJAhEokgCFizZg1at26Nf/75B3PmzCn9F1U0y+c16ZniL9qFCxlmSLvlDDP+/gwxBoxBhkgEb968Qb9+/TBt2jRkZGSgX79+2LdvX+m/MJvlSV84OAAzZqhumzGDIcYAMcgQlbELFy6gadOm2LdvH0xNTfHtt99i7969udYUKxVslid9kd/Cl1xeweAwyBCVoatXr6Jdu3Z4+PAhatSogdDQUEyePDnfOZVKBZvlSddx4UvKhkGGqAw1bdoU7733HgYPHoyrV6+iWbNm4hTCZnnSVVz4knJgkCEqZRcuXEB8fDwA+WzWO3bswI4dO8SdoLEkzfIc9URi4sKXlAODDFEpkclkWLJkCdq2bYvx48dDMYm2ubl52V5KyqkkzfIc9URiUyx8mddlUEWY4WR4BoVBhqgUvHjxAj179kRAQACysrJgbGyMjIwMscsqebM8Rz2RNrCyyv8yqIMDQ4yBYZAh0rCQkBB4eHjg2LFjsLCwwE8//YStW7fC1NRU7NJK3izPUU9EpGW4aCSRhmRlZeGrr77CZ599BplMhnr16mH37t1o2LCh2KWp0sSCe9lbYBQYYohIg9T9/maLDJGGxMbGIigoCDKZDCNGjMClS5e0L8QAmmmW56gnItISDDJEGlKhQgVs374dmzZtwqZNm2Clz9fpORkZEWkJBhmiYsrKysKCBQuwfft25bYOHTpgxIgRIlZVBjgZGRFpEQYZomJ49uwZfHx8sHjxYowfPx7R0dFil1Q2OBkZEWkZBhmiIjp27BiaNGmCkJAQWFtb48cff0SlSpXELqtscDIyItIyHLVEpKbMzEzMnz8fS5YsAQA0adIEu3fvRu3atUWurIxpYtQTEVEh1P3+LleGNRHprIyMDPj4+OCvv/4CAEycOBErV66Eubm5yJWJwMoq/6DCUUtEVMZ4aYnKlo6u02NiYgIvLy/Y2Nhg165d+OGHHwwzxBARaRleWqKyo1inJy4u98Rpik6kUqnWrJOSkZGBN2/ewNHRUXn/6dOnqFatmriFEREZAE6IR9pHh9bpefjwIdq1a4fevXsjPT0dgLxVhiGGiEi7MMhQ2dGRdXr2798PDw8PXLx4EREREbh9+7ao9RARUf4YZKhs5Qwz/v5aE2LS09Mxbdo09O3bF7GxsWjZsiXCwsLQpEkT0WoiIqKCMchQ2dPCdXoePHiANm3aYM2aNQCATz75BH/99RcvJRERaTkGGSp7WrhOz0cffYTLly/D3t4eBw4cwIoVK2BqaipaPUREpB4GGSpbWrpOz48//oju3bsjPDwcvXr1EqUGIiIqOgYZKjtatE7PvXv3sGHDBuX9WrVq4ciRI6hatWqpvzYREWkOgwyVHS1Zp2fnzp1o1qwZxo0bh5MnT5bqaxERUeniEgVUdqys5JPd5bVOjyLMlOI6PSkpKZg2bRrWrVsHAGjfvj3q1KlTKq9FRERlg0GGypZI6/TcuXMHgwYNwo0bNyCRSDB37lwsXLgQ5crxnwARkS7jpzjpve3bt2PcuHFISkqCo6Mjtm3bhs6dO4tdFhERaQD7yJDeS05ORlJSEjp27Ijw8HCGGCIiPcIWGdJLmZmZystGY8aMQfny5dG3b18YGxuLXBkREWkSW2RIrwiCgI0bN6Jx48Z4/fo1AEAikaB///4MMUREeohBhvRGYmIihg8fjtGjR+P27dv4/vvvxS6JiIhKGS8tkV64fv06Bg0ahIiICBgZGWHx4sWYPXu22GUREVEpY5AhnSYIAtavX4+pU6ciNTUVLi4u2LFjB9q1ayd2aUREVAZ4aYl02jfffIPx48cjNTUVPXr0QHh4OEMMEZEBYZAhnTZs2DDUqFEDy5Ytwx9//AGHUpxUj4iItA8vLZFOEQQBf/75J7p06QKJRAJ7e3vcvHkT5ubmYpdGREQiYIsM6Yy4uDgMGjQI3bp1w08//aTczhBDRGS42CJDOuHy5csYNGgQIiMjYWJigtTUVLFLIiIiLcAWGdJqgiBgzZo1aN26NSIjI1GtWjWcPXsWkydPFrs0IiLSAmyRIa315s0bjB49Gvv27QMA9OvXDz/99BPs7OxErYuIiLQHW2RIa12/fh379++Hqakpvv32W+zdu5chhoiIVLBFhrSWt7c3vvvuO7Rq1Qqenp5il0NERFqILTKkNV69eoUhQ4bgn3/+UW6bNGkSQwwREeVL1CBz5swZ9OrVC5UrV4ZEIlH2hVAYOXIkJBKJyq179+7iFEul6ty5c/Dw8MCuXbswYsQICIIgdklERKQDRA0ySUlJaNKkSYGrFHfv3h3Pnz9X3nbs2FGGFVJpk8lkWLJkCby9vfHkyRPUqlULQUFBkEgkYpdGREQ6QNQ+Mj169ECPHj0KPMbMzAxOTk5lVBGVpZcvX2L48OE4evQoAMDX1xdr166FjY2NyJUREZGu0Po+MqdPn4ajoyPq1KmDiRMn4tWrVwUen5aWhvj4eJUbaZ+7d+/Cw8MDR48ehYWFBTZs2IBffvmFIYaIiIpEq0ctde/eHf369YO7uzvu37+POXPmoEePHggNDYWxsXGejwkMDMSiRYvKuFIqqmrVqsHV1RVSqRS7d+9Gw4YNxS6JiIh0kETQkl6VEokEv//+O/r27ZvvMQ8ePECNGjVw4sQJ+Pj45HlMWloa0tLSlPfj4+NRtWpVxMXFwdbWVtNlUxG8ePEC5cuXh4mJCQDg+fPnsLW1hZWVlciVERGRtomPj4dUKi30+1vrLy1lV716dTg4OODevXv5HmNmZgZbW1uVG4kvODgYjRs3xrx585TbnJ2dGWKIiKhEdCrIPHnyBK9evYKzs7PYpZCasrKysHDhQnTp0gXR0dE4cuQIUlJSxC6LiIj0hKh9ZBITE1VaVyIjIxEeHg57e3vY29tj0aJF6N+/P5ycnHD//n34+/ujZs2a6Natm4hVk7qePXsGX19fhISEAADGjh2LNWvWwMLCQuTKiIhIX4gaZC5fvoyOHTsq78+YMQMAMGLECAQFBeH69evYvHkzYmNjUblyZXTt2hWLFy+GmZmZWCWTmo4dO4Zhw4bh5cuXsLa2xo8//ghfX1+xyyIiIj2jNZ19S4u6nYVIc968eQM3NzckJCSgSZMm2L17N2rXri12WUREpEPU/f7W6uHXpJvKly+PoKAgnD17FqtWrYK5ubnYJRERkZ5iiwxpxKFDh2BtbQ1vb2+xSyEiIj2gl8OvSftkZGTg008/xXvvvYchQ4bgxYsXYpdEREQGhJeWqNgePnyIwYMH4+LFiwCAgQMHQiqVilwVEREZEgYZKpZ9+/Zh1KhRiI2NhVQqxc8//4x+/fqJXRYRERkYXlqiIsnKysK0adPw/vvvIzY2Fi1btkRYWBhDDBERiYJBhorEyMgI0dHRAOTz/vz1119wd3cXuSoiIjJUHLVEasnMzES5cvIrkfHx8Th//jy6d+8uclVERKSvOGqJNCI1NRV+fn4YOHAgFJnX1taWIYaIiLQCO/tSvu7evYvBgwcjLCwMAHD+/Hm0adNG5KqIiIjeYosM5Wnnzp3w9PREWFgYHBwccPjwYYYYIiLSOgwypCIlJQXjx4/HBx98gISEBLRr1w7h4eHo0aOH2KURERHlwiBDKgYNGoR169ZBIpFg3rx5OHnyJFxcXMQui4iIKE/sI0Mq5syZg7CwMGzcuBFdunQRuxwiIqICMcgYuOTkZFy+fBnt27cHAHh5eeH+/fswMzMTuTIiIqLC8dKSAbt16xZatGiB7t274+bNm8rtDDFERKQrGGQMkCAI2LhxI5o3b45bt25BKpUiPj5e7LKIiIiKjEHGwCQmJmLEiBEYPXo0UlJS0LVrV1y7dg2tW7cWuzQiIqIiY5AxINevX0eLFi2wdetWGBkZ4csvv8SRI0fg6OgodmlERETFws6+BmTfvn24c+cOXFxcsGPHDrRr107skoiIiEqEQcaAzJ07F+np6Zg2bRocHBzELoeIiKjEeGlJj4WFhWHQoEFITU0FABgbG+OLL75giCEiIr3BIKOHBEHA999/j3feeQd79uzBF198IXZJREREpYKXlvRMXFwcPvroI+zduxcA0Lt3b8yYMUPkqoiIiEoHW2T0yOXLl9G0aVPs3bsXJiYmWLlyJfbt2wd7e3uxSyMiIioVbJHRE3v27MGHH36IjIwMVKtWDbt27ULLli3FLouIiKhUMcjoiRYtWsDKygqdOnXCTz/9BDs7O7FLIiIiKnUMMjrs6dOncHFxAQBUq1YNV65cgbu7OyQSiciVERERlQ32kdFBMpkMX3/9NapXr44jR44ot1evXp0hhoiIDAqDjI559eoVevfujZkzZyI9PR0HDx4UuyQiIiLR8NKSDjl37hyGDBmCJ0+ewMzMDKtXr8b48ePFLouIiEg0bJHRATKZDEuWLIG3tzeePHmCWrVq4cKFC5gwYQIvJRERkUFjkNEBwcHBCAgIQFZWFnx9fXHlyhV4eHiIXRYREZHoeGlJB3Tp0gWTJ0+Gh4cHRo8ezVYYIiKi/zDIaKGsrCysXr0aw4YNg6OjIwDg22+/FbkqIiIi7cNLS1omKioK3bp1w8yZMzFs2DDIZDKxSyIiItJabJHRIsHBwfjwww8RHR0NS0tL+Pr6wsiIWZOIiCg/DDJaICsrC59//jkWL14MQRDQsGFD7Nq1C/Xr1xe7NCIiUoMgCMjMzERWVpbYpegMY2NjlCtXrsT9PhlkRBYdHY3BgwcjJCQEAPDRRx9hzZo1sLS0FLkyIiJSR3p6Op4/f47k5GSxS9E5lpaWcHZ2hqmpabGfg0FGZBYWFnj69Cmsra3x448/wtfXV+ySiIhITTKZDJGRkTA2NkblypVhamrKkaVqEAQB6enpePnyJSIjI1GrVq1id6VgkBFBZmYmjI2NIZFIYGtri19//RXm5uaoXbu22KUREVERpKenQyaToWrVqmxJLyILCwuYmJjg4cOHSE9Ph7m5ebGehz1Jy9iTJ0/QsWNHfPfdd8ptjRs3ZoghItJhHJhRPJo4bzzzZejw4cPw8PDA2bNnsXjxYiQmJopdEhERkU5jkCkDGRkZ8Pf3x7vvvotXr17B09MToaGhsLa2Frs0IiIincYgU8oePnyI9u3bY/ny5QCAjz/+GOfOnUONGjVEroyIiAxVVlYWWrdujX79+qlsj4uLQ9WqVTF37twiPd+YMWPQqFEjpKenq2w/fPgwTE1NcfXq1RLXnB8GmVKUkJCAFi1a4MKFC7Czs8Nvv/2GNWvWwMzMTOzSiIjIgBkbG2PTpk04evQotm3bptw+ZcoU2NvbY+HChUV6vlWrViEhIUHlcbGxsRg7dizmz5+PZs2aaaz2nBhkSpGNjQ0++eQTtGzZEmFhYXj//ffFLomIiAgAULt2bSxZsgRTpkzB8+fPsX//fuzcuRNbtmwp8rwutra22LhxI77++mtcvHgRADBt2jS4uLggICCgNMpXkgiCIJTqK4gsPj4eUqkUcXFxsLW1LfXXe/DgAdLT01G3bl0A8jkGMjMzSzTZDxERaafU1FRERkbC3d091/DhpKSkfB9nbGyscnxBxxoZGcHCwqLQY62srNQtW0kQBHTq1AnGxsa4ceMGpkyZgnnz5in3F9aXc+jQoVi7dq3y/rRp03D06FEsXrwYI0aMwNWrV5Xfh3kp6Pyp+/3NeWQ06Ndff8Xo0aNRpUoV/O9//4OVlRWMjIwYYoiIDFBBIaBnz544dOiQ8r6jo2O+MwN7e3vj9OnTyvvVqlVDTExMruOK0y4hkUgQFBSEevXqoVGjRpg9e7bK/vDw8AIfnzNgBAYG4ujRoxgyZAi+/vrrAkOMpoh6aenMmTPo1asXKleuDIlEgn379qnsFwQBCxYsgLOzMywsLNC5c2fcvXtXnGILkJqaismTJ2PAgAGIj4+HnZ0dEhISxC6LiIioUD///DMsLS0RGRmJJ0+eqOyrWbNmgTdHR0eV4y0sLDBz5kxYWlpi6tSpZVK/qC0ySUlJaNKkCUaPHp2r5zQALFu2DN988w02b94Md3d3zJ8/H926dcOtW7eKPQOgpt29exeDBw9GWFgYAGDWrFlYvHgxTExMRK6MiIjEVNBcYcbGxir3X7x4ke+xOSeN+/fff0tUV3bnz5/HqlWrcPz4cXzxxRcYM2YMTpw4oVxmoaiXlgCgXLlyytnry4KoQaZHjx7o0aNHnvsEQcDq1asxb9489OnTBwCwZcsWVKpUCfv27cOQIUPKstQ87dy5E+PGjUNCQgIcHBywZcuWfH8fIiIyLEXps1JaxxYkOTkZI0eOxMSJE9GxY0e4u7ujUaNGWLt2LSZOnAig6JeWxKC1fWQiIyMRFRWFzp07K7dJpVK0atUKoaGh+QaZtLQ0pKWlKe/Hx8eXSn0ymQxr165FQkIC2rVrhx07dsDFxaVUXouIiEjTAgICIAgClixZAkDe92bFihWYOXMmevTogWrVqqFmzZoiV1k4rR1+HRUVBQCoVKmSyvZKlSop9+UlMDAQUqlUeatatWqp1GdkZITt27dj8eLFOHnyJEMMERHpjJCQEHz//ffYuHGjymKX48ePR+vWrTFmzJhidR4Wg9YGmeIKCAhAXFyc8vb48eNSe63KlStj3rx5KFdOaxu2iIiIcvH29kZmZibatm2ba9+xY8cQHBxc7D4uI0eORGxsbAkrVJ/WBhknJycAQHR0tMr26Oho5b68mJmZwdbWVuVGRERE+klrg4y7uzucnJwQHBys3BYfH4+LFy/Cy8tLxMqIiIhIW4h6TSQxMRH37t1T3o+MjER4eDjs7e3h6uqKadOm4YsvvkCtWrWUw68rV66Mvn37ilc0ERERaQ1Rg8zly5fRsWNH5f0ZM2YAAEaMGIFNmzbB398fSUlJGDduHGJjY9G2bVscPXpUa+aQISIiInFxrSUiIqJiKmitICqcJtZa0to+MkRERLpCz9sESo0mzhuDDBERUTEplqPJb8FHKpjivJVkWR9OgEJERFRMxsbGsLOzU66VZGlpWWZrDOkyQRCQnJyMFy9ewM7OLtfaU0XBIENERFQCirnNClr4kfJmZ2dX4Nxw6mCQISIiKgGJRAJnZ2c4OjoiIyND7HJ0homJSYlaYhQYZIiIiDTA2NhYI1/MVDTs7EtEREQ6i0GGiIiIdBaDDBEREeksve8jo5hsJz4+XuRKiIiISF2K7+3CJs3T+yCTkJAAAKhatarIlRAREVFRJSQkQCqV5rtf79dakslkePbsGWxsbDQ+SVF8fDyqVq2Kx48fcx2nYuD5Kzmew5Lh+SsZnr+S4znMnyAISEhIQOXKlWFklH9PGL1vkTEyMkKVKlVK9TVsbW35BiwBnr+S4zksGZ6/kuH5Kzmew7wV1BKjwM6+REREpLMYZIiIiEhnMciUgJmZGRYuXAgzMzOxS9FJPH8lx3NYMjx/JcPzV3I8hyWn9519iYiISH+xRYaIiIh0FoMMERER6SwGGSIiItJZDDJERESksxhkCnHmzBn06tULlStXhkQiwb59+1T2C4KABQsWwNnZGRYWFujcuTPu3r0rTrFaqrBzOHLkSEgkEpVb9+7dxSlWCwUGBqJFixawsbGBo6Mj+vbti4iICJVjUlNT4efnhwoVKsDa2hr9+/dHdHS0SBVrF3XOX4cOHXK9BydMmCBSxdonKCgIjRs3Vk7a5uXlhSNHjij38/1XsMLOH99/JcMgU4ikpCQ0adIE33//fZ77ly1bhm+++QZr167FxYsXYWVlhW7duiE1NbWMK9VehZ1DAOjevTueP3+uvO3YsaMMK9RuISEh8PPzw4ULF/Dnn38iIyMDXbt2RVJSkvKY6dOn4+DBg9izZw9CQkLw7Nkz9OvXT8SqtYc65w8Axo4dq/IeXLZsmUgVa58qVapgyZIluHLlCi5fvoxOnTqhT58++PvvvwHw/VeYws4fwPdfiQikNgDC77//rrwvk8kEJycnYfny5cptsbGxgpmZmbBjxw4RKtR+Oc+hIAjCiBEjhD59+ohSjy568eKFAEAICQkRBEH+njMxMRH27NmjPOb27dsCACE0NFSsMrVWzvMnCILg7e0tTJ06VbyidFD58uWFDRs28P1XTIrzJwh8/5UUW2RKIDIyElFRUejcubNym1QqRatWrRAaGipiZbrn9OnTcHR0RJ06dTBx4kS8evVK7JK0VlxcHADA3t4eAHDlyhVkZGSovA/r1q0LV1dXvg/zkPP8KWzbtg0ODg5o2LAhAgICkJycLEZ5Wi8rKws7d+5EUlISvLy8+P4ropznT4Hvv+LT+0UjS1NUVBQAoFKlSirbK1WqpNxHhevevTv69esHd3d33L9/H3PmzEGPHj0QGhoKY2NjscvTKjKZDNOmTUObNm3QsGFDAPL3oampKezs7FSO5fswt7zOHwD4+vrCzc0NlStXxvXr1zFr1ixERETgt99+E7Fa7XLjxg14eXkhNTUV1tbW+P3331G/fn2Eh4fz/aeG/M4fwPdfSTHIkOiGDBmi/LlRo0Zo3LgxatSogdOnT8PHx0fEyrSPn58fbt68ibNnz4pdik7K7/yNGzdO+XOjRo3g7OwMHx8f3L9/HzVq1CjrMrVSnTp1EB4ejri4OOzduxcjRoxASEiI2GXpjPzOX/369fn+KyFeWioBJycnAMjVOz86Olq5j4quevXqcHBwwL1798QuRatMnjwZf/zxB06dOoUqVaootzs5OSE9PR2xsbEqx/N9qCq/85eXVq1aAQDfg9mYmpqiZs2a8PT0RGBgIJo0aYI1a9bw/aem/M5fXvj+KxoGmRJwd3eHk5MTgoODldvi4+Nx8eJFlWufVDRPnjzBq1ev4OzsLHYpWkEQBEyePBm///47Tp48CXd3d5X9np6eMDExUXkfRkRE4NGjR3wfovDzl5fw8HAA4HuwADKZDGlpaXz/FZPi/OWF77+i4aWlQiQmJqqk4sjISISHh8Pe3h6urq6YNm0avvjiC9SqVQvu7u6YP38+KleujL59+4pXtJYp6Bza29tj0aJF6N+/P5ycnHD//n34+/ujZs2a6Natm4hVaw8/Pz9s374d+/fvh42NjbLfgVQqhYWFBaRSKcaMGYMZM2bA3t4etra2mDJlCry8vPDOO++IXL34Cjt/9+/fx/bt29GzZ09UqFAB169fx/Tp09G+fXs0btxY5Oq1Q0BAAHr06AFXV1ckJCRg+/btOH36NI4dO8b3nxoKOn98/2mA2MOmtN2pU6cEALluI0aMEARBPgR7/vz5QqVKlQQzMzPBx8dHiIiIELdoLVPQOUxOTha6du0qVKxYUTAxMRHc3NyEsWPHClFRUWKXrTXyOncAhI0bNyqPSUlJESZNmiSUL19esLS0FN5//33h+fPn4hWtRQo7f48ePRLat28v2NvbC2ZmZkLNmjWFTz/9VIiLixO3cC0yevRowc3NTTA1NRUqVqwo+Pj4CMePH1fu5/uvYAWdP77/Sk4iCIJQlsGJiIiISFPYR4aIiIh0FoMMERER6SwGGSIiItJZDDJERESksxhkiIiISGcxyBAREZHOYpAhIiIincUgQ0RERDqLQYaINEYikRR4++yzz8qslg4dOqi8dqVKlTBw4EA8fPhQecy///6rcoyNjQ0aNGgAPz8/3L17V+X5Nm3aBDs7uzKrn4jUwyBDRBrz/Plz5W316tWwtbVV2TZz5kzlsYIgIDMzs1TrGTt2LJ4/f45nz55h//79ePz4MYYOHZrruBMnTuD58+e4du0avvrqK9y+fRtNmjRRWQiRiLQTgwwRaYyTk5PyJpVKIZFIlPfv3LkDGxsbHDlyBJ6enjAzM8PZs2cxcuTIXIusTps2DR06dFDel8lkCAwMhLu7OywsLNCkSRPs3bu30HosLS3h5OQEZ2dnvPPOO5g8eTKuXr2a67gKFSrAyckJ1atXR58+fXDixAm0atUKY8aMQVZWVklPCxGVIgYZIipTs2fPxpIlS3D79m21V/cNDAzEli1bsHbtWvz999+YPn06hg4dipCQELVf9/Xr19i9ezdatWpV6LFGRkaYOnUqHj58iCtXrqj9GkRU9sqJXQARGZbPP/8cXbp0Ufv4tLQ0fPXVVzhx4gS8vLwAANWrV8fZs2fx448/wtvbO9/H/vDDD9iwYQMEQUBycjJq166NY8eOqfW6devWBSDvR9OyZUu16yWissUWGSIqU82bNy/S8ffu3UNycjK6dOkCa2tr5W3Lli24f/9+gY/98MMPER4ejmvXruHs2bOoWbMmunbtioSEhEJfVxAEAPIOzESkvdgiQ0RlysrKSuW+kZGRMjQoZGRkKH9OTEwEABw6dAguLi4qx5mZmRX4WlKpFDVr1gQA1KxZEz/99BOcnZ2xa9cufPTRRwU+9vbt2wAAd3f3Ao8jInExyBCRqCpWrIibN2+qbAsPD4eJiQkAoH79+jAzM8OjR48KvIykDmNjYwBASkpKgcfJZDJ88803cHd3R9OmTUv0mkRUuhhkiEhUnTp1wvLly7FlyxZ4eXnhl19+wc2bN5UBwsbGBjNnzsT06dMhk8nQtm1bxMXF4dy5c7C1tcWIESPyfe7k5GRERUUBAKKjo7F48WKYm5uja9euKse9evUKUVFRSE5Oxs2bN7F69Wr873//w6FDh5Thh4i0E4MMEYmqW7dumD9/Pvz9/ZGamorRo0dj+PDhuHHjhvKYxYsXo2LFiggMDMSDBw9gZ2eHZs2aYc6cOQU+9/r167F+/XoAQPny5dG4cWMcPnwYderUUTmuc+fOAOTDtd3c3NCxY0esW7dOeVmKiLSXRMh5cZqIiIhIR3DUEhEREeksBhkiIiLSWQwyREREpLMYZIiIiEhnMcgQERGRzmKQISIiIp3FIENEREQ6i0GGiIiIdBaDDBEREeksBhkiIiLSWQwyREREpLMYZIiIiEhn/R9G8vNlK7npsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "true = []\n",
    "pred = []\n",
    "\n",
    "for fold in best_set:\n",
    "    true.extend(fold[0])\n",
    "    pred.extend(fold[1])\n",
    "\n",
    "print(r2_score(true, pred))\n",
    "print(pearsonr(true, pred))\n",
    "print(mean_absolute_error(true, pred))\n",
    "\n",
    "# Generate jittered data\n",
    "jitter = 0.01  # Adjust the jittering amount as needed\n",
    "true_jittered = np.array(true) + np.random.uniform(low=-jitter, high=jitter, size=len(true))\n",
    "pred_jittered = np.array(pred) + np.random.uniform(low=-jitter, high=jitter, size=len(pred))\n",
    "\n",
    "plt.scatter(true_jittered, pred_jittered, alpha=0.7, color='red', marker='x')\n",
    "\n",
    "min_val = min(min(true), min(pred))\n",
    "max_val = max(max(true), max(pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='X=Y')\n",
    "\n",
    "plt.xlabel('True BDI')\n",
    "plt.ylabel('Predicted BDI')\n",
    "plt.title('Same T Prediction (before)')\n",
    "plt.legend()\n",
    "plt.savefig('same_t_pred_before.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Specify the filename for the CSV file\n",
    "filename = 'same-t-pred-before-vae.csv'\n",
    "\n",
    "# Create a list of rows with headers\n",
    "rows = [['true_post_bdi', 'predicted_post_bdi']]\n",
    "for true, pred in zip(true, pred):\n",
    "    rows.append([true, pred])\n",
    "\n",
    "import csv\n",
    "\n",
    "# Write the rows to the CSV file\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
