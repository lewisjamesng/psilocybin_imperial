{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, LeakyReLU\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "class EdgeModel(torch.nn.Module):\n",
    "    def __init__(self, nf, ef, gf, hidden_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = Seq(Lin(nf * 2 + ef + gf + 2, hidden_dim),\n",
    "                            LeakyReLU(),\n",
    "                            nn.Dropout(p=dropout),\n",
    "                            Lin(hidden_dim, ef))\n",
    "\n",
    "    def forward(self, src, dest, edge_index, edge_attr, u, batch):\n",
    "        # src, dest: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = torch.cat([src, dest, edge_index.t(), edge_attr, u.expand(src.shape[0], -1)], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self, nf, ef, gf, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.node_mlp_1 = Seq(Lin(nf + ef + 1, hidden_dim), \n",
    "                              ReLU(), \n",
    "                              nn.Dropout(p=dropout), \n",
    "                              Lin(hidden_dim, hidden_dim)\n",
    "                             )\n",
    "        self.node_mlp_2 = Seq(Lin(nf + hidden_dim + gf, hidden_dim),\n",
    "                              LeakyReLU(),\n",
    "                              nn.Dropout(p=dropout),\n",
    "                              Lin(hidden_dim, nf))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[row], row.view(-1, 1), edge_attr], dim=1)\n",
    "\n",
    "        out = self.node_mlp_1(out)\n",
    "        \n",
    "        out = scatter(out, col, dim=0, dim_size=x.size(0),\n",
    "                      reduce='sum')\n",
    "        \n",
    "        out = out / torch.norm(out, p=2)\n",
    "        \n",
    "        out = torch.cat([x, out, u.expand(x.shape[0], -1)], dim=1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "class GlobalModel(torch.nn.Module):\n",
    "    def __init__(self, gf, num_nodes, output_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.global_mlp = Seq(Lin(gf + num_nodes * nf, hidden_dim),\n",
    "                              LeakyReLU(), \n",
    "                              nn.Dropout(p=dropout),\n",
    "                              Lin(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        \n",
    "        out = torch.cat([\n",
    "            u,\n",
    "            x.t(),\n",
    "        ], dim=1)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "class MetaLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_model: Optional[torch.nn.Module] = None,\n",
    "        node_model: Optional[torch.nn.Module] = None,\n",
    "        global_model: Optional[torch.nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.edge_model = edge_model\n",
    "        self.node_model = node_model\n",
    "        self.global_model = global_model\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        for item in [self.node_model, self.edge_model, self.global_model]:\n",
    "            if hasattr(item, 'reset_parameters'):\n",
    "                item.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        edge_attr: Optional[Tensor] = None,\n",
    "        u: Optional[Tensor] = None,\n",
    "        batch: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
    "        \n",
    "        row = edge_index[0]\n",
    "        col = edge_index[1]\n",
    "\n",
    "        if self.edge_model is not None:\n",
    "            edge_attr = self.edge_model(x[row], x[col], edge_index, edge_attr, u,\n",
    "                                        batch if batch is None else batch[row])\n",
    "\n",
    "        if self.node_model is not None:\n",
    "            x = self.node_model(x, edge_index, edge_attr, u, batch)\n",
    "\n",
    "        if self.global_model is not None:\n",
    "            u = self.global_model(x, edge_index, edge_attr, u, batch)\n",
    "\n",
    "        return x, edge_attr, u\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(\\n'\n",
    "                f'  edge_model={self.edge_model},\\n'\n",
    "                f'  node_model={self.node_model},\\n'\n",
    "                f'  global_model={self.global_model}\\n'\n",
    "                f')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, Dropout, LeakyReLU\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "def meta_layer(nf, ef, gf, num_nodes, hidden_dim, output_dim, dropout=0.1, encode=False):\n",
    "    edge_model = EdgeModel(nf, ef, gf, hidden_dim, num_layers=4, dropout=dropout)\n",
    "    node_model = NodeModel(nf, ef, gf, hidden_dim, dropout=dropout)\n",
    "    global_model = GlobalModel(gf, num_nodes, output_dim, dropout=dropout)\n",
    "    return MetaLayer(\n",
    "        edge_model=None if encode else edge_model,\n",
    "        node_model=None if encode else node_model,\n",
    "        global_model=global_model)\n",
    "\n",
    "class VGAE(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, nf, ef, gf, hidden_dim, latent_dim=64, dropout_rate=0.1, l2_factor=0.01):\n",
    "        super(VGAE, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        self.edge_index = torch.triu(torch.ones((input_dim, input_dim)), diagonal=1).nonzero(as_tuple=False).t()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fake_x = torch.ones((num_nodes, nf))\n",
    "        self.fake_edge_attr = torch.ones((self.edge_index.shape[1], ef))\n",
    "        \n",
    "        self.meta_encode_1 = meta_layer(nf, ef, gf, num_nodes, hidden_dim, hidden_dim, dropout=dropout_rate)\n",
    "        self.meta_encode_2 = meta_layer(nf, ef, hidden_dim, num_nodes, hidden_dim, hidden_dim, dropout=dropout_rate)\n",
    "        self.meta_encode_3 = meta_layer(nf, ef, hidden_dim, num_nodes, hidden_dim, 2 * latent_dim, dropout=dropout_rate)\n",
    "    \n",
    "        self.meta_decode_e = Seq(Lin(latent_dim, 256), nn.LeakyReLU(), Lin(256, 256), nn.LeakyReLU(), Lin(256, 4950))\n",
    "        self.meta_decode_x = Seq(Lin(latent_dim, 64), nn.LeakyReLU(), Lin(64, 64), nn.LeakyReLU(), Lin(64, 100))\n",
    "        \n",
    "        self.dropout = Dropout(p=dropout_rate)\n",
    "        self.l2_factor = l2_factor\n",
    "\n",
    "    def encode(self, x, edge_attr, u):\n",
    "        x, edge_attr, u = self.meta_encode_1(x, self.edge_index, edge_attr=edge_attr, u=u)\n",
    "        x, edge_attr, u = self.meta_encode_2(x, self.edge_index, edge_attr=edge_attr, u=u)\n",
    "        x, edge_attr, enc = self.meta_encode_3(x, self.edge_index, edge_attr=edge_attr, u=u)\n",
    "        \n",
    "        mu = enc[:,:latent_dim]\n",
    "        logvar = enc[:,latent_dim:]\n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        edge_attr = self.meta_decode_e(z)\n",
    "        x = self.meta_decode_x(z)\n",
    "        u = None\n",
    "        return x, edge_attr.t(), u\n",
    "\n",
    "    def forward(self, x, edge_attr, u):\n",
    "        x_mag = torch.norm(x, p=2)\n",
    "        x = x / x_mag\n",
    "        \n",
    "        edge_mag = torch.norm(edge_attr, p=2)\n",
    "        edge_attr = edge_attr / edge_mag\n",
    "        \n",
    "        mu, logvar = self.encode(x, edge_attr=edge_attr, u=u)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x, edge_attr, u = self.decode(mu)\n",
    "        \n",
    "        x = x * x_mag\n",
    "        edge_attr = edge_attr * edge_mag\n",
    "        \n",
    "        return x, edge_attr, u, mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, recon_edge_attr, edge_attr, mu, logvar):\n",
    "        x_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        edge_attr_loss = F.mse_loss(recon_edge_attr, edge_attr, reduction='sum')\n",
    "        recon_loss = edge_attr_loss  + x_loss\n",
    "\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_reg = 0.\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2) ** 2\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = recon_loss #+ kl_divergence #+ self.l2_factor * l2_reg\n",
    "        return total_loss / self.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the hyperparameters\n",
    "input_dim = 100  # size of the graph adjacency matrix\n",
    "latent_dim = 64\n",
    "lr = 1e-3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import BrainGraphDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from utils import BrainGraphDataset, get_data_labels\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "dataroot = 'fc_matrices/hcp_100_ica/'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "categories = ['patient_n','condition','bdi_before']\n",
    "\n",
    "batch_graph_size = 1\n",
    "\n",
    "data_labels = get_data_labels()\n",
    "data_labels = data_labels[categories]\n",
    "\n",
    "data_labels.loc[data_labels[\"condition\"] == \"P\", \"condition\"] = 1\n",
    "data_labels.loc[data_labels[\"condition\"] == \"E\", \"condition\"] = -1\n",
    "data_labels['condition'] = data_labels['condition'].astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=cwd + dataroot + annotations,\n",
    "                            transform=None, extra_data=data_labels, setting='lz')\n",
    "\n",
    "dataroot = 'fc_matrices/psilo_ica_100_before/'\n",
    "psilo_dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=cwd + annotations,\n",
    "                            transform=None, extra_data=None, setting='lz')\n",
    "\n",
    "psilo_train_loader = DataLoader(psilo_dataset, batch_size=batch_graph_size)\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset, batch_size=batch_graph_size, shuffle=True)\n",
    "val_loader = psilo_train_loader\n",
    "\n",
    "best_val_loss = float('inf')  # set to infinity to start\n",
    "best_model_state = None\n",
    "\n",
    "# define a dictionary to store the loss curves for each configuration\n",
    "loss_curves = {}\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "nf = 1\n",
    "ef = 1\n",
    "gf = 2\n",
    "\n",
    "hidden_dim = 32\n",
    "\n",
    "model = VGAE(100, nf, ef, gf, hidden_dim, latent_dim, dropout_rate=0).to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "# print(model)        \n",
    "        \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_encode_1.edge_model.edge_mlp.0.weight: True\n",
      "meta_encode_1.edge_model.edge_mlp.0.bias: True\n",
      "meta_encode_1.edge_model.edge_mlp.3.weight: True\n",
      "meta_encode_1.edge_model.edge_mlp.3.bias: True\n",
      "meta_encode_1.node_model.node_mlp_1.0.weight: True\n",
      "meta_encode_1.node_model.node_mlp_1.0.bias: True\n",
      "meta_encode_1.node_model.node_mlp_1.3.weight: True\n",
      "meta_encode_1.node_model.node_mlp_1.3.bias: True\n",
      "meta_encode_1.node_model.node_mlp_2.0.weight: True\n",
      "meta_encode_1.node_model.node_mlp_2.0.bias: True\n",
      "meta_encode_1.node_model.node_mlp_2.3.weight: True\n",
      "meta_encode_1.node_model.node_mlp_2.3.bias: True\n",
      "meta_encode_1.global_model.global_mlp.0.weight: True\n",
      "meta_encode_1.global_model.global_mlp.0.bias: True\n",
      "meta_encode_1.global_model.global_mlp.3.weight: True\n",
      "meta_encode_1.global_model.global_mlp.3.bias: True\n",
      "meta_encode_2.edge_model.edge_mlp.0.weight: True\n",
      "meta_encode_2.edge_model.edge_mlp.0.bias: True\n",
      "meta_encode_2.edge_model.edge_mlp.3.weight: True\n",
      "meta_encode_2.edge_model.edge_mlp.3.bias: True\n",
      "meta_encode_2.node_model.node_mlp_1.0.weight: True\n",
      "meta_encode_2.node_model.node_mlp_1.0.bias: True\n",
      "meta_encode_2.node_model.node_mlp_1.3.weight: True\n",
      "meta_encode_2.node_model.node_mlp_1.3.bias: True\n",
      "meta_encode_2.node_model.node_mlp_2.0.weight: True\n",
      "meta_encode_2.node_model.node_mlp_2.0.bias: True\n",
      "meta_encode_2.node_model.node_mlp_2.3.weight: True\n",
      "meta_encode_2.node_model.node_mlp_2.3.bias: True\n",
      "meta_encode_2.global_model.global_mlp.0.weight: True\n",
      "meta_encode_2.global_model.global_mlp.0.bias: True\n",
      "meta_encode_2.global_model.global_mlp.3.weight: True\n",
      "meta_encode_2.global_model.global_mlp.3.bias: True\n",
      "meta_encode_3.edge_model.edge_mlp.0.weight: True\n",
      "meta_encode_3.edge_model.edge_mlp.0.bias: True\n",
      "meta_encode_3.edge_model.edge_mlp.3.weight: True\n",
      "meta_encode_3.edge_model.edge_mlp.3.bias: True\n",
      "meta_encode_3.node_model.node_mlp_1.0.weight: True\n",
      "meta_encode_3.node_model.node_mlp_1.0.bias: True\n",
      "meta_encode_3.node_model.node_mlp_1.3.weight: True\n",
      "meta_encode_3.node_model.node_mlp_1.3.bias: True\n",
      "meta_encode_3.node_model.node_mlp_2.0.weight: True\n",
      "meta_encode_3.node_model.node_mlp_2.0.bias: True\n",
      "meta_encode_3.node_model.node_mlp_2.3.weight: True\n",
      "meta_encode_3.node_model.node_mlp_2.3.bias: True\n",
      "meta_encode_3.global_model.global_mlp.0.weight: True\n",
      "meta_encode_3.global_model.global_mlp.0.bias: True\n",
      "meta_encode_3.global_model.global_mlp.3.weight: True\n",
      "meta_encode_3.global_model.global_mlp.3.bias: True\n",
      "meta_decode_e.0.weight: True\n",
      "meta_decode_e.0.bias: True\n",
      "meta_decode_e.2.weight: True\n",
      "meta_decode_e.2.bias: True\n",
      "meta_decode_e.4.weight: True\n",
      "meta_decode_e.4.bias: True\n",
      "meta_decode_x.0.weight: True\n",
      "meta_decode_x.0.bias: True\n",
      "meta_decode_x.2.weight: True\n",
      "meta_decode_x.2.bias: True\n",
      "meta_decode_x.4.weight: True\n",
      "meta_decode_x.4.bias: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_requires_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: {param.requires_grad}')\n",
    "\n",
    "# Print the requires_grad attribute for all modules\n",
    "print_requires_grad(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:23<15:33, 23.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Train Loss: 1.9071 - Val Loss: 4.1507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:48<15:20, 24.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Train Loss: 0.4962 - Val Loss: 3.9107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [01:12<14:58, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Train Loss: 0.4825 - Val Loss: 3.8455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [01:37<14:34, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Train Loss: 0.4846 - Val Loss: 3.8396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [02:01<14:11, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - Train Loss: 0.4842 - Val Loss: 3.8467\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [02:25<13:46, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - Train Loss: 0.4840 - Val Loss: 3.8847\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [02:50<13:22, 24.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - Train Loss: 0.4840 - Val Loss: 3.8940\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [02:59<14:05, 25.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m graph[:, model\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m0\u001b[39m], model\u001b[38;5;241m.\u001b[39medge_index[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mto(device)        \n\u001b[1;32m     24\u001b[0m u \u001b[38;5;241m=\u001b[39m base_bdi\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m recon_x, recon_edge_attr, recon_u, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function(recon_x\u001b[38;5;241m.\u001b[39mview(lz\u001b[38;5;241m.\u001b[39mshape), lz, recon_edge_attr\u001b[38;5;241m.\u001b[39mview(edge_attr\u001b[38;5;241m.\u001b[39mshape), edge_attr, mu, logvar)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m, in \u001b[0;36mVGAE.forward\u001b[0;34m(self, x, edge_attr, u)\u001b[0m\n\u001b[1;32m     61\u001b[0m edge_mag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(edge_attr, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     62\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m edge_attr \u001b[38;5;241m/\u001b[39m edge_mag\n\u001b[0;32m---> 64\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m     66\u001b[0m x, edge_attr, u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(mu)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mVGAE.encode\u001b[0;34m(self, x, edge_attr, u)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_attr, u):\n\u001b[1;32m     37\u001b[0m     x, edge_attr, u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_encode_1(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, u\u001b[38;5;241m=\u001b[39mu)\n\u001b[0;32m---> 38\u001b[0m     x, edge_attr, u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_encode_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     x, edge_attr, enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_encode_3(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, u\u001b[38;5;241m=\u001b[39mu)\n\u001b[1;32m     41\u001b[0m     mu \u001b[38;5;241m=\u001b[39m enc[:,:latent_dim]\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mMetaLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr, u, batch)\u001b[0m\n\u001b[1;32m     36\u001b[0m col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model(x, edge_index, edge_attr, u, batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mEdgeModel.forward\u001b[0;34m(self, src, dest, edge_index, edge_attr, u, batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, dest, edge_index, edge_attr, u, batch):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# src, dest: [E, F_x], where E is the number of edges.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# edge_attr: [E, F_e]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# u: [B, F_u], where B is the number of graphs.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# batch: [E] with max entry B - 1.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mlp(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    \n",
    "    batch_counter = 0\n",
    "    loss = 0.\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        \n",
    "        data = data  # move data to device\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (graph, lz, base_bdi) = data\n",
    "        graph = graph.to(device)\n",
    "        lz = lz.t().to(device)\n",
    "\n",
    "        edge_attr = graph[:, model.edge_index[0], model.edge_index[1]].t().to(device)        \n",
    "        \n",
    "        u = base_bdi.to(device)\n",
    "        \n",
    "        recon_x, recon_edge_attr, recon_u, mu, logvar = model(lz, edge_attr, u)\n",
    "\n",
    "        loss += model.loss_function(recon_x.view(lz.shape), lz, recon_edge_attr.view(edge_attr.shape), edge_attr, mu, logvar)\n",
    "        \n",
    "        if batch_idx % batch_size == 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            loss = 0.\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(val_loader):\n",
    "            (graph, lz, base_bdi) = data\n",
    "            graph = graph.to(device)\n",
    "            lz = lz.t().to(device)\n",
    "\n",
    "            edge_attr = graph[:, model.edge_index[0], model.edge_index[1]].t().to(device)        \n",
    "            u = base_bdi.to(device)\n",
    "\n",
    "            recon_x, recon_edge_attr, recon_u, mu, logvar = model(lz, edge_attr, u)\n",
    "            \n",
    "            loss = model.loss_function(recon_x.view(lz.shape), lz, recon_edge_attr.view(edge_attr.shape), edge_attr, mu, logvar)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    # append losses to lists\n",
    "    train_losses.append(train_loss/len(dataset))\n",
    "    val_losses.append(val_loss/len(psilo_dataset))\n",
    "\n",
    "    # save the model if the validation loss is at its minimum\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    # print the lossestorch.nn.init.xavier_uniform_\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\\n')\n",
    "    with open('gmm_train.txt', 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\\n')\n",
    "\n",
    "# save the best model for this configuration\n",
    "torch.save(best_model_state, f'vgae_weights/meta_layers_best_overfit.pt')\n",
    "\n",
    "# add the loss curves to the dictionary\n",
    "loss_curves[f\"loss_curves\"] = {\"train_loss\": train_losses, \"val_loss\": val_losses}\n",
    "\n",
    "# save the loss curves to a file\n",
    "with open(\"loss_curves_meta_layers.json\", \"w\") as f:\n",
    "    json.dump(loss_curves, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"loss_curves_meta_layers.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "for n_comp, loss_dict in loss_curves.items():\n",
    "\n",
    "    val_losses = loss_dict[\"val_loss\"]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "    plt.plot(epochs, val_losses, label=f\"{n_comp}\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Curves for MetaLayers\")\n",
    "plt.legend()\n",
    "plt.ylim((1500, 3000))\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"loss_curves_meta_layers.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "for n_comp, loss_dict in loss_curves.items():\n",
    "    val_losses = loss_dict[\"train_loss\"]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "    plt.plot(epochs, val_losses, label=f\"{n_comp}\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Curves for MetaLayers\")\n",
    "plt.legend()\n",
    "plt.ylim((0, 100))\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         u \u001b[38;5;241m=\u001b[39m base_bdi\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m         recon_x, recon_edge_attr, recon_u, mu, logvar \u001b[38;5;241m=\u001b[39m model(lz, edge_attr, u)\n\u001b[0;32m---> 19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function(recon_x\u001b[38;5;241m.\u001b[39mview(lz\u001b[38;5;241m.\u001b[39mshape), lz, recon_edge_attr\u001b[38;5;241m.\u001b[39mview(edge_attr\u001b[38;5;241m.\u001b[39mshape), edge_attr, \u001b[43mrecon_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(u\u001b[38;5;241m.\u001b[39mshape), u, mu, logvar)\n\u001b[1;32m     21\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print the validation loss \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "# load the weights\n",
    "# model.load_state_dict(torch.load(f'vgae_weights/meta_layers_best_overfit.pt', map_location=device))\n",
    "\n",
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# calculate the validation loss\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        (graph, lz, base_bdi) = data\n",
    "        graph = graph.to(device)\n",
    "        lz = lz.t().to(device)\n",
    "\n",
    "        edge_attr = graph[:, model.edge_index[0], model.edge_index[1]].t().to(device)        \n",
    "        u = base_bdi.to(device)\n",
    "\n",
    "        recon_x, recon_edge_attr, recon_u, mu, logvar = model(lz, edge_attr, u)\n",
    "        loss = model.loss_function(recon_x.view(lz.shape), lz, recon_edge_attr.view(edge_attr.shape), edge_attr, recon_u.view(u.shape), u, mu, logvar)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "# print the validation loss \n",
    "print(f'Meta Layer: Validation Loss = {val_loss/len(dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights\n",
    "model.load_state_dict(torch.load(f'vgae_weights/meta_layers_best_overfit.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "# select a batch from the validation data loader\n",
    "data, _ = next(iter(train_loader))\n",
    "\n",
    "# pass the batch through the trained model to obtain the reconstructed output\n",
    "(graph, lz, base_bdi) = data\n",
    "graph = graph.to(device)\n",
    "lz = lz.t().to(device)\n",
    "\n",
    "edge_attr = graph[:, model.edge_index[0], model.edge_index[1]].t().to(device)        \n",
    "u = base_bdi.to(device)\n",
    "\n",
    "recon_x, recon_edge_attr, u, mu, logvar = model(lz, edge_attr, u)\n",
    "\n",
    "recon_edge_attr = recon_edge_attr.detach()\n",
    "\n",
    "# Create a SparseTensor object from the edge_index and edge_attr tensors\n",
    "recon = torch.zeros((100, 100))\n",
    "\n",
    "print(edge_attr.shape)\n",
    "print(recon_edge_attr.shape)\n",
    "\n",
    "for i in range(model.edge_index.shape[1]):\n",
    "    recon[model.edge_index[0,i], model.edge_index[1,i]] = recon_edge_attr[i]\n",
    "    recon[model.edge_index[1,i], model.edge_index[0,i]] = recon_edge_attr[i]\n",
    "\n",
    "# reshape the output to a 100x100 matrix (assuming the input_dim is 100x100)\n",
    "\n",
    "# plot the original and reconstructed matrices for the first sample in the batch\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plotting.plot_matrix(graph.view(100, 100), colorbar=True, vmax=0.8, vmin=-0.8, axes=ax1)\n",
    "ax1.set_title('Original')\n",
    "plotting.plot_matrix(recon.detach(), colorbar=True, vmax=0.8, vmin=-0.8, axes=ax2)\n",
    "ax2.set_title('Reconstructed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = 'fc_matrices/psilo_ica_100_before'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "psilo_dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=cwd + annotations,\n",
    "                            transform=None, extra_data=None, setting='no_label')\n",
    "\n",
    "psilo_train_loader = DataLoader(psilo_dataset, batch_size=batch_size)\n",
    "\n",
    "# select a batch from the validation data loader\n",
    "data, _ = next(iter(psilo_train_loader))\n",
    "\n",
    "# pass the batch through the trained model to obtain the reconstructed output\n",
    "recon, _, _ = model(data.view(-1, input_dim))\n",
    "\n",
    "# reshape the output to a 100x100 matrix (assuming the input_dim is 100x100)\n",
    "recon = recon.view(-1, 100, 100)\n",
    "\n",
    "for i in range(3):\n",
    "    # plot the original and reconstructed matrices for the first sample in the batch\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    plotting.plot_matrix(data[i], colorbar=True, vmax=0.8, vmin=-0.8, axes=ax1)\n",
    "    ax1.set_title('Original')\n",
    "    plotting.plot_matrix(recon[i].detach(), colorbar=True, vmax=0.8, vmin=-0.8, axes=ax2)\n",
    "    ax2.set_title('Reconstructed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "# load the weights\n",
    "model.load_state_dict(torch.load(f'vgae_weights/gmm7_best.pt', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "\n",
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# calculate the validation loss\n",
    "val_losses = []\n",
    "with torch.no_grad():\n",
    "    for n_comp in range(2, 11):\n",
    "        val_loss = 0.0\n",
    "        model.load_state_dict(torch.load(f'vgae_weights/gmm{n_comp}_best.pt', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "        for batch_idx, (data, _) in enumerate(psilo_train_loader):\n",
    "            recon, mu, logvar = model(data.view(-1, input_dim))\n",
    "            loss = loss_function_gmm(recon, data.view(-1, input_dim), mu, logvar, n_components=5)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(psilo_dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'gmm_{n_comp}: {val_loss} loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_list = list(range(2, 11))\n",
    "\n",
    "# plot the validation loss for each n_components value\n",
    "plt.plot(n_components_list, val_losses)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs. Number of GMM Components')\n",
    "plt.savefig('gmm_component_testing.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "input_dim = 100 * 100\n",
    "\n",
    "model = VGAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model.load_state_dict(torch.load('vgae_weights/gmm_5_hidden256_latent64.pt'))\n",
    "\n",
    "psilo_mus = []\n",
    "hcp_mus = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, _) in enumerate(psilo_train_loader):\n",
    "        mu, _= model.encode(data.view(-1, input_dim))\n",
    "        psilo_mus.append(mu)\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        mu, _ = model.encode(data.view(-1, input_dim))\n",
    "        hcp_mus.append(mu)\n",
    "        \n",
    "psilo_mus = torch.stack(psilo_mus, dim=1)\n",
    "hcp_mus = torch.stack(hcp_mus, dim=1)\n",
    "        \n",
    "# Concatenate the encoded representations and create labels\n",
    "x = torch.cat((psilo_mus, hcp_mus), dim=0)\n",
    "labels = torch.cat((torch.zeros(psilo_mus.shape[0]), torch.ones(hcp_mus.shape[0])), dim=0)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(labels.shape)\n",
    "\n",
    "# Use t-SNE to reduce the dimensionality of the encoded representations\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "x_tsne = tsne.fit_transform(x)\n",
    "\n",
    "# Plot the t-SNE embeddings\n",
    "plt.scatter(x_tsne[:, 0], x_tsne[:, 1], c=labels, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "input_dim = 100 * 100  # size of the graph adjacency matrix\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "\n",
    "dataroot = 'fc_matrices/hcp_100_ica/'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=cwd + dataroot + annotations,\n",
    "                            transform=None, extra_data=None, setting='no_label')\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# instantiate the model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loss_curves = {}\n",
    "best_val_losses = {}  # create a dictionary to store the best validation loss for each configuration\n",
    "\n",
    "best_n = 3\n",
    "\n",
    "for hidden_dim in [256, 512]:\n",
    "    for latent_dim in [64, 128]:\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        model = VGAE(input_dim, hidden_dim, latent_dim).to(device)  # move model to device\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        best_val_loss = float('inf')  # initialize the best validation loss to infinity\n",
    "        \n",
    "        with open('gmm_train_overfit.txt', 'a') as f:\n",
    "            f.write(f'Hidden dim: {hidden_dim}, latent_dim: {latent_dim}\\n')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            # training\n",
    "            model.train()\n",
    "            # define the optimizer and the loss function\n",
    "\n",
    "            for batch_idx, (data, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "                data = data.to(device)  # move input data to device\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                recon, mu, logvar = model(data.view(-1, input_dim))\n",
    "                loss = loss_function_gmm(recon, data.view(-1, input_dim), mu, logvar, n_components=best_n)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, _) in tqdm(enumerate(psilo_train_loader), total=len(psilo_train_loader)):\n",
    "                    data = data.to(device)  # move input data to device\n",
    "                    recon, mu, logvar = model(data.view(-1, input_dim))\n",
    "                    loss = loss_function_gmm(recon, data.view(-1, input_dim), mu, logvar, n_components=best_n)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            # append losses to lists\n",
    "            train_losses.append(train_loss/len(train_dataset))\n",
    "            val_losses.append(val_loss/len(psilo_dataset))\n",
    "\n",
    "            with open('gmm_train_overfit.txt', 'a') as f:\n",
    "                f.write(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\\n')\n",
    "                \n",
    "            # update the best validation loss and save the model weights if it's the best so far for this configuration\n",
    "            if val_losses[-1] < best_val_loss:\n",
    "                best_val_loss = val_losses[-1]\n",
    "                best_val_losses[(hidden_dim, latent_dim)] = best_val_loss\n",
    "                torch.save(model.state_dict(), f'vgae_weights/gmm_{best_n}_hidden{hidden_dim}_latent{latent_dim}.pt')\n",
    "\n",
    "        # plot the losses\n",
    "        plt.plot(val_losses, label=f'Validation Loss (hidden_dim={hidden_dim}, latent_dim={latent_dim})')\n",
    "        \n",
    "                # add the loss curves to the dictionary\n",
    "        loss_curves[f\"hidden{hidden_dim}_latent_dim{latent_dim}\"] = {\"train_loss\": train_losses, \"val_loss\": val_losses}\n",
    "\n",
    "# save the loss curves to a file\n",
    "with open(\"loss_curves_overfit_new.json\", \"w\") as f:\n",
    "    json.dump(loss_curves, f)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"loss_curves_overfit.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(10, 8))\n",
    "for n_comp, loss_dict in loss_curves.items():\n",
    "    val_losses = loss_dict[\"val_loss\"]\n",
    "    epochs = range(1, len(val_losses) + 1)\n",
    "    plt.plot(epochs, val_losses, label=f\"{n_comp}\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.title(\"Validation Loss Curves for Different Net Architectures\")\n",
    "plt.legend()\n",
    "plt.ylim((350, 500))\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "input_dim = 100 * 100  # size of the graph adjacency matrix\n",
    "hidden_dims = [256, 128, 64]\n",
    "latent_dims = [64, 32, 16]\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "\n",
    "dataroot = 'fc_matrices/hcp_100_ica/'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for latent_dim in latent_dims:\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        model = VGAE(input_dim, hidden_dim, latent_dim)\n",
    "        \n",
    "        # load in the model weights\n",
    "        model.load_state_dict(torch.load(f'vgae_weights/gmm_5_hidden{hidden_dim}_latent{latent_dim}.pt'))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for batch_idx, (data, _) in tqdm(enumerate(psilo_train_loader), total=len(psilo_train_loader)):\n",
    "                recon, mu, logvar = model(data.view(-1, input_dim))\n",
    "                loss = loss_function_gmm(recon, data.view(-1, input_dim), mu, logvar, n_components=5)\n",
    "                val_loss += loss.item()\n",
    "            val_losses.append(val_loss/len(psilo_dataset))\n",
    "\n",
    "        # print the validation loss for this configuration\n",
    "        print(f'Hidden Dim: {hidden_dim}, Latent Dim: {latent_dim}, Validation Loss: {val_losses[-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentMLP(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(LatentMLP, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the VGAE model\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "input_dim = 100 * 100\n",
    "output_dim = 1\n",
    "\n",
    "vgae = VGAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "# load the trained VGAE weights\n",
    "vgae.load_state_dict(torch.load('vgae_weights/gmm_5_hidden256_latent64.pt'))\n",
    "\n",
    "# freeze the weights of the VGAE\n",
    "for param in vgae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# instantiate the LatentMLP model\n",
    "mlp = LatentMLP(latent_dim, hidden_dim, output_dim)\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "dataroot = 'fc_matrices/psilo_100_ica/'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=annotations,\n",
    "                            transform=None, extra_data=None, setting='no_label')\n",
    "\n",
    "# # define the data loader for the new dataset\n",
    "# new_dataset = MyNewDataset()\n",
    "# new_loader = DataLoader(new_dataset, batch_size=batch_size)\n",
    "\n",
    "# train the MLP on the new dataset\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(psilo_train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the latent embeddings from the VGAE\n",
    "        _, mu, _ = vgae(inputs)\n",
    "        \n",
    "        # pass the latent embeddings through the MLP\n",
    "        outputs = mlp(mu)\n",
    "        \n",
    "        # calculate the loss and backpropagate\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfcbe3febb699c87ff4898d01faf13d6a57f0e8fbe0b31d844ec717796aaa0a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
