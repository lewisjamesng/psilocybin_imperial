{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from nilearn import plotting\n",
    "\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('..')\n",
    "from utils import BrainGraphDataset, project_root, make_edge_index\n",
    "from models import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set the random seed for reproducibility\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the hyperparameters\n",
    "input_dim = 4950 # size of the graph adjacency matrix\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "\n",
    "dataroot = 'fc_matrices/hcp_100_ica/'\n",
    "root = project_root()\n",
    "\n",
    "dataset = BrainGraphDataset(img_dir=os.path.join(root, dataroot),\n",
    "                            annotations_file=os.path.join(root, dataroot, annotations),\n",
    "                            transform=None, extra_data=None, setting='upper_triangular')\n",
    "\n",
    "# split the dataset into training and validation sets\n",
    "num_samples = len(dataset)\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size = int(0.1 * num_samples)\n",
    "test_size =  num_samples - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# define the data loaders\n",
    "val_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "for dropout in [0, 0.05 ,0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    \n",
    "    model = VAE(input_dim, [hidden_dim] * 2, latent_dim).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(root, f'vae_weights/vae_dropout_{dropout}.pt'), map_location=device))\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(val_loader):\n",
    "            data = data.to(device)  # move data to device\n",
    "            recon, mu, logvar, z = model(data.view(-1, input_dim))\n",
    "            mse_loss, gmm_loss, l2_reg = model.loss(recon, data.view(-1, input_dim), mu, logvar, n_components=3)\n",
    "            val_loss += mse_loss.item()\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)  # move data to device\n",
    "            recon, mu, logvar, z = model(data.view(-1, input_dim))\n",
    "            mse_loss, gmm_loss, l2_reg = model.loss(recon, data.view(-1, input_dim), mu, logvar, n_components=3)\n",
    "            val_loss += mse_loss.item()\n",
    "    val_loss /= (len(test_dataset) + len(val_dataset))\n",
    "    print(f'Dropout {dropout} - Test Loss: {test_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfcbe3febb699c87ff4898d01faf13d6a57f0e8fbe0b31d844ec717796aaa0a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
