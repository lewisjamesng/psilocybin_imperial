{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VAEReg' from 'models' (/rds/general/user/ljn19/home/big/container/psilocybrain/mlp_weights/../models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentMLP, VAE, VAEReg\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrainGraphDataset, project_root\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VAEReg' from 'models' (/rds/general/user/ljn19/home/big/container/psilocybrain/mlp_weights/../models.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import LatentMLP, VAE, VAEReg\n",
    "from utils import BrainGraphDataset, project_root\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# instantiate the VGAE model\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "input_dim = 100 * 100\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "vae = VAEReg(input_dim, hidden_dim, latent_dim, dropout=0.1)\n",
    "\n",
    "# load the trained VGAE weights\n",
    "vae.load_state_dict(torch.load('vae_weights/dropout_0.05_0.01_best.pt', map_location=device))\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "dataroot = 'fc_matrices/psilo_ica_100_before'\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "# Convert the model to the device\n",
    "vae.to(device)\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "psilo_dataset = BrainGraphDataset(img_dir=cwd + dataroot,\n",
    "                            annotations_file=cwd + annotations,\n",
    "                            transform=None, extra_data=None, setting='graph_and_baseline')\n",
    "\n",
    "# Define the train, validation, and test ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Get the number of samples in the dataset\n",
    "num_samples = len(psilo_dataset)\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "train_size = int(train_ratio * num_samples)\n",
    "val_size = int(val_ratio * num_samples)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(psilo_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Define the dataloaders for each set\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=val_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_size, shuffle=False)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Dictionary to store training and validation curves\n",
    "curves = {}\n",
    "\n",
    "for dropout in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    print(f'Dropout {dropout}')\n",
    "    # instantiate the LatentMLP model\n",
    "    mlp = LatentMLP(latent_dim, hidden_dim, output_dim, dropout=dropout)\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    # Convert the MLP to the device\n",
    "    mlp.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_mlp_state = None\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # train the MLP on the new dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs\n",
    "            (graphs, base_bdis), labels = data\n",
    "\n",
    "            graphs = graphs.to(device)\n",
    "            base_bdis = base_bdis.to(device)\n",
    "\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get the latent embeddings from the VGAE\n",
    "            _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "            # pass the latent embeddings through the MLP\n",
    "\n",
    "            outputs = mlp(zs, base_bdis)\n",
    "\n",
    "            # calculate the loss and backpropagate\n",
    "            loss = mlp.loss(outputs, labels.view(outputs.shape))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation check\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                (graphs, base_bdis), labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                base_bdis = base_bdis.to(device)\n",
    "\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the latent embeddings from the VGAE\n",
    "                _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # pass the latent embeddings through the MLP\n",
    "                outputs = mlp(zs, base_bdis)\n",
    "                val_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "        val_loss /= len(val_set)\n",
    "        \n",
    "        # Save the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_mlp_state = mlp.state_dict().copy()\n",
    "\n",
    "        # Print statistics and perform testing every 5 epochs\n",
    "        if epoch % 30 == 9:\n",
    "            print('[%d, %5d] loss: %.3f, val_loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (len(train_set)), val_loss))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        train_losses.append(running_loss / (len(train_set)))\n",
    "        val_losses.append(val_loss)\n",
    "                            \n",
    "    # Add the dropout and its curves to the dictionary\n",
    "    curves[str(dropout)] = {\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses\n",
    "    }\n",
    "    \n",
    "    torch.save(best_mlp_state, f'mlp_weights/dropout_{dropout}')\n",
    "\n",
    "# Save the curves dictionary to a JSON file\n",
    "with open('mlp_dropout_tests.json', 'w') as f:\n",
    "    json.dump(curves, f)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the curves from the JSON file\n",
    "with open('mlp_dropout_tests.json', 'r') as f:\n",
    "    curves = json.load(f)\n",
    "\n",
    "# Plot the curves for each dropout value\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure()\n",
    "\n",
    "for dropout, curve_data in curves.items():\n",
    "    train_losses = curve_data['train_loss']\n",
    "    plt.plot(epochs, train_losses, label=f'Training Loss (Dropout {dropout})')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title(f'Training Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation curve\n",
    "plt.figure()\n",
    "\n",
    "for dropout, curve_data in curves.items():\n",
    "    val_losses = curve_data['val_loss']\n",
    "    plt.plot(epochs, val_losses, label=f'Validation Loss (Dropout {dropout})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title(f'Validation Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and perform final testing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model on the test set and plot the results\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_size, shuffle=True)\n",
    "\n",
    "names = ['Train', 'Test']\n",
    "datasets = [train_set, test_set]\n",
    "loaders = [train_loader, test_loader]\n",
    "\n",
    "for dropout in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    \n",
    "    mlp.load_state_dict(torch.load(f'mlp_weights/dropout_{dropout}.pt', map_location=device))\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for i in range(2):\n",
    "        with torch.no_grad():\n",
    "            for data in loaders[i]:\n",
    "                (graphs, base_bdis), labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                base_bdis = base_bdis.to(device)\n",
    "\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the latent embeddings from the VGAE\n",
    "                _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # pass the latent embeddings through the MLP\n",
    "\n",
    "                outputs = mlp(zs, base_bdis)\n",
    "                criterion = nn.L1Loss(reduction='sum')\n",
    "                test_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "\n",
    "                # Convert back to CPU and numpy arrays for plotting\n",
    "                labels = labels.cpu().numpy().reshape(-1)\n",
    "                predictions = outputs.cpu().numpy().reshape(-1)\n",
    "\n",
    "                # Plot the results\n",
    "                plt.scatter(labels, predictions)\n",
    "                plt.xlabel('True values')\n",
    "                plt.ylabel('Predicted values')\n",
    "                plt.title(f'Predicted vs true BDI, dropout {dropout} ({names[i]})')\n",
    "                plt.xlim([0,50])\n",
    "                plt.ylim([0,50])\n",
    "                plt.show()\n",
    "\n",
    "        # Print the test loss\n",
    "        test_loss /= len(datasets[i])\n",
    "        print(f'{names[i]} loss: %.3f' % test_loss)\n",
    "        correlation_matrix = np.corrcoef(labels, predictions)\n",
    "        pearson_coefficient = correlation_matrix[0, 1]\n",
    "        print(f'Pearson coefficient: {pearson_coefficient}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['fine_tune', 'mixed_class']\n",
    "infiles = ['fine_tune_best.pt', 'mixed_class_0.05_0.01_best.pt']\n",
    "outfiles = ['fine_tune_mlp_dropout_0.1', 'mixed_class_mlp_dropout_0.1']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# instantiate the VGAE model\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "input_dim = 100 * 100\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "# for j in range(2):\n",
    "#     vae = VAEReg(input_dim, hidden_dim, latent_dim, dropout=0.1)\n",
    "#     # load the trained VGAE weights\n",
    "#     vae.load_state_dict(torch.load(f'vgae_weights/{infiles[j]}', map_location=device))\n",
    "\n",
    "#     # Convert the model to the device\n",
    "#     vae.to(device)\n",
    "\n",
    "#     # freeze the weights of the VAE\n",
    "#     for param in vae.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     # define the optimizer and the loss function\n",
    "\n",
    "#     criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "#     dataroot = 'fc_matrices/psilo_ica_100_after'\n",
    "#     cwd = os.getcwd() + '/'\n",
    "\n",
    "#     num_epochs = 300\n",
    "\n",
    "#     import json\n",
    "\n",
    "#     # Dictionary to store training and validation curves\n",
    "#     curves = {}\n",
    "\n",
    "#     # instantiate the LatentMLP model\n",
    "#     mlp = LatentMLP(latent_dim, hidden_dim, output_dim, dropout=0.1)\n",
    "#     optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "#     # Convert the MLP to the device\n",
    "#     mlp.to(device)\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_mlp_state = None\n",
    "\n",
    "#     # Lists to store training and validation losses\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     num_epochs = 1000\n",
    "#     # train the MLP on the new dataset\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for i, data in enumerate(train_loader, 0):\n",
    "#             # get the inputs\n",
    "#             (graphs, base_bdis), labels = data\n",
    "\n",
    "#             graphs = graphs.to(device)\n",
    "#             base_bdis = base_bdis.to(device)\n",
    "\n",
    "#             labels = labels.to(device).float()\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # get the latent embeddings from the VGAE\n",
    "#             _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "#             # pass the latent embeddings through the MLP\n",
    "\n",
    "#             outputs = mlp(zs, base_bdis)\n",
    "\n",
    "#             # calculate the loss and backpropagate\n",
    "#             loss = mlp.loss(outputs, labels.view(outputs.shape))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # print statistics\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Validation check\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for data in val_loader:\n",
    "#                 (graphs, base_bdis), labels = data\n",
    "\n",
    "#                 graphs = graphs.to(device)\n",
    "#                 base_bdis = base_bdis.to(device)\n",
    "\n",
    "#                 labels = labels.to(device).float()\n",
    "\n",
    "#                 # zero the parameter gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # get the latent embeddings from the VGAE\n",
    "#                 _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "#                 # pass the latent embeddings through the MLP\n",
    "#                 outputs = mlp(zs, base_bdis)\n",
    "#                 val_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "#         val_loss /= len(val_set)\n",
    "\n",
    "#         train_losses.append(running_loss / len(train_set))\n",
    "#         val_losses.append(val_loss)\n",
    "        \n",
    "#         # Save the best model so far\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             best_mlp_state = mlp.state_dict().copy()\n",
    "\n",
    "#         # Print statistics and perform testing every 5 epochs\n",
    "#         if epoch % 10 == 9:\n",
    "#             print('[%d, %5d] loss: %.3f, val_loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / (len(train_set)), val_loss))\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     curves[names[j]] = {'val_losses': val_losses, 'train_losses': train_losses}\n",
    "#     torch.save(best_mlp_state, f'mlp_weights/{outfiles[j]}.pt')\n",
    "    \n",
    "#     with open('mlp_hyper_curves.json', 'w') as f:\n",
    "#         json.dump(curves, f)\n",
    "# print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"mlp_hyper_curves.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "val_losses = loss_curves['mixed_class']['val_losses']\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "epochs = range(1, len(val_losses) + 1)\n",
    "plt.plot(epochs, val_losses, label=\"Val\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.ylim((2.5, 15))\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"mlp_hyper_curves.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "train_losses = loss_curves['mixed_class']['train_losses']\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Train Loss Curves\")\n",
    "plt.legend()\n",
    "plt.ylim((0, 15))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on the fine tune vae\n",
    "names = ['Train', 'Test']\n",
    "\n",
    "for mlp_path in ['fine_tune_mlp_dropout_0.1.pt', 'mixed_class_mlp_dropout_0.1.pt']:\n",
    "    mlp.load_state_dict(torch.load(f'mlp_weights/{mlp_path}', map_location=device))\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for i in range(2):\n",
    "        with torch.no_grad():\n",
    "            for data in loaders[i]:\n",
    "                (graphs, base_bdis), labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                base_bdis = base_bdis.to(device)\n",
    "\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the latent embeddings from the VGAE\n",
    "                _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # pass the latent embeddings through the MLP\n",
    "\n",
    "                outputs = mlp(zs, base_bdis)\n",
    "                criterion = nn.L1Loss(reduction='sum')\n",
    "                test_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "\n",
    "                # Convert back to CPU and numpy arrays for plotting\n",
    "                labels = labels.cpu().numpy().reshape(-1)\n",
    "                predictions = outputs.cpu().numpy().reshape(-1)\n",
    "\n",
    "                # Plot the results\n",
    "                plt.scatter(labels, predictions)\n",
    "                plt.xlabel('True values')\n",
    "                plt.ylabel('Predicted values')\n",
    "                plt.title(f'Predicted vs true BDI ({names[i]})')\n",
    "                plt.xlim([0,50])\n",
    "                plt.ylim([0,50])\n",
    "                plt.show()\n",
    "\n",
    "        # Print the test loss\n",
    "        test_loss /= len(datasets[i])\n",
    "        print(f'{names[i]} loss: %.3f' % test_loss)\n",
    "        correlation_matrix = np.corrcoef(labels, predictions)\n",
    "        pearson_coefficient = correlation_matrix[0, 1]\n",
    "        print(f'Pearson coefficient: {pearson_coefficient}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
