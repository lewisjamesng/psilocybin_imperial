{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "[1,     4] loss: 11.966, val_loss: 9.092\n",
      "[2,     4] loss: 10.542, val_loss: 7.527\n",
      "[3,     4] loss: 8.796, val_loss: 6.189\n",
      "[4,     4] loss: 7.558, val_loss: 6.341\n",
      "[5,     4] loss: 8.068, val_loss: 6.605\n",
      "[6,     4] loss: 7.974, val_loss: 5.883\n",
      "[7,     4] loss: 7.055, val_loss: 5.761\n",
      "[8,     4] loss: 7.160, val_loss: 5.872\n",
      "[9,     4] loss: 7.242, val_loss: 5.854\n",
      "[10,     4] loss: 7.168, val_loss: 5.716\n",
      "[11,     4] loss: 6.997, val_loss: 5.779\n",
      "[12,     4] loss: 6.985, val_loss: 5.750\n",
      "[13,     4] loss: 6.961, val_loss: 5.731\n",
      "[14,     4] loss: 6.932, val_loss: 5.762\n",
      "[15,     4] loss: 6.916, val_loss: 5.768\n",
      "[16,     4] loss: 6.903, val_loss: 5.811\n",
      "[17,     4] loss: 7.018, val_loss: 5.846\n",
      "[18,     4] loss: 6.859, val_loss: 5.769\n",
      "[19,     4] loss: 6.957, val_loss: 5.798\n",
      "[20,     4] loss: 6.975, val_loss: 5.875\n",
      "[21,     4] loss: 6.870, val_loss: 5.869\n",
      "[22,     4] loss: 6.829, val_loss: 5.835\n",
      "[23,     4] loss: 6.846, val_loss: 5.886\n",
      "[24,     4] loss: 6.827, val_loss: 5.836\n",
      "[25,     4] loss: 6.879, val_loss: 5.845\n",
      "[26,     4] loss: 6.802, val_loss: 5.911\n",
      "[27,     4] loss: 6.831, val_loss: 5.881\n",
      "[28,     4] loss: 6.785, val_loss: 5.862\n",
      "[29,     4] loss: 6.751, val_loss: 5.890\n",
      "[30,     4] loss: 6.789, val_loss: 5.895\n",
      "[31,     4] loss: 6.782, val_loss: 5.847\n",
      "[32,     4] loss: 6.798, val_loss: 5.964\n",
      "[33,     4] loss: 6.803, val_loss: 5.909\n",
      "[34,     4] loss: 6.710, val_loss: 5.915\n",
      "[35,     4] loss: 6.749, val_loss: 5.881\n",
      "[36,     4] loss: 6.690, val_loss: 5.908\n",
      "[37,     4] loss: 6.676, val_loss: 5.935\n",
      "[38,     4] loss: 6.758, val_loss: 5.900\n",
      "[39,     4] loss: 6.705, val_loss: 5.907\n",
      "[40,     4] loss: 6.662, val_loss: 5.965\n",
      "[41,     4] loss: 6.735, val_loss: 5.965\n",
      "[42,     4] loss: 6.703, val_loss: 5.969\n",
      "[43,     4] loss: 6.645, val_loss: 5.978\n",
      "[44,     4] loss: 6.641, val_loss: 6.027\n",
      "[45,     4] loss: 6.698, val_loss: 6.005\n",
      "[46,     4] loss: 6.721, val_loss: 6.004\n",
      "[47,     4] loss: 6.637, val_loss: 5.993\n",
      "[48,     4] loss: 6.629, val_loss: 5.960\n",
      "[49,     4] loss: 6.601, val_loss: 5.998\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/tmp/pbs.7724891.pbs/ipykernel_2246010/670334633.py\", line 92, in <module>\n",
      "    for i, data in enumerate(train_loader, 0):\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 678, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 298, in __getitem__\n",
      "    return self.dataset[self.indices[idx]]\n",
      "  File \"/rds/general/user/ljn19/home/big/container/psilocybrain/mlp_weights/../utils.py\", line 87, in __getitem__\n",
      "    repr_end = (graph_to_repr(graph), self.get_data_label(idx).float(), torch.tensor(lz).float().view(lz.shape[0], -1))\n",
      "  File \"/rds/general/user/ljn19/home/big/container/psilocybrain/mlp_weights/../utils.py\", line -1, in graph_to_repr\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/rds/general/user/ljn19/home/anaconda3/envs/fyp/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import LatentMLP, VAE\n",
    "from utils import BrainGraphDataset, project_root\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "root = project_root()\n",
    "\n",
    "# instantiate the VGAE model\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "input_dim = 4950\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "vae = VAE(input_dim, [128] * 2, latent_dim)\n",
    "\n",
    "# load the trained VGAE weights\n",
    "vae.load_state_dict(torch.load(os.path.join(root, 'vae_weights/vae_dropout_psilo_schaefer_before_0.pt'), map_location=device))\n",
    "\n",
    "# define the optimizer and the loss function\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "dataroot = 'fc_matrices/psilo_schaefer_before'\n",
    "\n",
    "\n",
    "# Convert the model to the device\n",
    "vae.to(device)\n",
    "\n",
    "annotations = 'annotations.csv'\n",
    "psilo_dataset = BrainGraphDataset(img_dir=os.path.join(root, dataroot),\n",
    "                            annotations_file=os.path.join(root, annotations),\n",
    "                            transform=None, extra_data=None, setting='upper_triangular_baseline_lz')\n",
    "\n",
    "# Define the train, validation, and test ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Get the number of samples in the dataset\n",
    "num_samples = len(psilo_dataset)\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "train_size = int(train_ratio * num_samples)\n",
    "val_size = int(val_ratio * num_samples)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(psilo_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Define the dataloaders for each set\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=val_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_size, shuffle=False)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Dictionary to store training and validation curves\n",
    "curves = {}\n",
    "\n",
    "\n",
    "# instantiate the LatentMLP model\n",
    "mlp = LatentMLP(latent_dim, hidden_dim, output_dim, dropout=0, extra_dim=100)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "# Convert the MLP to the device\n",
    "mlp.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_mlp_state = None\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print('Started Training')\n",
    "# train the MLP on the new dataset\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        (graphs, base_bdis, lz), labels = data\n",
    "\n",
    "        graphs = graphs.to(device)\n",
    "        base_bdis = base_bdis.to(device)\n",
    "        lz = lz.to(device).view(-1, 100)\n",
    "\n",
    "        base_bdis = torch.cat([base_bdis, lz], dim=1)\n",
    "\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the latent embeddings from the VGAE\n",
    "        _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "        # pass the latent embeddings through the MLP\n",
    "\n",
    "        outputs = mlp(zs, base_bdis)\n",
    "\n",
    "        # calculate the loss and backpropagate\n",
    "        loss = mlp.loss(outputs, labels.view(outputs.shape))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation check\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            (graphs, base_bdis, lz), labels = data\n",
    "\n",
    "            graphs = graphs.to(device)\n",
    "            base_bdis = base_bdis.to(device)\n",
    "            lz = lz.to(device).view(-1, 100)\n",
    "            base_bdis = torch.cat([base_bdis, lz], dim=1)\n",
    "\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get the latent embeddings from the VGAE\n",
    "            _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "            # pass the latent embeddings through the MLP\n",
    "            outputs = mlp(zs, base_bdis)\n",
    "            val_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "    val_loss /= len(test_set)\n",
    "\n",
    "    # Save the best model so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_mlp_state = mlp.state_dict().copy()\n",
    "\n",
    "    # Print statistics and perform testing every 5 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print('[%d, %5d] loss: %.3f, val_loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / (len(train_set)), val_loss))\n",
    "        running_loss = 0.0\n",
    "\n",
    "    train_losses.append(running_loss / (len(train_set)))\n",
    "    val_losses.append(val_loss)\n",
    "                            \n",
    "#     # Add the dropout and its curves to the dictionary\n",
    "#     curves[str(dropout)] = {\n",
    "#         'train_loss': train_losses,\n",
    "#         'val_loss': val_losses\n",
    "#     }\n",
    "    \n",
    "#     torch.save(best_mlp_state, f'mlp_weights/dropout_{dropout}')\n",
    "\n",
    "# # Save the curves dictionary to a JSON file\n",
    "# with open('mlp_dropout_tests.json', 'w') as f:\n",
    "#     json.dump(curves, f)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the curves from the JSON file\n",
    "with open('mlp_dropout_tests.json', 'r') as f:\n",
    "    curves = json.load(f)\n",
    "\n",
    "# Plot the curves for each dropout value\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure()\n",
    "\n",
    "for dropout, curve_data in curves.items():\n",
    "    train_losses = curve_data['train_loss']\n",
    "    plt.plot(epochs, train_losses, label=f'Training Loss (Dropout {dropout})')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title(f'Training Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation curve\n",
    "plt.figure()\n",
    "\n",
    "for dropout, curve_data in curves.items():\n",
    "    val_losses = curve_data['val_loss']\n",
    "    plt.plot(epochs, val_losses, label=f'Validation Loss (Dropout {dropout})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title(f'Validation Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and perform final testing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model on the test set and plot the results\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=train_size, shuffle=True)\n",
    "\n",
    "names = ['Train', 'Test']\n",
    "datasets = [train_set, test_set]\n",
    "loaders = [train_loader, test_loader]\n",
    "\n",
    "for dropout in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    \n",
    "    mlp.load_state_dict(torch.load(f'mlp_weights/dropout_{dropout}.pt', map_location=device))\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for i in range(2):\n",
    "        with torch.no_grad():\n",
    "            for data in loaders[i]:\n",
    "                (graphs, base_bdis), labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                base_bdis = base_bdis.to(device)\n",
    "\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the latent embeddings from the VGAE\n",
    "                _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # pass the latent embeddings through the MLP\n",
    "\n",
    "                outputs = mlp(zs, base_bdis)\n",
    "                criterion = nn.L1Loss(reduction='sum')\n",
    "                test_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "\n",
    "                # Convert back to CPU and numpy arrays for plotting\n",
    "                labels = labels.cpu().numpy().reshape(-1)\n",
    "                predictions = outputs.cpu().numpy().reshape(-1)\n",
    "\n",
    "                # Plot the results\n",
    "                plt.scatter(labels, predictions)\n",
    "                plt.xlabel('True values')\n",
    "                plt.ylabel('Predicted values')\n",
    "                plt.title(f'Predicted vs true BDI, dropout {dropout} ({names[i]})')\n",
    "                plt.xlim([0,50])\n",
    "                plt.ylim([0,50])\n",
    "                plt.show()\n",
    "\n",
    "        # Print the test loss\n",
    "        test_loss /= len(datasets[i])\n",
    "        print(f'{names[i]} loss: %.3f' % test_loss)\n",
    "        correlation_matrix = np.corrcoef(labels, predictions)\n",
    "        pearson_coefficient = correlation_matrix[0, 1]\n",
    "        print(f'Pearson coefficient: {pearson_coefficient}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['fine_tune', 'mixed_class']\n",
    "infiles = ['fine_tune_best.pt', 'mixed_class_0.05_0.01_best.pt']\n",
    "outfiles = ['fine_tune_mlp_dropout_0.1', 'mixed_class_mlp_dropout_0.1']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# instantiate the VGAE model\n",
    "hidden_dim = 256\n",
    "latent_dim = 64\n",
    "input_dim = 100 * 100\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "# for j in range(2):\n",
    "#     vae = VAEReg(input_dim, hidden_dim, latent_dim, dropout=0.1)\n",
    "#     # load the trained VGAE weights\n",
    "#     vae.load_state_dict(torch.load(f'vgae_weights/{infiles[j]}', map_location=device))\n",
    "\n",
    "#     # Convert the model to the device\n",
    "#     vae.to(device)\n",
    "\n",
    "#     # freeze the weights of the VAE\n",
    "#     for param in vae.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     # define the optimizer and the loss function\n",
    "\n",
    "#     criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "#     dataroot = 'fc_matrices/psilo_ica_100_after'\n",
    "#     cwd = os.getcwd() + '/'\n",
    "\n",
    "#     num_epochs = 300\n",
    "\n",
    "#     import json\n",
    "\n",
    "#     # Dictionary to store training and validation curves\n",
    "#     curves = {}\n",
    "\n",
    "#     # instantiate the LatentMLP model\n",
    "#     mlp = LatentMLP(latent_dim, hidden_dim, output_dim, dropout=0.1)\n",
    "#     optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "#     # Convert the MLP to the device\n",
    "#     mlp.to(device)\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_mlp_state = None\n",
    "\n",
    "#     # Lists to store training and validation losses\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     num_epochs = 1000\n",
    "#     # train the MLP on the new dataset\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for i, data in enumerate(train_loader, 0):\n",
    "#             # get the inputs\n",
    "#             (graphs, base_bdis), labels = data\n",
    "\n",
    "#             graphs = graphs.to(device)\n",
    "#             base_bdis = base_bdis.to(device)\n",
    "\n",
    "#             labels = labels.to(device).float()\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # get the latent embeddings from the VGAE\n",
    "#             _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "#             # pass the latent embeddings through the MLP\n",
    "\n",
    "#             outputs = mlp(zs, base_bdis)\n",
    "\n",
    "#             # calculate the loss and backpropagate\n",
    "#             loss = mlp.loss(outputs, labels.view(outputs.shape))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # print statistics\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Validation check\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for data in val_loader:\n",
    "#                 (graphs, base_bdis), labels = data\n",
    "\n",
    "#                 graphs = graphs.to(device)\n",
    "#                 base_bdis = base_bdis.to(device)\n",
    "\n",
    "#                 labels = labels.to(device).float()\n",
    "\n",
    "#                 # zero the parameter gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # get the latent embeddings from the VGAE\n",
    "#                 _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "#                 # pass the latent embeddings through the MLP\n",
    "#                 outputs = mlp(zs, base_bdis)\n",
    "#                 val_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "#         val_loss /= len(val_set)\n",
    "\n",
    "#         train_losses.append(running_loss / len(train_set))\n",
    "#         val_losses.append(val_loss)\n",
    "        \n",
    "#         # Save the best model so far\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             best_mlp_state = mlp.state_dict().copy()\n",
    "\n",
    "#         # Print statistics and perform testing every 5 epochs\n",
    "#         if epoch % 10 == 9:\n",
    "#             print('[%d, %5d] loss: %.3f, val_loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / (len(train_set)), val_loss))\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     curves[names[j]] = {'val_losses': val_losses, 'train_losses': train_losses}\n",
    "#     torch.save(best_mlp_state, f'mlp_weights/{outfiles[j]}.pt')\n",
    "    \n",
    "#     with open('mlp_hyper_curves.json', 'w') as f:\n",
    "#         json.dump(curves, f)\n",
    "# print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"mlp_hyper_curves.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "val_losses = loss_curves['mixed_class']['val_losses']\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "epochs = range(1, len(val_losses) + 1)\n",
    "plt.plot(epochs, val_losses, label=\"Val\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.ylim((2.5, 15))\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load in the loss curves from file\n",
    "with open(\"mlp_hyper_curves.json\", \"r\") as f:\n",
    "    loss_curves = json.load(f)\n",
    "\n",
    "train_losses = loss_curves['mixed_class']['train_losses']\n",
    "# plot the validation loss curves for each number of GMM components\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train\")\n",
    "\n",
    "# add labels and legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Train Loss Curves\")\n",
    "plt.legend()\n",
    "plt.ylim((0, 15))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on the fine tune vae\n",
    "names = ['Train', 'Test']\n",
    "\n",
    "for mlp_path in ['fine_tune_mlp_dropout_0.1.pt', 'mixed_class_mlp_dropout_0.1.pt']:\n",
    "    mlp.load_state_dict(torch.load(f'mlp_weights/{mlp_path}', map_location=device))\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for i in range(2):\n",
    "        with torch.no_grad():\n",
    "            for data in loaders[i]:\n",
    "                (graphs, base_bdis), labels = data\n",
    "\n",
    "                graphs = graphs.to(device)\n",
    "                base_bdis = base_bdis.to(device)\n",
    "\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the latent embeddings from the VGAE\n",
    "                _, _, _, zs = vae(graphs.view(-1, input_dim))\n",
    "\n",
    "                # pass the latent embeddings through the MLP\n",
    "\n",
    "                outputs = mlp(zs, base_bdis)\n",
    "                criterion = nn.L1Loss(reduction='sum')\n",
    "                test_loss += criterion(outputs, labels.view(outputs.shape)).item()\n",
    "\n",
    "                # Convert back to CPU and numpy arrays for plotting\n",
    "                labels = labels.cpu().numpy().reshape(-1)\n",
    "                predictions = outputs.cpu().numpy().reshape(-1)\n",
    "\n",
    "                # Plot the results\n",
    "                plt.scatter(labels, predictions)\n",
    "                plt.xlabel('True values')\n",
    "                plt.ylabel('Predicted values')\n",
    "                plt.title(f'Predicted vs true BDI ({names[i]})')\n",
    "                plt.xlim([0,50])\n",
    "                plt.ylim([0,50])\n",
    "                plt.show()\n",
    "\n",
    "        # Print the test loss\n",
    "        test_loss /= len(datasets[i])\n",
    "        print(f'{names[i]} loss: %.3f' % test_loss)\n",
    "        correlation_matrix = np.corrcoef(labels, predictions)\n",
    "        pearson_coefficient = correlation_matrix[0, 1]\n",
    "        print(f'Pearson coefficient: {pearson_coefficient}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
